<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
					<textarea data-template>
						![Logo](./imgs/Logo_Roma_Tre.jpg) <!-- .element: style="width: 20%;" -->

						# Exploring LLM abilities
						## A deep dive into LLMs

						Andrea Matarazzo <!-- .element: style="font-size: 0.4em; margin-top: 2em" -->

						A.A. 2023/2024 <!-- .element: style="font-size: 0.3em" -->

						---

						# Advent of LLMs

						![history](./imgs/history.png)

						---

						<!-- .slide: data-background-image="./imgs/llm.png" data-background-size="80%" data-background-opacity="0.2" -->

						# Tasks

						Text generation <!-- .element: class="fragment semi-fade-out" data-fragment-index="1" -->

						<div class="fragment fade-in" data-fragment-index="1">

						Translation <!-- .element: class="fragment grow" data-fragment-index="1" -->

						Question answering <!-- .element: class="fragment grow" data-fragment-index="1" -->

						Summarization <!-- .element: class="fragment grow" data-fragment-index="1" -->

						Sentiment analysis <!-- .element: class="fragment grow" data-fragment-index="1" -->

						Reasoning <!-- .element: class="fragment grow" data-fragment-index="1" -->

						Planning <!-- .element: class="fragment grow" data-fragment-index="1" -->

						</div>

						---

						# LLM applications

						![domain](./imgs/domain.png)

						---

						# Emergent abilities

						<div class="r-stack">
						<div class="fragment fade-out" data-fragment-index="0">

						![scaling](./imgs/scaling-phase.png)

						"Emergence is when quantitative changes in a system result in qualitative changes in behavior" <!-- .element: style="font-style: italic; font-size: 0.5em" -->

						</div>
						<div class="fragment current-visible" style="display:flex" data-fragment-index="0">
						<img src="./imgs/icl.png" style="width: 500px;" />

						- In-Context Learning (ICL) <!-- .element: class="fragment highlight-current-blue" data-fragment-index="0" -->
						- Chain of Thought (CoT)
						- Program of Thought (PoT)
						- Planning

						</div>
						<div class="fragment current-visible" style="display:flex" data-fragment-index="1">
						<img src="./imgs/cot.png" style="width: 500px;" />

						- In-Context Learning (ICL)
						- Chain of Thought (CoT) <!-- .element: class="fragment highlight-current-blue" data-fragment-index="1" -->
						- Program of Thought (PoT)
						- Planning

						</div>

						<div class="fragment current-visible" style="display:flex" data-fragment-index="2">
						<img src="./imgs/pot.png" style="width: 500px;" />

						- In-Context Learning (ICL)
						- Chain of Thought (CoT)
						- Program of Thought (PoT) <!-- .element: class="fragment highlight-current-blue" data-fragment-index="2" -->
						- Planning

						</div>
						<div class="fragment" style="display:flex" data-fragment-index="3">
						<img src="./imgs/planning.png" style="width: 500px;" />

						- In-Context Learning (ICL)
						- Chain of Thought (CoT)
						- Program of Thought (PoT)
						- Planning <!-- .element: class="fragment highlight-current-blue" data-fragment-index="3" -->

						</div>

						</div>

						---

						# Chain of Thought

						Where this ability comes from?

						***

						- Factors
							- Model size
							- Instruction tuning
							- Pre-training corpora
						- Hypothesis is that CoT is elicited by training on **code**

						---

						# Experiments

						- LLaMA models
							- Same architecture <!-- .element: style="font-size: 0.9em" -->
								- LLaMA 3.x does not significantly deviate from LLaMA 1 and 2 <!-- .element: style="font-size: 0.6em" -->
							- Same model size (7B) <!-- .element: style="font-size: 0.9em" -->
							- Increasing code % in pre-training corpora <!-- .element: style="font-size: 0.9em" -->
								- from 5% to 17% <!-- .element: style="font-size: 0.6em" -->
						- GSM8k and GSM-hard datasets
							- Mathematical word problems datasets <!-- .element: style="font-size: 0.9em" -->
							- Same problems <!-- .element: style="font-size: 0.9em" -->
								- GSM-hard replaces numbers with large numbers <!-- .element: style="font-size: 0.6em" -->
								- GSM-hard PoT annotated datasets <!-- .element: style="font-size: 0.6em" -->

						---

						## LLaMa CoT on GSM8k

						CoT is elicited by training on code

						***

						<div style="display: flex;">
						<div style="height:470px; max-width: 500px">
						<canvas data-chart="bar">
						<!--
						{
						 "options": { "scales": { "x": { "stacked": true } } },
						 "data": {
						  "labels": ["LLaMA2 7B", "Code LLaMA 7B", "LLaMA2 13B", "LLaMA3 7B", "LLaMA3.1 7B" ],
						  "datasets":[
						   {
							"data":[3.1,3.99,10.53,31,75.9],
							"label":"0-shot"
						   },
						   {
							"data":[15.7, 16.3, 35.8, 47, 80.9],
							"label":"5-shot"
						   }
						  ]
						 }
						}
						-->
						</canvas>
						</div>
						<div>

						- model size is not a deciding factor <!-- .element: style="font-size: 0.8em" -->
							- ...but it improves CoT <!-- .element: style="font-size: 0.7em" -->
							- Some 175B don't do CoT <!-- .element: style="font-size: 0.7em" -->
						- fine-tuning doesn't affect CoT  <!-- .element: style="font-size: 0.8em" -->
							- CoT on Code LLaMA (fine-tuned) is not better than LLaMA2 <!-- .element: style="font-size: 0.7em" -->
						- code in pre-training corpora is the key <!-- .element: style="font-size: 0.8em" -->
							- all tested LLaMA models trained on code do CoT <!-- .element: style="font-size: 0.7em" -->
							- code-davinci-002 vs text-davinci-002 <!-- .element: style="font-size: 0.7em" -->

						</div>
						</div>

						---

						## LLaMa CoT on GSM-hard
						LLMs cannot reason

						***

						<div style="display: flex;">
						<div style="height:470px; width:400px;">
						<canvas data-chart="bar">
						<!--
						{
						 "options": { "scales": { "x": { "stacked": true } } },
						 "data": {
						  "labels": ["LLaMA2 7B", "Code LLaMA 7B", "LLaMA2 13B", "LLaMA3 7B", "LLaMA3.1 7B" ],
						  "datasets":[
						   {
							"data":[0.1, 1.30, 0.1, 5.4, 7.85],
							"label":"0-shot"
						   },
						   {
							"data":[0.1, 1.5, 0.1, 7.4, 9.64],
							"label":"5-shot"
						   }
						  ]
						 }
						}
						-->
						</canvas>
						</div>
						<div>

						- LLMs are retrievers, more than reasoners
						- LLMs learn to do Bayesian inference during pre-training

						</div>
						</div>

						---

						## LLaMa PoT on GSM-hard
						LLMs cannot plan

						***

						<div style="display: flex;">
						<div style="height:470px; width:400px;">
						<canvas data-chart="bar">
						<!--
						{
						 "options": { "scales": { "x": { "stacked": true } } },
						 "data": {
						  "labels": ["LLaMA2 7B", "Code LLaMA 7B", "LLaMA2 13B", "LLaMA3 7B", "LLaMA3.1 7B" ],
						  "datasets":[
						   {
							"data":[0.1, 1.30, 0.1, 5.4, 7.85],
							"label":"0-shot"
						   },
						   {
							"data":[16.69, 27.6, 36, 56.1, 62.36],
							"label":"5-shot"
						   }
						  ]
						 }
						}
						-->
						</canvas>
						</div>
						<div>

						- Same as previous experiment
							- ...but using Python compiler to execute generated code <!-- .element: style="font-size: 0.7em" -->
						- Good at code generation
							- ...but poor in leveraging the generated code to give the correct answer <!-- .element: style="font-size: 0.7em" -->
						- Poor at planning
							- Self-verification and self-improvement essential for planning <!-- .element: style="font-size: 0.7em" -->

						</div>
						</div>

						---

						# Conclusions

						![](./imgs/systems.png) <!-- .element: style="width: 50%;" -->

						"The automatic operations of System 1 generate surprisingly complex patterns of ideas, but only the slower System 2 can construct thoughts in an orderly series of steps." <!-- .element: style="font-size: 0.5em" -->

						Daniel Kahneman <!-- .element: style="font-size: 0.3em" --> *in Thinking, Fast and Slow*

						"Why does LLM respond in constant time, even for polynomial or exponential problems?" <!-- .element: style="font-size: 0.5em" -->

						Subbarao Kambhampati <!-- .element: style="font-size: 0.3em" --> *in LLMs Canâ€™t Plan, But Can Help Planning in LLM-Modulo Frameworks*

						---

						# The end

						Thank you!
						
					</textarea>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<!-- Chart plugin -->
		<script src="plugin/chart/plugin.js"></script>
		<script src="plugin/chart/chart.min.js"></script>

		<!-- Chart plugin -->
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX, RevealChart ],
				chart: {
					defaults: {
						color: 'lightgray',
						scale: {
							grid: { color: "lightgray" },
							min: 0,
							max: 100,
						}
					},
					line: { borderColor: [ "#e60049", "#0bb4ff", "#50e991", "#e6d800", "#9b19f5", "#ffa300", "#dc0ab4", "#b3d4ff", "#00bfa0"] },
					bar: {
						backgroundColor: [ "#e60049", "#0bb4ff", "#50e991", "#e6d800", "#9b19f5", "#ffa300", "#dc0ab4", "#b3d4ff", "#00bfa0"]
					},
				}
			});
		</script>
	</body>
</html>
