%! Author = amatarazzo
%! Date = 10/03/24
@article{survey,
  author        = {Wayne Xin Zhao and Kun Zhou\textasteriskcentered and Junyi Li\textasteriskcentered and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  year          = {2023},
  title         = {A Survey of Large Language Models},
  publisher     = {ArXiv}
}
@article{nlm,
  title         = {A Neural Probabilistic Language Model},
  author        = {Bengio, Yoshua and Ducharme, Rejean and Vincent, Pascal and Janvin, Christian},
  journal       = {Journal of Machine Learning Research},
  volume        = {3},
  pages         = {1137--1155},
  year          = {2003}
}
@inproceedings{word2vec,
  title         = {Distributed Representations of Words and Phrases and Their Compositionality},
  author        = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeffrey},
  booktitle     = {Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a Meeting Held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  editor        = {Burges, C. J. C. and Bottou, L. and Ghahramani, Z. and Weinberger, K. Q.},
  year          = {2013},
  pages         = {3111--3119}
}
@inproceedings{word2vec2,
  title         = {Efficient Estimation of Word Representations in Vector Space},
  author        = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  booktitle     = {1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  editor        = {Bengio, Yoshua and LeCun, Yann},
  year          = {2013}
}
@misc{elmo,
  author        = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year          = {2018},
  title         = {Deep Contextualized Word Representations},
  howpublished  = {arXiv preprint},
  url           = {https://arxiv.org/abs/1802.05365}
}
@inproceedings{devlin2019bert,
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title         = {Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle     = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019},
  editor        = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  volume        = {1},
  number        = {Long and Short Papers},
  series        = {NAACL-HLT '19},
  pages         = {4171--4186},
  year          = {2019},
  month         = jun,
  address       = {Minneapolis, MN, USA},
  publisher     = {Association for Computational Linguistics},
  doi           = {10.18653/v1/N19-1423},
  url           = {https://www.aclweb.org/anthology/N19-1423}
}
@online{radford2019language,
  author        = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya et al.},
  title         = {Language Models Are Unsupervised Multitask Learners},
  howpublished  = {OpenAI Blog},
  year          = {2019},
  url           = {https://openai.com/blog/better-language-models/}
}
@misc{brown2020language,
  title         = {Language Models Are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{raffel2023exploring,
  author        = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Mengye and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal       = {Journal of Machine Learning Research},
  volume        = {21},
  pages         = {140:1--140:67},
  year          = {2020}
}
@article{emergent1,
  author        = {Philip W. Anderson},
  year          = {1972},
  title         = {More is Different: Broken Symmetry and the Nature of the Hierarchical Structure of Science},
  publisher     = {Science},
  url           = {http://www.lanais.famaf.unc.edu.ar/cursos/em/Anderson-MoreDifferent-1972.pdf}
}
@misc{emergent2,
  title         = {Emergent Abilities of Large Language Models},
  author        = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  year          = {2022},
  eprint        = {2206.07682},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{chatgpt,
  title         = {Towards a Human-like Open-Domain Chatbot},
  author        = {Daniel Adiwardana and Minh-Thang Luong and David R. So and Jamie Hall and Noah Fiedel and Romal Thoppilan and Zi Yang and Apoorv Kulshreshtha and Gaurav Nemade and Yifeng Lu and Quoc V. Le},
  year          = {2020},
  eprint        = {2001.09977},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{gpt4,
  title         = {{GPT-4} Technical Report},
  author        = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Sim\'{o}n Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and \L{}ukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and \L{}ukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David M\'{e}ly and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer\'{o}n Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year          = {2024},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{clip,
  title         = {Learning Transferable Visual Models From Natural Language Supervision},
  author        = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  year          = {2021},
  eprint        = {2103.00020},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{dall-e,
  title         = {Zero-Shot Text-to-Image Generation},
  author        = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
  year          = {2021},
  eprint        = {2102.12092},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{kaplan2020scaling,
  author        = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title         = {Scaling Laws for Neural Language Models},
  journal       = {CoRR},
  volume        = {abs/2001.08361},
  year          = {2020}
}
@article{hoffmann2022training,
  author        = {Hoffmann, Jan and Borgeaud, Samuel and Mensch, Andrei and Buchatskaya, Elena and Cai, Tianlong and Rutherford, Edward and de Las Casas, Daniel and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Alexander and Hennigan, Tim and Noland, Emily and Millican, Kelsey and van den Driessche, Gilles and Damoc, Bogdan and Guy, Adam and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  title         = {Training Compute-Optimal Large Language Models},
  journal       = {CoRR},
  volume        = {abs/2203.15556},
  year          = {2022}
}
@article{icl,
  author        = {D. Dai AND Y. Sun AND L. Dong AND Y. Hao AND Z. Sui AND F. Wei},
  year          = {2022},
  title         = {Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers},
  publisher     = {CoRR}
}
@article{ouyang2022training,
  title         = {Training Language Models to Follow Instructions with Human Feedback},
  author        = {Ouyang, L. and Wu, J. and Jiang, X. and Almeida, D. and Wainwright, C. L. and Mishkin, P. and Zhang, C. and Agarwal, S. and Slama, K. and Ray, A. and Schulman, J. and Hilton, J. and Kelton, F. and Miller, L. and Simens, M. and Askell, A. and Welinder, P. and Christiano, P. F. and Leike, J. and Lowe, R.},
  journal       = {CoRR},
  volume        = {abs/2203.02155},
  year          = {2022}
}
@inproceedings{wei2022fine,
  title         = {Fine-tuned Language Models are Zero-shot Learners},
  author        = {Wei, J. and Bosma, M. and Zhao, V. Y. and Guu, K. and Yu, A. W. and Lester, B. and Du, N. and Dai, A. M. and Le, Q. V.},
  booktitle     = {The Tenth International Conference on Learning Representations, ICLR 2022},
  year          = {2022},
  address       = {Virtual Event},
  month         = {April 25-29},
  organization  = {OpenReview.net}
}
@article{chung2022scaling,
  title         = {Scaling Instruction-Finetuned Language Models},
  author        = {Chung, H. W. and Hou, L. and Longpre, S. and Zoph, B. and Tay, Y. and Fedus, W. and Li, E. and Wang, X. and Dehghani, M. and Brahma, S. and Webson, A. and Gu, S. S. and Dai, Z. and Suzgun, M. and Chen, X. and Chowdhery, A. and Narang, S. and Mishra, G. and Yu, A. and Zhao, V. Y. and Huang, Y. and Dai, A. M. and Yu, H. and Petrov, S. and Chi, E. H. and Dean, J. and Devlin, J. and Roberts, A. and Zhou, D. and Le, Q. V. and Wei, J.},
  journal       = {CoRR},
  volume        = {abs/2210.11416},
  year          = {2022}
}
@article{wei2022chain,
  title         = {Chain of thought prompting elicits reasoning in large language models},
  author        = {Wei, J. and Wang, X. and Schuurmans, D. and Bosma, M. and Chi, E. H. and Le, Q. and Zhou, D.},
  journal       = {CoRR},
  volume        = {abs/2201.11903},
  year          = {2022}
}
@misc{vaswani2023attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  note          = {v7}
}
@inproceedings{christiano2017deep,
  title         = {Deep reinforcement learning from human preferences},
  author        = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle     = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017},
  editor        = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year          = {2017},
  pages         = {4299--4307},
  address       = {Long Beach, CA, USA},
  organization  = {Curran Associates, Inc.},
  date          = {2017-12-04/2017-12-09}
}
@inproceedings{hendrycks2021measuring,
  title         = {Measuring Massive Multitask Language Understanding},
  author        = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year          = {2021}
}
@article{henighan2020scaling,
  title         = {Scaling Laws for Autoregressive Generative Modeling},
  author        = {Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B. and Dhariwal, Prafulla and Gray, Scott and others},
  journal       = {arXiv preprint arXiv:2010.14701},
  year          = {2020}
}
@misc{chen2021evaluating,
  title         = {Evaluating Large Language Models Trained on Code},
  author        = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year          = {2021},
  howpublished  = {arXiv preprint arXiv:2107.03374}
}
@misc{touvron2023llama,
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\'{e}e Lacroix and Baptiste Rozi\`{e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year          = {2023},
  eprint        = {2302.13971},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{shazeer2020glu,
  title         = {GLU Variants Improve Transformer},
  author        = {Shazeer, Noam},
  journal       = {arXiv preprint arXiv:2002.05202},
  year          = {2020}
}
@article{su2021roformer,
  title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author        = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal       = {arXiv preprint arXiv:2104.09864},
  year          = {2021}
}
@misc{gemma_google_ai,
  author        = {Jeanine Banks and Tris Warkentin},
  title         = {Gemma: Google introduces new state-of-the-art open models},
  howpublished  = {Google AI Blog},
  year          = {2024},
  url           = {https://blog.google/technology/developers/gemma-open-models/}
}
@misc{gemmateam2024gemma,
  title         = {Gemma: Open Models Based on Gemini Research and Technology},
  author        = {Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivi\`{e}re and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and L\'{e}onard Hussenot and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Am\'{e}lie H\'{e}liou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Cl\'{e}ment Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Miku\l{}a and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Pier Giuseppe Sessa and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Cl\'{e}ment Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
  year          = {2024},
  eprint        = {2403.08295},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{alsentzer2019publicly,
  title         = {Publicly available clinical BERT embeddings},
  author        = {Alsentzer, Emily and others},
  journal       = {arXiv preprint arXiv:1904.03323},
  year          = {2019}
}
@article{shickel2018deep,
  title         = {Deep EHR: A survey of recent advances in deep learning techniques for electronic health record (EHR) analysis},
  author        = {Shickel, Benjamin and others},
  journal       = {IEEE journal of biomedical and health informatics},
  volume        = {22},
  number        = {5},
  pages         = {1589--1604},
  year          = {2018},
  publisher     = {IEEE}
}
@article{zhavoronkov2019deep,
  title         = {Deep learning enables rapid identification of potent DDR1 kinase inhibitors},
  author        = {Zhavoronkov, Alex and others},
  journal       = {Nature Biotechnology},
  volume        = {37},
  pages         = {1038--1040},
  year          = {2019},
  doi           = {10.1038/d41573-019-00170-0}
}
@article{kocaballi2019personalization,
  title         = {The Personalization of Conversational Agents in Health Care: Systematic Review},
  author        = {Kocaballi, A. Baki and Berkovsky, Shlomo and Quiroz, Juan C. and Laranjo, Liliana and Tong, Huong Ly and Rezazadegan, Dana and others},
  journal       = {Journal of Medical Internet Research},
  volume        = {21},
  number        = {11},
  year          = {2019},
  doi           = {10.2196/15360},
  url           = {https://www.jmir.org/2019/11/e15360/}
}
@article{beam2018big,
  title         = {Big Data and Machine Learning in Health Care},
  author        = {Beam, Andrew L. and Kohane, Isaac S.},
  journal       = {JAMA},
  volume        = {319},
  number        = {13},
  pages         = {1317--1318},
  year          = {2018}
}
@article{chen2019single,
  title         = {Single-cell trajectories reconstruction, exploration and mapping of omics data with STREAM},
  author        = {Chen, H. and others},
  journal       = {Nature Communications},
  volume        = {10},
  number        = {1},
  pages         = {1903},
  year          = {2019},
  doi           = {10.1038/s41467-019-09670-4},
  url           = {https://www.nature.com/articles/s41467-019-09670-4}
}
@article{hamburg2010path,
  title         = {The Path to Personalized Medicine},
  author        = {Hamburg, M. A. and Collins, F. S.},
  journal       = {New England Journal of Medicine},
  volume        = {363},
  number        = {4},
  pages         = {301--304},
  year          = {2010},
  doi           = {10.1056/NEJMp1006304},
  url           = {https://sci-hub.se/10.1056/NEJMp1006304}
}
@article{buehler2018deep,
  title         = {Deep learning and algorithmic trading},
  author        = {Buehler, Hans and Gonon, Lukas and Teichmann, Josef and Wood, Ben},
  journal       = {Financial Markets and Portfolio Management},
  volume        = {32},
  number        = {3},
  pages         = {239--260},
  year          = {2018},
  publisher     = {Springer}
}
@article{li2020natural,
  title         = {Natural language processing in risk management and compliance},
  author        = {Li, Jin and Spangler, Scott and Yu, Yue},
  journal       = {Journal of Risk Management in Financial Institutions},
  volume        = {13},
  number        = {2},
  pages         = {158--175},
  year          = {2020},
  publisher     = {Henry Stewart Publications}
}
@article{pal2021enhancing,
  title         = {Enhancing customer service through AI-driven virtual assistants in the banking sector},
  author        = {Pal, Arpan and Kundu, Aniruddha and Chakraborty, Rajdeep},
  journal       = {Journal of Banking and Financial Technology},
  volume        = {5},
  number        = {1},
  pages         = {1--12},
  year          = {2021},
  publisher     = {Springer}
}
@article{smith2019improving,
  title         = {Improving fraud detection in financial services through deep learning},
  author        = {Smith, Timothy and Kumar, Manish},
  journal       = {Journal of Financial Crime},
  volume        = {26},
  number        = {4},
  pages         = {1062--1073},
  year          = {2019},
  publisher     = {Emerald Publishing Limited}
}
@article{jones2020ethical,
  title         = {Ethical considerations for AI in finance},
  author        = {Jones, Michael and Barn, Ravinder and Karim, Mohammed and Nurse, Jason R C},
  journal       = {AI \& Society},
  volume        = {35},
  number        = {1},
  pages         = {287--300},
  year          = {2020},
  publisher     = {Springer}
}
@article{zhang2021medical,
  title         = {Medical image analysis with artificial intelligence},
  author        = {Zhang, Jun and others},
  journal       = {IEEE Transactions on Biomedical Engineering},
  volume        = {68},
  number        = {5},
  pages         = {1375--1379},
  year          = {2021},
  publisher     = {IEEE}
}
@article{li2021survey,
  title         = {A survey on deep learning in medical image analysis},
  author        = {Li, Zhi and Zhang, Qiang and Dou, Qi and others},
  journal       = {Medical image analysis},
  volume        = {67},
  pages         = {101813},
  year          = {2021},
  publisher     = {Elsevier}
}
@article{singhal2022large,
  title         = {Large language models encode clinical knowledge},
  author        = {Singhal, Karan and Azizi, Sebti and Tu, Tony and Mahdavi, Seyed Sepehr and Wei, Jason and Chung, Han Wu and Scales, Nicholas and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen R. and others},
  journal       = {arXiv preprint arXiv:2212.13138},
  year          = {2022}
}
@article{wang2022self,
  title         = {Self-consistency improves chain of thought reasoning in language models},
  author        = {Wang, X. and Wei, J. and Schuurmans, D. and Le, Q. and Chi, E. and Zhou, D.},
  journal       = {arXiv preprint arXiv:2203.11171},
  year          = {2022}
}
@misc{wu2023bloomberggpt,
  title         = {BloombergGPT: A Large Language Model for Finance},
  author        = {Shijie Wu and Ozan Irsoy and Steven Lu and Vadim Dabravolski and Mark Dredze and Sebastian Gehrmann and Prabhanjan Kambadur and David Rosenberg and Gideon Mann},
  year          = {2023},
  eprint        = {2303.17564},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{luo2022biogpt,
  title         = {BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining},
  author        = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
  journal       = {Briefings in Bioinformatics},
  volume        = {23},
  number        = {6},
  year          = {2022},
  month         = {sep},
  doi           = {10.1093/bib/bbac409},
  url           = {https://doi.org/10.1093\%2Fbib\%2Fbbac409}
}
@misc{bolton2023biomedlm,
  title         = {BioMedLM},
  author        = {Bolton, Elliot and Hall, David and Yasunaga, Michihiro and Lee, Tony and Manning, Chris and Liang, Percy},
  year          = {2023},
  howpublished  = {\url{https://github.com/stanford-crfm/BioMedLM}}
}
@misc{taylor2022galactica,
  title         = {Galactica: A Large Language Model for Science},
  author        = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  year          = {2022},
  month         = {11},
  howpublished  = {\url{http://arxiv.org/abs/2211.09085}},
  note          = {arXiv:2211.09085}
}
@misc{liang2022holistic,
  title         = {Holistic Evaluation of Language Models},
  author        = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and R{\'e}, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel J. and Zheng, Lucia and Y{\"u}ksekg{\"o}n{\"u}l, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri S. and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  year          = {2022},
  journal       = {CoRR},
  volume        = {abs/2211.09110},
  doi           = {10.48550/arXiv.2211.09110},
  url           = {https://doi.org/10.48550/arXiv.2211.09110}
}
@misc{lee2024survey,
  title         = {A Survey of Large Language Models in Finance (FinLLMs)},
  author        = {Jean Lee and Nicholas Stevens and Soyeon Caren Han and Minseok Song},
  year          = {2024},
  eprint        = {2402.02315},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{xie2023pixiu,
  title         = {Pixiu: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance},
  author        = {Xie, Q. and Han, W. and Zhang, X. and Lai, Y. and Peng, M. and Lopez-Lira, A. and Huang, J.},
  booktitle     = {Proceedings of NeurIPS Datasets and Benchmarks},
  year          = {2023}
}
@article{yang2023investlm,
  title         = {InvestLM: A Large Language Model for Investment Using Financial Domain Instruction Tuning},
  author        = {Yang, Yi and Tang, Yixuan and Tam, Kar Yan},
  journal       = {arXiv preprint arXiv:2309.13064},
  year          = {2023}
}
@article{wang2023fingpt,
  title         = {FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},
  author        = {Wang, Neng and Yang, Hongyang and Wang, Christina Dan},
  journal       = {arXiv preprint arXiv:2309.13064},
  year          = {2023}
}
@article{malo2014good,
  title         = {Good debt or bad debt: Detecting semantic orientations in economic texts},
  author        = {Malo, Pekka and Sinha, Ankur and Korhonen, Pekka and Wallenius, Jyrki and Takala, Pyry},
  journal       = {JASIST},
  volume        = {65},
  number        = {4},
  pages         = {782--796},
  year          = {2014}
}
@article{maia2018www,
  title         = {WWW'18 Open Challenge: Financial Opinion Mining and Question Answering},
  author        = {Maia, Macedo and Handschuh, Siegfried and Freitas, Andr{\'e} and others},
  journal       = {Companion Proceedings of WWW},
  pages         = {1941--1942},
  year          = {2018}
}
@inproceedings{alvarado2015domain,
  title         = {Domain adaption of named entity recognition to support credit risk assessment},
  author        = {Alvarado, Julio Cesar Salinas and Verspoor, Karin and Baldwin, Timothy},
  booktitle     = {Proceedings of ALTA Workshop},
  pages         = {84--90},
  year          = {2015}
}
@article{loukas2022finer,
  title         = {Finer: Financial numeric entity recognition for xbrl tagging},
  author        = {Lefteris Loukas and Manos Fergadiotis and Ilias Chalkidis and Eirini Spyropoulou and others},
  journal       = {Proceedings of ACL},
  pages         = {4419--4431},
  year          = {2022}
}
@inproceedings{chen2022convfinqa,
  title         = {ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering},
  author        = {Chen, Zhiyu and Li, Shiyang and Smiley, Charese and Ma, Zhiqiang and Shah, Sameena and Wang, William Yang},
  booktitle     = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages         = {6279--6292},
  year          = {2022}
}
@inproceedings{xu2018stock,
  title         = {Stock movement prediction from tweets and historical prices},
  author        = {Xu, Yumo and Cohen, Shay B},
  booktitle     = {Proceedings of ACL},
  pages         = {1970--1979},
  year          = {2018}
}
@inproceedings{wu2018hybrid,
  title         = {Hybrid deep sequential modeling for social text-driven stock prediction},
  author        = {Wu, Huizhe and Zhang, Wei and Shen, Weiwei and Wang, Jun},
  booktitle     = {Proceedings of ACM CIKM},
  pages         = {1627--1630},
  year          = {2018}
}
@inproceedings{soun2022accurate,
  title         = {Accurate stock movement prediction with self-supervised learning from sparse noisy tweets},
  author        = {Soun, Yejun and Yoo, Jaemin and Cho, Minyong and Jeon, Jihyeong and Kang, U},
  booktitle     = {IEEE Big Data},
  pages         = {1691--1700},
  year          = {2022}
}
@inproceedings{sinha2021impact,
  title         = {Impact of news on the commodity market: Dataset and results},
  author        = {Sinha, Ankur and Khandait, Tanmay},
  booktitle     = {Proceedings of FICC},
  pages         = {589--601},
  year          = {2021}
}
@inproceedings{mukherjee2022ectsum,
  title         = {ECTSum: A New Benchmark Dataset for Bullet Point Summarization of Long Earnings Call Transcripts},
  author        = {Mukherjee, Rajdeep and Bohra, Abhinav and Banerjee, Akash and Sharma, Soumya and others},
  booktitle     = {Proceedings of EMNLP},
  pages         = {10893--10906},
  year          = {2022}
}
@inproceedings{sharma2022finred,
  title         = {Finred: A dataset for relation extraction in financial domain},
  author        = {Sharma, Soumya and Nayak, Tapas and Bose, Arusarka and Meena, Ajay Kumar and others},
  booktitle     = {Companion Proceedings of WWW},
  pages         = {595--597},
  year          = {2022}
}
@inproceedings{zhou2021trade,
  title         = {Trade the event: Corporate events detection for news-based event-driven trading},
  author        = {Zhou, Zhihan and Ma, Liqian and Liu, Han},
  booktitle     = {Findings of ACL-IJCNLP},
  pages         = {2114--2124},
  year          = {2021}
}
@inproceedings{mariko2020financial,
  title         = {The financial document causality detection shared task (fincausal 2020)},
  author        = {Mariko, Dominique and Akl, Hanna Abi and Labidurie, Estelle and others},
  booktitle     = {Proceedings of the Workshop on FNP-FNS},
  pages         = {23--32},
  year          = {2020}
}
@inproceedings{zheng2021global,
  title         = {Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context},
  author        = {Zheng, Xinyi and Burdick, Douglas and Popa, Lucian and Zhong, Xu and Wang, Nancy Xin Ru},
  booktitle     = {Proceedings of the IEEE/CVF WACV},
  pages         = {697--706},
  year          = {2021}
}
@inproceedings{li2020maec,
  title         = {MAEC: A Multimodal Aligned Earnings Conference Call Dataset for Financial Risk Prediction},
  author        = {Li, Jiazheng and Yang, Linyi and Smyth, Barry and Dong, Ruihai},
  booktitle     = {Proceedings of ACM CIKM},
  pages         = {3063--3070},
  year          = {2020}
}
@inproceedings{mathur2022monopoly,
  title         = {Monopoly: Financial prediction from monetary policy conference videos using multimodal cues},
  author        = {Mathur, Puneet and Neerkaje, Atula and Chhibber, Malika and Sawhney, Ramit and others},
  booktitle     = {Proceedings of ACM MM},
  pages         = {2276--2285},
  year          = {2022}
}
@inproceedings{gerz2021multilingual,
  title         = {Multilingual and cross-lingual intent detection from spoken data},
  author        = {Gerz, Daniela and Su, Pei-Hao and Kuszto, Razvan and Mondal, Avishek and Lis, Micha\l{} and others},
  booktitle     = {Proceedings of EMNLP},
  pages         = {7468--7475},
  year          = {2021}
}
@inproceedings{lee2023stockemotions,
  title         = {StockEmotions: Discover Investor Emotions for Financial Sentiment Analysis and Multivariate Time Series},
  author        = {Lee, Jean and Youn, Hoyoul Luis and Poon, Josiah and Han, Soyeon Caren},
  booktitle     = {AAAI-24 Bridge},
  year          = {2023}
}
@inproceedings{jorgensen2023multifin,
  title         = {MultiFin: A Dataset for Multilingual Financial NLP},
  author        = {J\o{}rgensen, Rasmus and Brandt, Oliver and Hartmann, Mareike and Dai, Xiang and Igel, Christian and Elliott, Desmond},
  booktitle     = {Findings of the European Chapter of the Association for Computational Linguistics (EACL)},
  pages         = {864--879},
  year          = {2023}
}
@misc{li2023chatgpt,
  title         = {Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks},
  author        = {Xianzhi Li and Samuel Chan and Xiaodan Zhu and Yulong Pei and Zhiqiang Ma and Xiaomo Liu and Sameena Shah},
  year          = {2023},
  eprint        = {2305.05862},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{chen2022afinqa,
  title         = {FinQA: A Dataset of Numerical Reasoning Over Financial Data},
  author        = {Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and Wang, William Yang},
  year          = {2022},
  note          = {Presumed publication year and citation style as "2022a", specifics such as journal name, volume, issue, pages, and DOI are not provided and should be added}
}
@misc{workshop2023bloom,
  title         = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author        = {BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ili\'{c} and Daniel Hesslow and Roman Castagn\'{e} and Alexandra Sasha Luccioni and Fran\c{c}ois Yvon and Matthias Gall\'{e} and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Beno\^{\i}t Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Lauren\c{c}on and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo Gonz\'{a}lez Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and G\'{e}rard Dupont and Germ\'{a}n Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and J\"{o}rg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Mu\~{n}oz and Maraim Masoud and Mar\'{\i}a Grandury and Mario \v{S}a\v{s}ko and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis L\'{o}pez and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Ta\c{s}ar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre Fran\c{c}ois Lavall\'{e}e and R\'{e}mi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and St\'{e}phane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aur\'{e}lie N\'{e}v\'{e}ol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zden\v{e}k Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Mu\~{n}oz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Cl\'{e}mentine Fourrier and Daniel Le\'{o}n Peri\~{n}\'{a}n and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc P\`{a}mies and Maria A Castillo and Marianna Nezhurina and Mario S\"{a}nger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Th\'{e}o Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
  year          = {2023},
  eprint        = {2211.05100},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{hendrycks2016gelu,
  title         = {Gaussian Error Linear Units (GELUs)},
  author        = {Hendrycks, Dan and Gimpel, Kevin},
  journal       = {arXiv preprint arXiv:1606.08415},
  year          = {2016}
}
@inproceedings{lescao2022milliongpuhours,
  title         = {What language model to train if you have one million GPU hours?},
  author        = {Le Scao, Teven and Wang, Thomas and Hesslow, Daniel and Bekman, Stas and Bari, M Saiful and Biderman, Stella and Elsahar, Hady and Muennighoff, Niklas and Phang, Jason and Press, Ofir and others},
  booktitle     = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages         = {765--782},
  year          = {2022},
  month         = {December},
  publisher     = {Association for Computational Linguistics},
  address       = {Abu Dhabi, United Arab Emirates},
  url           = {https://aclanthology.org/2022.findings-emnlp.54}
}
@article{malinka2023educationalimpact,
  title         = {On the educational impact of ChatGPT: Is artificial intelligence ready to obtain a university degree?},
  author        = {Malinka, K. and Peres{\'i}ni, M. and Firc, A. and Hujnak, O. and Janus, F.},
  journal       = {CoRR},
  volume        = {abs/2303.11146},
  year          = {2023},
  url           = {https://arxiv.org/abs/2303.11146}
}
@article{susnjak2022chatgpt,
  title         = {ChatGPT: The end of online exam integrity?},
  author        = {Susnjak, T.},
  journal       = {CoRR},
  volume        = {abs/2212.09292},
  year          = {2022},
  url           = {https://arxiv.org/abs/2212.09292}
}
@article{blairstanek2023gpt3statutory,
  author        = {Andrew Blair-Stanek and Nils Holzenberger and Benjamin Van Durme},
  title         = {Can {GPT-3} perform statutory reasoning?},
  journal       = {CoRR},
  volume        = {abs/2302.06100},
  year          = {2023},
  url           = {https://arxiv.org/abs/2302.06100},
  archiveprefix = {arXiv},
  eprint        = {2302.06100}
}
@article{trautmann2022legalprompt,
  author        = {Daniel Trautmann and Aleksandra Petrova and Frank Schilder},
  title         = {Legal prompt engineering for multilingual legal judgement prediction},
  journal       = {CoRR},
  volume        = {abs/2212.02199},
  year          = {2022},
  url           = {https://arxiv.org/abs/2212.02199},
  archiveprefix = {arXiv},
  eprint        = {2212.02199}
}
@article{choi2023chatgptlaw,
  author        = {Jinho H. Choi and Kristin E. Hickman and Andrew Monahan and Daniel Schwarcz},
  title         = {ChatGPT goes to law school},
  year          = {2023},
  howpublished  = {Available at SSRN},
  url           = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=number},
  note          = {Accessed: 2024-02-14}
}
@article{kojima2022largelanguagemodels,
  title         = {Large Language Models are Zero-Shot Reasoners},
  author        = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal       = {CoRR},
  volume        = {abs/2205.11916},
  year          = {2022},
  doi           = {10.48550/arXiv.2205.11916},
  url           = {https://doi.org/10.48550/arXiv.2205.11916}
}
@article{nay2022lawinformscode,
  title         = {Law informs code: A legal informatics approach to aligning artificial intelligence with humans},
  author        = {Nay, J. J.},
  journal       = {CoRR},
  volume        = {abs/2209.13020},
  year          = {2022},
  eprint        = {2209.13020},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CY},
  url           = {https://arxiv.org/abs/2209.13020}
}
@article{jin2019pubmedqa,
  title         = {PubMedQA: A Dataset for Biomedical Research Question Answering},
  author        = {Jin, Qingyu and Dhingra, Bhuwan and Liu, Zhengdong and Cohen, William W. and Lu, Xinghua},
  year          = 2019,
  journal       = {Proceedings of EMNLP-IJCNLP},
  pages         = {2567--2577}
}
@misc{krithara2022bioasq,
  title         = {BioASQ-QA: A manually curated corpus for biomedical question answering},
  author        = {Krithara, Anastasia and Nentidis, Anastasios and Bougiatiotis, Konstantinos and Paliouras, Georgios},
  year          = 2022
}
@article{lewkowycz2022minerva,
  title         = {Solving quantitative reasoning problems with language models},
  author        = {Lewkowycz, Aitor and others},
  year          = 2022,
  journal       = {CoRR},
  volume        = {abs/2206.14858}
}
@article{zhang2023smallstep,
  title         = {One small step for generative AI, one giant leap for AGI: A complete survey on ChatGPT in AIGC era},
  author        = {Zhang, C. and Zhang, C. and Li, C. and Qiao, Y. and Zheng, S. and Dam, S. K. and Zhang, M. and Kim, J. U. and Kim, S. T. and Choi, J. and Park, G. and Bae, S. and Lee, L. and Hui, P. and Kweon, I. S. and Hong, C. S.},
  journal       = {CoRR},
  volume        = {abs/2304.06488},
  year          = {2023},
  eprint        = {2304.06488},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2304.06488}
}
@article{haman2023usingchatgpt,
  title         = {Using ChatGPT to Conduct a Literature Review},
  author        = {Haman, Micha\l{} and Skolnik, Marcin},
  journal       = {Accountability in Research},
  year          = {2023},
  publisher     = {Taylor \& Francis}
}
@article{aydin2022openaichatgpt,
  title         = {OpenAI ChatGPT Generated Literature Review: Digital Twin in Healthcare},
  author        = {Ayd\i{}n, \"{O}\u{g}uzhan and Karaarslan, Emre},
  journal       = {SSRN Electronic Journal},
  year          = {2022},
  url           = {https://ssrn.com/abstract=number},
  note          = {Please replace "number" with the actual abstract number.}
}
@misc{park2023chatgpt,
  title         = {Can ChatGPT be Used to Generate Scientific Hypotheses?},
  author        = {Park, Yujin J. and others},
  year          = 2023
}
@article{hassan2023chatgptdatascientist,
  title         = {ChatGPT as Your Personal Data Scientist},
  author        = {Hassan, Md Mahadi and Knipper, Richard A. and Santu, Shakked K. K.},
  journal       = {CoRR},
  volume        = {abs/2305.13657},
  year          = {2023},
  eprint        = {2305.13657},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2305.13657}
}
@article{cheng2023gpt4dataanalyst,
  title         = {Is GPT-4 a Good Data Analyst?},
  author        = {Cheng, Long and Li, Xiang and Bing, Lidong},
  journal       = {CoRR},
  volume        = {abs/2305.15038},
  year          = {2023},
  eprint        = {2305.15038},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2305.15038}
}
@article{alkaissi2023artificialhallucinations,
  title         = {Artificial Hallucinations in ChatGPT: Implications in Scientific Writing},
  author        = {Hussam Alkaissi, S. I. M.},
  journal       = {PubMed},
  year          = {2023},
  note          = {Available on PubMed},
  url           = {https://pubmed.ncbi.nlm.nih.gov/ARTICLE\_ID}
}
@article{azaria2023chatgptexperts,
  title         = {ChatGPT is a Remarkable Tool – For Experts},
  author        = {Azaria, A. and Azoulay, R. and Reches, S.},
  journal       = {CoRR},
  volume        = {abs/2306.03102},
  year          = {2023},
  eprint        = {2306.03102},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2306.03102}
}
@article{buruk2023academicwriting,
  title         = {Academic Writing with GPT-3.5: Reflections on Practices, Efficacy and Transparency},
  author        = {Buruk, O. O.},
  journal       = {CoRR},
  volume        = {abs/2304.11079},
  year          = {2023},
  eprint        = {2304.11079},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2304.11079}
}
@article{liu2023reviewergpt,
  title         = {Reviewergpt? An Exploratory Study on Using Large Language Models for Paper Reviewing},
  author        = {Liu, R. and Shah, N. B.},
  journal       = {CoRR},
  volume        = {abs/2306.00622},
  year          = {2023},
  eprint        = {2306.00622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2306.00622}
}
@article{kosinski2023theoryofmind,
  title         = {Theory of Mind May Have Spontaneously Emerged in Large Language Models},
  author        = {Kosinski, M.},
  journal       = {CoRR},
  volume        = {abs/2302.02083},
  year          = {2023},
  eprint        = {2302.02083},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2302.02083}
}
@article{amin2023affectivecomputing,
  title         = {Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT},
  author        = {Amin, M. M. and Cambria, E. and Schuller, B. W.},
  journal       = {CoRR},
  volume        = {abs/2303.03186},
  year          = {2023},
  eprint        = {2303.03186},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2303.03186}
}
@article{sridhara2023chatgptsoftware,
  title         = {ChatGPT: A Study on Its Utility for Ubiquitous Software Engineering Tasks},
  author        = {Sridhara, G. and R. H. G. and Mazumdar, S.},
  journal       = {CoRR},
  volume        = {abs/2305.16837},
  year          = {2023},
  eprint        = {2305.16837},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2305.16837}
}
@article{sun2023code,
  title         = {Automatic Code Summarization via ChatGPT: How Far Are We?},
  author        = {Sun, W. and Fang, C. and You, Y. and Miao, Y. and Liu, Y. and Li, Y. and Deng, G. and Huang, S. and Chen, Y. and Zhang, Q. and Qian, H. and Liu, Y. and Chen, Z.},
  journal       = {CoRR},
  volume        = {abs/2305.12865},
  year          = {2023},
  eprint        = {2305.12865},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2305.12865}
}
@article{xia2023conversationalrepair,
  title         = {Conversational Automated Program Repair},
  author        = {Xia, C. S. and Zhang, L.},
  journal       = {CoRR},
  volume        = {abs/2301.13246},
  year          = {2023},
  eprint        = {2301.13246},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2301.13246}
}
@online{kim2022replacegrammarly,
  author        = {Sung Kim},
  title         = {Replace Grammarly Premium with OpenAI ChatGPT},
  year          = {2022},
  url           = {https://medium.com/geekculture/replace-grammarly-premium-with-openai-chatgpt-320049179c79}
}
@online{maximov2023englishgrammar,
  author        = {Lev Maximov},
  title         = {Do You Know English Grammar Better Than ChatGPT?},
  year          = {2023},
  url           = {https://medium.com/writing-cooperative/do-you-know-english-grammar-better-than-chatgpt-8fc550f23681}
}
@inproceedings{liu2019roberta,
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author        = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle     = {arXiv preprint arXiv:1907.11692},
  year          = {2019}
}
@inproceedings{strubell2019energy,
  title         = {Energy and Policy Considerations for Deep Learning in NLP},
  author        = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle     = {ACL 2019},
  year          = {2019}
}
@article{bender2021dangers,
  title         = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author        = {Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  journal       = {FAccT '21},
  year          = {2021}
}
@inproceedings{ruder2019transfer,
  title         = {Transfer Learning in Natural Language Processing},
  author        = {Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas},
  editor        = {Sarkar, Anoop and Strube, Michael},
  booktitle     = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  month         = jun,
  year          = {2019},
  address       = {Minneapolis, Minnesota},
  publisher     = {Association for Computational Linguistics},
  url           = {https://aclanthology.org/N19-5004},
  doi           = {10.18653/v1/N19-5004},
  pages         = {15--18},
  abstract      = {The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.}
}
@misc{gururangan2020don,
  title         = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author        = {Suchin Gururangan and Ana Marasovi\'{c} and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
  year          = {2020},
  eprint        = {2004.10964},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{phang2019sentence,
  title         = {Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks},
  author        = {Jason Phang and Thibault F\'{e}vry and Samuel R. Bowman},
  year          = {2019},
  eprint        = {1811.01088},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{howard2018universal,
  title         = {Universal Language Model Fine-tuning for Text Classification},
  author        = {Jeremy Howard and Sebastian Ruder},
  year          = {2018},
  eprint        = {1801.06146},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@book{zhu2005semi,
  title         = {Semi-supervised Learning Literature Survey},
  author        = {Zhu, Xiaojin},
  year          = {2005},
  publisher     = {University of Wisconsin-Madison Department of Computer Sciences}
}
@book{chapelle2009semi,
  title         = {Semi-supervised Learning},
  author        = {Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
  year          = {2009},
  publisher     = {MIT Press}
}
@misc{yang2017transfer,
  title         = {Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks},
  author        = {Zhilin Yang and Ruslan Salakhutdinov and William W. Cohen},
  year          = {2017},
  eprint        = {1703.06345},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{bergmann2023semi,
  author        = {Dave Bergmann},
  title         = {What Is Semi-Supervised Learning?},
  year          = {2023},
  url           = {https://www.ibm.com/cloud/learn/semi-supervised-learning},
  urldate       = {2023-12-12},
  note          = {IBM}
}
@article{lee2013pseudo,
  title         = {Pseudo-Label: The Simple and Efficient Semi-supervised Learning Method for Deep Neural Networks},
  author        = {Lee, Dong-Hyun},
  journal       = {ICML 2013 Workshop: Challenges in Representation Learning (WREPL)},
  year          = {2013}
}
@inproceedings{sajjadi2016regularization,
  title         = {Regularization with stochastic transformations and perturbations for deep semi-supervised learning},
  author        = {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
  booktitle     = {Advances in neural information processing systems},
  pages         = {1163--1171},
  year          = {2016}
}
@book{vapnik1998statistical,
  title         = {Statistical Learning Theory},
  author        = {Vapnik, Vladimir},
  year          = {1998},
  publisher     = {Wiley-Interscience}
}
@inproceedings{joachims1999transductive,
  title         = {Transductive inference for text classification using support vector machines},
  author        = {Joachims, Thorsten},
  booktitle     = {ICML},
  year          = {1999},
  organization  = {Citeseer}
}
@book{mitchell1997machine,
  title         = {Machine Learning},
  author        = {Mitchell, Tom M.},
  year          = {1997},
  publisher     = {McGraw-Hill}
}
@inproceedings{zhou2004learning,
  title         = {Learning with unlabeled data and its application to image retrieval},
  author        = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas Navin and Weston, Jason and Scholkopf, Bernhard},
  booktitle     = {Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining},
  year          = {2004},
  organization  = {ACM}
}
@inproceedings{belkin2006manifold,
  title         = {Manifold regularization: A geometric framework for learning from labeled and unlabeled examples},
  author        = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  booktitle     = {Journal of machine learning research},
  year          = {2006},
  organization  = {MIT Press}
}
@inproceedings{zhu2015aligning,
  title         = {Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author        = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard S. and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle     = {2015 IEEE International Conference on Computer Vision (ICCV)},
  pages         = {19--27},
  year          = {2015},
  organization  = {IEEE Computer Society},
  address       = {Santiago, Chile},
  month         = {dec},
  doi           = {10.1109/ICCV.2015.10}
}
@misc{projectgutenberg,
  title         = {Project Gutenberg},
  howpublished  = {\url{https://www.gutenberg.org/}},
  note          = {Accessed: 2024-04-14}
}
@article{trinh2018simple,
  title         = {A Simple Method for Commonsense Reasoning},
  author        = {Trinh, Trieu H. and Le, Quoc V.},
  journal       = {CoRR},
  volume        = {abs/1806.02847},
  year          = {2018},
  eprint        = {1806.02847},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@inproceedings{zellers2019defending,
  title         = {Defending Against Neural Fake News},
  author        = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle     = {Advances in Neural Information Processing Systems 32},
  editor        = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alch{\'e}-Buc, Florence and Fox, Emily B. and Garnett, Roman},
  pages         = {9051--9062},
  year          = {2019},
  address       = {Vancouver, BC, Canada},
  publisher     = {NeurIPS},
  note          = {NeurIPS 2019, December 8-14}
}
@misc{gokaslan2019openwebtext,
  author        = {Gokaslan, Aaron and Pavlick, Ellie and Tellex, Stefanie},
  title         = {OpenWebText Corpus},
  year          = {2019},
  howpublished  = {\url{http://Skylion007.github.io/OpenWebTextCorpus}}
}
@inproceedings{baumgartner2020pushshift,
  title         = {The Pushshift Reddit Dataset},
  author        = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  booktitle     = {Proceedings of the Fourteenth International AAAI Conference on Web and Social Media},
  pages         = {830--839},
  year          = {2020},
  publisher     = {AAAI Press},
  address       = {Atlanta, Georgia, USA},
  note          = {ICWSM 2020, Held Virtually}
}
@misc{wikipedia,
  title         = {Wikipedia},
  howpublished  = {\url{https://en.wikipedia.org/wiki/Main\_Page}},
  note          = {Accessed: 2024-04-14}
}
@misc{bigquerydataset,
  title         = {BigQuery Dataset},
  howpublished  = {\url{https://cloud.google.com/bigquery?hl=zh-cn}},
  note          = {Accessed: 2024-04-14}
}
@article{gao2021pile,
  title         = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author        = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal       = {CoRR},
  volume        = {abs/2101.00027},
  year          = {2021},
  eprint        = {2101.00027},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{laurencon2022bigscience,
  title         = {The BigScience ROOTS Corpus: A 1.6 TB Composite Multilingual Dataset},
  author        = {Lauren\c{c}on, Herv{\'e} and Saulnier, Thibault and Wang, Teven and Akiki, Colin and del Moral, Angelina V. and Le Scao, Teven and Von Werra, Leandro and Mou, Charlie and Ponferrada, E. G. and Nguyen, Hang et al.},
  booktitle     = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year          = {2022},
  organization  = {NeurIPS}
}
@misc{commoncrawl,
  title         = {Common Crawl},
  howpublished  = {\url{https://commoncrawl.org/}},
  note          = {Accessed: 2024-04-15}
}
@misc{radford2018improving,
  title         = {Improving Language Understanding by Generative Pre-training},
  author        = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year          = {2018},
  note          = {Available online}
}
@article{nijkamp2022codegen,
  title         = {CodeGen: An Open Large Language Model for Code with Multi-turn Program Synthesis},
  author        = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal       = {arXiv preprint arXiv:2203.13474},
  year          = {2022}
}
@article{smith2022deepspeed,
  title         = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  author        = {Smith, Samyam and Patwary, Mostofa and Norick, Bryan and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhen and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zheng, Emily and Child, Rewon and Aminabadi, Raul Y. and Bernauer, James and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Shital and Catanzaro, Bryan},
  journal       = {CoRR},
  volume        = {abs/2201.11990},
  year          = {2022}
}
@article{zhang2022opt,
  title         = {OPT: open pre-trained transformer language models},
  author        = {Zhang, Sheng and Roller, Stephen and Goyal, Nitish and Artetxe, Mikel and Chen, Mingda and Chen, Shiyue and Dewan, Chandan and Diab, Mona T. and Li, Xiang and Lin, Xiang Vincent and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Konstantin and Simig, Deniz and Koura, Paul S. and Sridhar, Anirudh and Wang, Tianyi and Zettlemoyer, Luke},
  journal       = {CoRR},
  volume        = {abs/2205.01068},
  year          = {2022}
}
@article{chowdhery2022palm,
  title         = {PaLM: Scaling Language Modeling with Pathways},
  author        = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhik and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jamie and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Trevor and Levskaya, Anna and Ghemawat, Sanjay and Dev, Sharan and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kensen and Fedus, Liam and Zhou, Daisy and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexey and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Tharun S. and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zihang and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeffrey and Petrov, Slav and Fiedel, Noah},
  journal       = {CoRR},
  volume        = {abs/2204.02311},
  year          = {2022}
}
@article{austin2021program,
  title         = {Program synthesis with large language models},
  author        = {Austin, James and Odena, Augustus and Nye, Maxwell I. and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Chris J. and Terry, Michael and Le, Quoc V. and Sutton, Charles},
  journal       = {CoRR},
  volume        = {abs/2108.07732},
  year          = {2021}
}
@article{li2022competition,
  title         = {Competition-level code generation with AlphaCode},
  author        = {Li, Y. and Choi, D. H. and Chung, J. and Kushman, N. and Schrittwieser, J. and Leblond, R. and Eccles, T. and Keeling, J. and Gimeno, F. and Lago, A. D. and others},
  journal       = {Science},
  year          = {2022}
}
@inproceedings{xu2022systematic,
  title         = {A Systematic Evaluation of Large Language Models of Code},
  author        = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J.},
  booktitle     = {MAPS\@PLDI},
  year          = {2022}
}
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoff- mann, H. F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. M. Jayakumar, E. Buchatskaya, D. Budden, E. Suther- land, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sotti- aux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d’Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. J. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, “Scaling language models: Methods, analysis & insights from training gopher,” CoRR, vol. abs/2112.11446, 2021
@article{rae2021scaling,
  title         = {Scaling language models: Methods, analysis \& insights from training Gopher},
  author        = {Rae, Jack W. and Borgeaud, Samuel and Cai, Tom and Millican, Kevin and Hoffmann, Jannis and Song, H. Francis and Aslanides, John and Henderson, Scott and Ring, Robert and Young, Stephen and Rutherford, Edward and Hennigan, Tom and Menick, Jacob and Cassirer, Alex and Powell, Robert and van den Driessche, George and Hendricks, Luke A. and Rauh, Michael and Huang, Peter and Glaese, Alexander and Welbl, Johannes and Dathathri, Sumanth and Huang, Sharan and Uesato, Jonathan and Mellor, Joe and Higgins, Iain and Creswell, Antonia and McAleese, Neil and Wu, Andrew and Elsen, Erich and Jayakumar, Siddhant M. and Buchatskaya, Elena and Budden, David and Sutherland, Ewan and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, James and Li, X. L. and Kuncoro, Adji B. and Nematzadeh, Azalia and Gribovskaya, Daria and Donato, Davide and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean and Tsimpoukelli, Maria and Grigorev, Nikita and Fritz, David and Sottiaux, Thibaut and Pajarskas, Titas and Pohlen, Thomas and Gong, Zhi and Toyama, Daniel and de Masson d'Autume, Charles and Li, Yutong and Terzi, Tugrul and Mikulik, Vojtech and Babuschkin, Igor and Clark, Andrew and de Las Casas, David and Guy, Alex and Jones, Chris and Bradbury, James and Johnson, Michael J. and Hechtman, Ben A. and Weidinger, Luke and Gabriel, Ilya and Isaac, William S. and Lockhart, Edward and Osindero, Simon and Rimell, Luke and Dyer, Chris and Vinyals, Oriol and Ayoub, Karim and Stanway, Jonathan and Bennett, Luke and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  journal       = {CoRR},
  volume        = {abs/2112.11446},
  year          = {2021},
  eprint        = {2112.11446},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{longpre2023pretrainer,
  title         = {A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity},
  author        = {Longpre, Scott and Yauney, Greg and Reif, Emily and Lee, Kaitao and Roberts, Adam and Zoph, Dan and Zhou, Dianqi and Wei, Jason and Robinson, Kyle and Mimno, David and others},
  journal       = {arXiv preprint arXiv:2305.13169},
  year          = {2023}
}
@article{chen2023datajuicer,
  title         = {Data-Juicer: A One-Stop Data Processing System for Large Language Models},
  author        = {Chen, Dong and Huang, Yuxuan and Ma, Zhiyi and Chen, Hao and Pan, Xinyu and Ge, Cheng and Gao, Dong and Xie, Yuxuan and Liu, Zhiyuan and Gao, Jianfeng and Li, Yizhong and Ding, Bo and Zhou, Jie},
  journal       = {arXiv preprint arXiv:2305.13169},
  year          = {2023}
}
@inproceedings{du2022glam,
  title         = {GLAM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author        = {Du, Nan and Huang, Yuxuan and Dai, Andrew M. and Tong, Shiyu and Lepikhin, Dmitry and Xu, Yanshuai and Krikun, Maxim and Zhou, Yuxiong and Yu, Andrew W. and Firat, Orhan and Zoph, Barret and Fedus, William and Bosma, Maarten P. and Zhou, Zhi and Wang, Tianyi and Wang, Yifan E. and Webster, Kevin and Pellat, Marie and Robinson, Kyle and Meier-Hellstern, Kathy S. and Duke, Trevor and Dixon, Luke and Zhang, Kevin and Le, Quoc V. and Wu, Yuxiong and Chen, Zhifeng and Cui, Can},
  booktitle     = {International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA},
  year          = {2022},
  pages         = {5547--5569}
}
@misc{huggingface2023perplexity,
  title         = {Perplexity - Transformers},
  url           = {https://huggingface.co/docs/transformers/perplexity},
  note          = {Accessed: 2024-04-06},
  organization  = {Hugging Face},
  year          = {2023}
}
@article{hernandez2022scaling,
  title         = {Scaling laws and interpretability of learning from repeated data},
  author        = {Hernandez, Daniel and Brown, Tom B. and Conerly, Thomas and DasSarma, Gaurav and Drain, Daniel and Showk, Sina Ehsani and Elhage, Nabil and Hatfield-Dodds, Luke and Henighan, Tom and Hume, Thomas and Johnston, Scott and Mann, Benjamin and Olah, Chris and Olsson, Carl and Amodei, Dario and Joseph, Nicholas and Kaplan, Jared and McCandlish, Sam},
  journal       = {CoRR},
  volume        = {abs/2205.10487},
  year          = {2022},
  eprint        = {2205.10487},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{holzman2020curious,
  title         = {The Curious Case of Neural Text Degeneration},
  author        = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal       = {8th International Conference on Learning Representations, ICLR 2020},
  year          = {2020},
  note          = {OpenReview.net},
  url           = {https://openreview.net/forum?id=rygGQyrFvH}
}
@inproceedings{lee2022deduplicating,
  title         = {Deduplicating training data makes language models better},
  author        = {Lee, Kenton and Ippolito, Daniel and Nystrom, Daniel and Zhang, Chris and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle     = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022},
  year          = {2022},
  pages         = {8424--8445}
}
@article{carlini2022quantifying,
  title         = {Quantifying memorization across neural language models},
  author        = {Carlini, Nicholas and Ippolito, Daniel and Jagielski, Marcin and Lee, Kenton and Tram\`{e}r, Florian and Zhang, Chris},
  journal       = {CoRR},
  volume        = {abs/2202.12488},
  year          = {2022},
  eprint        = {2202.12488},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{carlini2021extracting,
  title         = {Extracting training data from large language models},
  author        = {Carlini, Nicholas and Tram\`{e}r, Florian and Wallace, Eric and Jagielski, Marcin and Herbert-Voss, Andreas and Lee, Kenton and Roberts, Adam and Brown, Tom B. and Song, Dawn and Erlingsson, \'{U}lfar and Oprea, Alina and Raffel, Colin},
  booktitle     = {30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021},
  year          = {2021},
  pages         = {2633--2650}
}
@inproceedings{lafferty2001conditional,
  title         = {Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
  author        = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
  booktitle     = {Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001)},
  editor        = {Brodley, Carla E. and Danyluk, Andrea P.},
  pages         = {282--289},
  year          = {2001},
  address       = {Williams College, Williamstown, MA, USA},
  publisher     = {Morgan Kaufmann},
  month         = jun
}
@inproceedings{sennrich2016neural,
  title         = {Neural machine translation of rare words with subword units},
  author        = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle     = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers},
  year          = {2016},
  publisher     = {The Association for Computer Linguistics}
}
@inproceedings{kudo2018sentencepiece,
  title         = {Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author        = {Kudo, Taku and Richardson, John},
  booktitle     = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations},
  editor        = {Blanco, Eduardo and Lu, Wei},
  year          = {2018},
  address       = {Brussels, Belgium},
  publisher     = {Association for Computational Linguistics},
  month         = oct
}
@article{wu2016google,
  title         = {Google's Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation},
  author        = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Lukasz and Gouws, Stephan and Kato, Yoshiki and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeff},
  journal       = {CoRR},
  volume        = {abs/1609.08144},
  year          = {2016},
  url           = {http://arxiv.org/abs/1609.08144},
  eprint        = {1609.08144},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{lewis2020bart,
  title         = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author        = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle     = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages         = {7871--7880},
  year          = {2020},
  organization  = {Association for Computational Linguistics},
  url           = {https://www.aclweb.org/anthology/2020.acl-main.703}
}
@misc{li2021prefixtuning,
  title         = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author        = {Xiang Lisa Li and Percy Liang},
  year          = {2021},
  eprint        = {2101.00190},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{zhang2022examining,
  title         = {Examining scaling and transfer of language model architectures for machine translation},
  author        = {Zhang, Biao and Ghorbani, Amir and Bapna, Ankur and Cheng, Yuxiang and Garcia, Xavier and Shen, Jonathan and Firat, Orhan},
  booktitle     = {International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA},
  year          = {2022},
  pages         = {26176--26192}
}
@inproceedings{dong2019unified,
  title         = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
  author        = {Dong, Li and Yang, Nan and Wang, Wei and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  booktitle     = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  year          = {2019},
  pages         = {13042--13054}
}
@article{li2022ptuning,
  title         = {P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author        = {Li, Xiang Lisa and Liang, Percy},
  journal       = {CoRR},
  volume        = {abs/2202.12108},
  year          = {2022},
  eprint        = {2202.12108},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{raschka2023encoderdecoder,
  author        = {Raschka, Sebastian},
  title         = {Understanding Encoder and Decoder},
  year          = {2023},
  url           = {https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder},
  urldate       = {2024-04-13}
}
@article{zeng2021pangu,
  title         = {Pangu-$\alpha$: Large-scale autoregressive pretrained Chinese language models with auto-parallel computation},
  author        = {Zeng, Weihua and Ren, Xiang and Su, Tao and Wang, Haoyuan and Liao, Yuxuan and Wang, Zhiyuan and Jiang, Xiaowei and Yang, Zhi and Wang, Kai and Zhang, Xiaodong and Li, Chen and Gong, Zhe and Yao, Yuxian and Huang, Xiaodan and Wang, Jie and Yu, Jinsong and Guo, Qipeng and Yu, Yuxian and Zhang, Yuxian and Wang, Jie and Tao, Haoyu and Yan, Da and Yi, Zhi and Peng, Fei and Jiang, Fan and Zhang, Huan and Deng, Li and Zhang, Yizhong and Lin, Zhiyuan and Zhang, Chen and Zhang, Shuai and Guo, Ming and Gu, Sheng and Fan, Guotong and Wang, Yuxuan and Jin, Xiaodong and Liu, Qun and Tian, Ying},
  journal       = {CoRR},
  volume        = {abs/2104.12369},
  year          = {2021},
  eprint        = {2104.12369},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{thoppilan2022lamda,
  title         = {LaMDA: Language Models for Dialog Applications},
  author        = {Thoppilan, Romal and Freitas, Daniel De and Hall, Jason and Shazeer, Noam and Kulshreshtha, Abhishek and Cheng, Hongrae and Jin, Annie and Bos, Timothee and Baker, Lucy and Du, Yuan and Li, Yifeng and Lee, Hyung Won and Zheng, Henry Shu and Ghafouri, Ammar and Menegali, Matheus and Huang, Yuhui and Krikun, Maxim and Lepikhin, Dmitry and Qin, Jiahui and Chen, Dehao and Xu, Yexin and Chen, Zhifeng and Roberts, Adam and Bosma, Martin and Zhou, Yonghui and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marcus and Meier-Hellstern, Katherine S. and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Romulo Drummond and Duke, Trevor and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Meredith and Hutchinson, Ben and Olson, Kristen and Molina, Adam and Hoffman-John, Elizabeth and Lee, Jennifer and Aroyo, Lora and Rajakumar, Rajesh and Butryna, Anna and Lamm, Matthew and Kuzmina, Valentina and Fenton, Josh and Cohen, Alon and Bernstein, Robert and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Clara and Croak, Marian and Chi, Ed H. and Le, Quoc},
  journal       = {CoRR},
  volume        = {abs/2201.08239},
  year          = {2022},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  eprint        = {2201.08239}
}
@article{lieber2021jurassic,
  title         = {Jurassic-1: Technical details and evaluation},
  author        = {Lieber, Or and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal       = {White Paper. AI21 Labs},
  volume        = {1},
  year          = {2021}
}
@article{touvron2023llama2,
  title         = {LLaMA 2: Open Foundation and Fine-Tuned Chat Models},
  author        = {Touvron, Hugo and Martin, Ludovic and Stone, Kevin and Albert, Paul and Alma- hairi, Antoine and Babaei, Yashar and Bashlykov, Nikita and Batra, Saurabh and Bhargava, Pranav and Bhosale, Shubham and others},
  journal       = {arXiv preprint arXiv:2307.09288},
  year          = {2023}
}
@online{penedo2023refinedweb,
  title         = {The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only},
  author        = {Penedo, Gerardo and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Rares and Cappelli, Andrea and Alobeidli, Hamad and Pannier, Benjamin and Almazrouei, Eisa and Launay, Julien},
  year          = {2023},
  eprint        = {2306.01116},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{zeng2022glm130b,
  title         = {GLM-130B: An Open Bilingual Pre-trained Model},
  author        = {Zeng, Ailing and Liu, Xinxin and Du, Zihan and Wang, Zhiwei and Lai, Heng and Ding, Ming and Yang, Zitao and Xu, Yixuan and Zheng, Weijie and Xia, Xue and Tam, Wai Lam and Ma, Zhiyuan and Xue, Yixiao and Zhai, Junfeng and Chen, Wei and Zhang, Peng and Dong, Yan and Tang, Jie},
  year          = {2022},
  eprint        = {2210.02414},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{bahdanau2014neural,
  title         = {Neural machine translation by jointly learning to align and translate},
  author        = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal       = {CoRR},
  volume        = {abs/1409.0473},
  year          = {2014},
  eprint        = {1409.0473},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{britz2017massive,
  title         = {Massive exploration of neural machine translation architectures},
  author        = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc V.},
  journal       = {CoRR},
  volume        = {abs/1703.03906},
  year          = {2017},
  eprint        = {1703.03906},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{gu2022efficiently,
  title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
  author        = {Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle     = {The Tenth International Conference on Learning Representations},
  year          = {2022},
  url           = {https://openreview.net/forum?id=uYLFoz1vlAC},
  note          = {Accessed: 2024-04-13}
}
@article{mehta2022long,
  title         = {Long Range Language Modeling via Gated State Spaces},
  author        = {Mehta, Hrushikesh and Gupta, Ankush and Cutkosky, Ashok and Neyshabur, Behnam},
  journal       = {CoRR},
  volume        = {abs/2206.13947},
  year          = {2022},
  doi           = {10.48550/arXiv.2206.13947},
  url           = {https://doi.org/10.48550/arXiv.2206.13947}
}
@article{dao2022hungry,
  title         = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author        = {Dao, Tri and Fu, Daniel Y. and Saab, Khaled K. and Thomas, Albert W. and Rudra, Atri and R{\'e}, Christopher},
  journal       = {CoRR},
  volume        = {abs/2212.14052},
  year          = {2022},
  doi           = {10.48550/arXiv.2212.14052},
  url           = {https://doi.org/10.48550/arXiv.2212.14052}
}
@inproceedings{poli2023hyena,
  title         = {Hyena hierarchy: Towards larger convolutional language models},
  author        = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  booktitle     = {ICML},
  year          = {2023}
}
@article{peng2023rwkv,
  title         = {RWKV: Reinventing RNNs for the Transformer Era},
  author        = {Peng, Bin and Alcaide, Eduardo and Anthony, Quincy and Albalak, Aaron and Arcadinho, Sofia and Cao, Hui and Cheng, Xiaohu and Chung, Minjun and Grella, Matt and G.V., Krishna Kishore and He, Xiang and Hou, Han and Kazienko, Przemys{\l}aw and Kocon, Jan and Kong, Ji and Koptyra, Bartosz and Lau, Hok Shing and Mantri, Krishna Sai Inkoolu and Mom, Fabian and Saito, Akira and Tang, Xiao and Wang, Bing and Wind, John Sebastian and Wozniak, Stanis{\l}aw and Zhang, Rui and Zhang, Zhen and Zhao, Qian and Zhou, Ping and Zhu, Jun and Zhu, Rong},
  journal       = {CoRR},
  volume        = {abs/2305.13048},
  year          = {2023},
  url           = {https://doi.org/10.48550/arXiv.2305.13048},
  doi           = {10.48550/arXiv.2305.13048}
}
@article{sun2023retentive,
  title         = {Retentive Network: A Successor to Transformer for Large Language Models},
  author        = {Sun, Yi and Dong, Lei and Huang, Shuming and Ma, Shujie and Xia, Ying and Xue, Jingjing and Wang, Jin and Wei, Furu},
  journal       = {CoRR},
  volume        = {abs/2307.08621},
  year          = {2023},
  eprint        = {2307.08621},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2307.08621}
}
@inproceedings{ding2021cogview,
  title         = {CogView: Mastering Text-to-Image Generation via Transformers},
  author        = {Ding, Ming and Yang, Zizhao and Hong, Wei and Zheng, Weijie and Zhou, Cheng and Yin, Dong and Lin, Jie and Zou, Xiaochun and Shao, Zhiyang and Yang, Hui and Tang, Jie},
  booktitle     = {Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual},
  year          = {2021},
  pages         = {19822--19835}
}
@article{ba2016layer,
  title         = {Layer normalization},
  author        = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  journal       = {CoRR},
  volume        = {abs/1607.06450},
  year          = {2016},
  eprint        = {1607.06450},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{zhang2019root,
  title         = {Root Mean Square Layer Normalization},
  author        = {Zhang, Biao and Sennrich, Rico},
  booktitle     = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  year          = {2019},
  pages         = {12360--12371}
}
@article{wang2022deepnet,
  title         = {DeepNet: Scaling Transformers to 1,000 Layers},
  author        = {Wang, Haoyuan and Ma, Shujie and Dong, Lei and Huang, Shuming and Zhang, Dong and Wei, Furu},
  journal       = {CoRR},
  volume        = {abs/2203.00555},
  year          = {2022},
  eprint        = {2203.00555},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{nair2010rectified,
  title         = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  author        = {Nair, Vinod and Hinton, Geoffrey E.},
  booktitle     = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  year          = {2010},
  pages         = {807--814}
}
@inproceedings{wang2018glue,
  title         = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author        = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  booktitle     = {Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP\@EMNLP 2018, Brussels, Belgium, November 1, 2018},
  editor        = {Linzen, Tal and Chrupala, Grzegorz and Alishahi, Afra},
  publisher     = {Association for Computational Linguistics},
  year          = {2018},
  pages         = {353--355}
}
@article{ramachandran2017searching,
  title         = {Searching for activation functions},
  author        = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  journal       = {arXiv preprint arXiv:1710.05941},
  year          = {2017}
}
@article{ioffe2015batch,
  title         = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author        = {Ioffe, Sergey and Szegedy, Christian},
  journal       = {CoRR},
  volume        = {abs/1502.03167},
  year          = {2015},
  eprint        = {1502.03167},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{narang2021transformer,
  title         = {Do Transformer Modifications Transfer Across Implementations and Applications?},
  author        = {Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, William and F{\'e}vry, Thibault and Matena, Michael and Malkan, Kellie and Fiedel, Nick and Shazeer, Noam and Lan, Zhenzhong and Zhou, Yuxiang and Li, Wei and Ding, Ning and Marcus, Joshua and Roberts, Adam and Raffel, Colin},
  booktitle     = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021},
  year          = {2021},
  pages         = {5758--5773}
}
@inproceedings{xiong2020layer,
  title         = {On Layer Normalization in the Transformer Architecture},
  author        = {Xiong, Ruibo and Yang, Yuxian and He, Di and Zheng, Kai and Zheng, Shizhen and Xing, Chen and Zhang, Hui and Lan, Yanyan and Wang, Lu and Liu, Tie-Yan},
  booktitle     = {ICML},
  year          = {2020}
}
@inproceedings{baevski2019adaptive,
  title         = {Adaptive Input Representations for Neural Language Modeling},
  author        = {Baevski, Alexei and Auli, Michael},
  booktitle     = {7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019},
  year          = {2019},
  note          = {OpenReview.net}
}
@inproceedings{liu2020understanding,
  title         = {Understanding the difficulty of training transformers},
  author        = {Liu, Lizi and Liu, Xiang and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  booktitle     = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020},
  year          = {2020},
  pages         = {5747--5763}
}
@article{he2016deep,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal       = {CoRR},
  volume        = {abs/1512.03385},
  year          = {2016},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{glorot2011deep,
  title         = {Deep Sparse Rectifier Neural Networks},
  author        = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  journal       = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011},
  year          = {2011}
}
@article{maas2013rectifier,
  title         = {Rectifier nonlinearities improve neural network acoustic models},
  author        = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
  journal       = {CoRR},
  volume        = {abs/1312.6026},
  year          = {2013},
  eprint        = {1312.6026},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{hendrycks2016gaussian,
  title         = {Gaussian Error Linear Units (GELUs)},
  author        = {Hendrycks, Dan and Gimpel, Kevin},
  journal       = {arXiv preprint arXiv:1606.08415},
  year          = {2016}
}
@inproceedings{press2022train,
  title         = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author        = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
  booktitle     = {The Tenth International Conference on Learning Representations},
  year          = {2022},
  url           = {https://openreview.net/forum?id=JZJ9Zz1vZ6},
  note          = {Accessed: 2024-04-13}
}
@article{shaw2018self,
  title         = {Self-attention with relative position representations},
  author        = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal       = {CoRR},
  volume        = {abs/1803.02155},
  year          = {2018},
  eprint        = {1803.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{peng2021random,
  title         = {Random Feature Attention},
  author        = {Peng, Baolin and Li, Xiang and Liang, Percy},
  journal       = {CoRR},
  volume        = {abs/2106.14448},
  year          = {2021},
  eprint        = {2106.14448},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{zaheer2020big,
  title         = {Big Bird: Transformers for Longer Sequences},
  author        = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kunal and Ainslie, Jonathon and Alberti, Chris and Ontan\~o\'n, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle     = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual},
  year          = {2020}
}
@article{child2019generating,
  title         = {Generating Long Sequences with Sparse Transformers},
  author        = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal       = {CoRR},
  volume        = {abs/1904.10509},
  year          = {2019},
  eprint        = {1904.10509},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{shazeer2019fast,
  title         = {Fast Transformer Decoding: One Write-Head is All You Need},
  author        = {Shazeer, Noam},
  journal       = {CoRR},
  volume        = {abs/1911.02150},
  year          = {2019},
  url           = {http://arxiv.org/abs/1911.02150},
  eprint        = {1911.02150},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{li2023starcoder,
  title         = {StarCoder: may the source be with you!},
  author        = {Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and Jo\~{a}o Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Mu\~{n}oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
  year          = {2023},
  eprint        = {2305.06161},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{ainslie2023gqa,
  title         = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author        = {Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr\'{o}n and Sumit Sanghai},
  year          = {2023},
  eprint        = {2305.13245},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{dao2022flashattention,
  title         = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author        = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher R\'{e}},
  year          = {2022},
  eprint        = {2205.14135},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{liu2022fast,
  title         = {Fast and Memory-Efficient Attention with FlashAttention-2},
  author        = {Liu, Lizi and Liu, Xiang and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal       = {CoRR},
  volume        = {abs/2205.14135},
  year          = {2022},
  eprint        = {2205.14135},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{vllm2023,
  title         = {vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention},
  howpublished  = {Available online},
  year          = {2023},
  url           = {https://vllm.ai/}
}
@article{tay2021longrange,
  title         = {Long Range Arena: A Benchmark for Efficient Transformers},
  author        = {Tay, Yi and Baevski, Alexei and Fan, Angela and Nogueira, Rodrigo and Auli, Michael},
  journal       = {CoRR},
  volume        = {abs/2011.04006},
  year          = {2021},
  eprint        = {2011.04006},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{huggingfaceQuantization,
  author        = {{Hugging Face}},
  title         = {Quantization},
  year          = {2024},
  url           = {https://huggingface.co/docs/transformers/main/en/main\_classes/quantization},
  urldate       = {2024-04-22},
  note          = {Accessed: 2024-04-22}
}
@inproceedings{mishra2022crosstask,
  title         = {Cross-task generalization via natural language crowdsourcing instructions},
  author        = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle     = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages         = {3470--3487},
  year          = {2022},
  editor        = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  address       = {Dublin, Ireland},
  month         = {May 22-27},
  organization  = {ACL},
  url           = {https://aclanthology.org/2022.acl-long.243}
}
@article{bach2022promptsource,
  title         = {PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
  author        = {Bach, Sebastian H. and Sanh, Victor and Yong, Zhi-Xuan and Webson, Alex and Raffel, Colin and Nayak, Nikhil V. and Sharma, Ankit and Kim, Tae-Hwan and Bari, Md. Saiful and F{\'e}vry, Thibault and Alyafeai, Zaid and Dey, Manan and Santilli, Anthony and Sun, Zhi and Ben-David, Shai and Xu, Chen and Chhablani, Gaurav and Wang, Hao and Fries, Jason A. and AlShaibani, Mohammed S. and Sharma, Shubham and Thakker, Urvish and Almubarak, Khalid and Tang, Xinyi and Radev, Dragomir R. and Jiang, Ming-Ting and Rush, Alexander M.},
  journal       = {CoRR},
  volume        = {abs/2202.12108},
  year          = {2022},
  eprint        = {2202.12108},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{wang2022super,
  title         = {Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  author        = {Wang, Yada and Mishra, Swaroop and Alipoormolabashi, Parisa and Kordi, Yaser and Mirzaei, Amir and Naik, Aniruddha and Ashok, Anirudh and Dhanasekaran, Arun S. and Arunkumar, Anirudh and Stap, Daniel and Pathak, Eshaan and Karamanolakis, George and Lai, Hui Guan and Purohit, Ishan and Mondal, Indranil and Anderson, Jennifer and Kuznia, Kevin and Doshi, Khyati and Pal, Koustuv and Patel, Manan and Moradshahi, Mehrdad and Parmar, Mihir and Purohit, Mihir and Varshney, Naman and Kaza, Pranav R. and Verma, Prateek and Puri, Raghav Karia and Doshi, Sagar and Sampat, Sagar K. and Mishra, Swaroop R. A and Patro, Srikar and Dixit, Tanmay and Shen, Xiang},
  journal       = {CoRR},
  volume        = {abs/2209.13107},
  year          = {2022},
  eprint        = {2209.13107},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{tang2022mvp,
  title         = {MVP: Multi-Task Supervised Pre-training for Natural Language Generation},
  author        = {Tang, Tian and Li, Jindong and Zhao, Wenxuan and Wen, Ji-Rong},
  journal       = {CoRR},
  volume        = {abs/2206.12131},
  year          = {2022},
  eprint        = {2206.12131},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{muennighoff2022crosslingual,
  title         = {Crosslingual Generalization Through Multitask Finetuning},
  author        = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Le Scao, Teven and Bari, M. Saiful and Shen, Sheng and Yong, Zheng Xin and Schoelkopf, Hannes and Tang, Xiang and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Alvin and Raff, Edward and Raffel, Colin},
  journal       = {CoRR},
  volume        = {abs/2211.01786},
  year          = {2022},
  url           = {https://arxiv.org/abs/2211.01786}
}
@misc{bai2022training,
  title         = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author        = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
  year          = {2022},
  eprint        = {2204.05862},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{guo2023close,
  title         = {How close is ChatGPT to human experts? Comparison corpus, evaluation, and detection},
  author        = {Guo, Binbin and Zhang, Xiang and Wang, Zhiyuan and Jiang, Min and Nie, Jian-Yun and Ding, Yuxiang and Yue, Jie and Wu, Yan},
  journal       = {CoRR},
  volume        = {abs/2301.07597},
  year          = {2023},
  eprint        = {2301.07597},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{koepf2023openassistant,
  title         = {OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author        = {Kopf, Andreas and Kilcher, Yannic and von Rutte, David and Anagnostidis, Stefanos and Tam, Zhi-Rui and Stevens, Kevin and Barhoum, Ahmed and Duc, Nhat Minh and Stanley, Oliver and Nagyfi, Robert and others},
  journal       = {arXiv preprint arXiv:2304.07327},
  year          = {2023}
}
@article{wang2022selfinstruct,
  title         = {Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author        = {Wang, Yada and Kordi, Yaser and Mishra, Swaroop and Liu, Anqi and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal       = {CoRR},
  volume        = {abs/2212.10560},
  year          = {2022}
}
@misc{taori2023stanford,
  title         = {Stanford ALPACA: An Instruction-Following LLaMA Model},
  author        = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Ting and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  year          = {2023},
  howpublished  = {\url{https://github.com/tatsu-lab/stanford-alpaca}}
}
@article{fedus2021switch,
  title         = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author        = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal       = {J. Mach. Learn. Res},
  pages         = {1--40},
  year          = {2021}
}
@article{xu2023baize,
  title         = {Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
  author        = {Xu, Chen and Guo, Dong and Duan, Nan and McAuley, Julian},
  journal       = {arXiv preprint arXiv:2304.01196},
  year          = {2023}
}
@article{ji2023towards,
  title         = {Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation},
  author        = {Ji, Yuxuan and Gong, Yuxuan and Deng, Yuxuan and Peng, Yuxin and Niu, Qian and Ma, Bin and Li, Xiang},
  journal       = {arXiv preprint arXiv:2304.07854},
  year          = {2023}
}
@online{wang2022supernaturalinstructions,
  author        = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  title         = {Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  year          = {2022},
  eprint        = {2204.07705},
  eprinttype    = {arxiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2204.07705}
}
@article{hubara2017quantized,
  title         = {Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations},
  author        = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal       = {J. Mach. Learn. Res},
  volume        = {18},
  pages         = {6869--6898},
  year          = {2017}
}
@article{miyashita2016convolutional,
  title         = {Convolutional Neural Networks Using Logarithmic Data Representation},
  author        = {Miyashita, Daisuke and Lee, Edward H. and Murmann, Boris},
  journal       = {CoRR},
  volume        = {abs/1603.01025},
  year          = {2016},
  eprint        = {1603.01025},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{jacob2017quantization,
  title         = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author        = {Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
  year          = {2017},
  eprint        = {1712.05877},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{sanhetal2022multitask,
  title         = {Multitask prompted training enables zero-shot task generalization},
  author        = {Sanh, Victor and Webson, Alex and Raffel, Colin and Bach, Sebastian H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Alex and Stiegler, Adam and Raja, Anirudh and Dey, Manan and Bari, M. Saiful and Xu, Chen and Thakker, Urvish and Sharma, Shubham S. and Szczechla, Eric and Kim, Tae-Hwan and Chhablani, Gaurav and Nayak, Nikhil V. and Datta, Debanjan and Chang, Jason and Jiang, Ming-Ting and Wang, Hao and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harsh and Bawden, Robert and Wang, Thomas and Neeraj, Tanmay and Rozen, Jonathan and Sharma, Ankit and Santilli, Anthony and F{\'e}vry, Thibault and Fries, Jason A. and Teehan, Ryan and Scao, Teven Le and Biderman, Stella and Gao, Li and Wolf, Thomas and Rush, Alexander M.},
  journal       = {The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022},
  year          = {2022},
  note          = {OpenReview.net}
}
@article{radford2023gpt4,
  title         = {GPT-4: A Large-Scale Generative Pre-trained Transformer},
  author        = {Radford, Alec and Kim, Jong Wook and Child, Rewon and Wu, Jeff and Amodei, Dario and Sutskever, Ilya},
  journal       = {CoRR},
  volume        = {abs/2304.07409},
  year          = {2023},
  eprint        = {2304.07409},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{liu2019multi,
  title         = {Multi-task deep neural networks for natural language understanding},
  author        = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal       = {CoRR},
  volume        = {abs/1901.11504},
  year          = {2019},
  eprint        = {1901.11504},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{aghajanyan2021muppet,
  title         = {Muppet: Massive multi-task representations with pre-finetuning},
  author        = {Aghajanyan, Anna and Gupta, Ankit and Shrivastava, Abhinav and Chen, Xiang and Zettlemoyer, Luke and Gupta, Saurabh},
  journal       = {CoRR},
  volume        = {abs/2109.08668},
  year          = {2021},
  eprint        = {2109.08668},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{longpre2023flan,
  title         = {The FLAN Collection: Designing Data and Methods for Effective Instruction Tuning},
  author        = {Longpre, Shayne and Hou, Lu and Vu, Trieu and Webson, Abigail and Chung, Hyung Won and Tay, Yi and Zhou, Da and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  journal       = {CoRR},
  volume        = {abs/2301.13688},
  year          = {2023},
  url           = {https://arxiv.org/abs/2301.13688}
}
@article{sanh2021distilbert,
  title         = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author        = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal       = {Proceedings of the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS},
  year          = {2019},
  pages         = {12--23}
}
@article{chen2023maybe,
  title         = {Maybe Only 0.5\% Data Is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning},
  author        = {Chen, H. and Zhang, Y. and Zhang, Q. and Yang, H. and Hu, X. and Ma, X. and Yanggong, Y. and Zhao, J.},
  journal       = {arXiv preprint arXiv:2305.09246},
  year          = {2023}
}
@article{iyer2022opt,
  title         = {OPT-IML: Scaling Language Model Instruction Meta Learning Through the Lens of Generalization},
  author        = {Iyer, Srinivasan and Lin, Xiang Vincent and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Deniz and Yu, Peng and Shuster, Konstantin and Wang, Thomas and Liu, Qian and Koura, Pierre-Simon and Li, Xiang and O'Horo, Benjamin and Pereyra, Gabriel and Wang, Jue and Dewan, Chandan and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal       = {CoRR},
  volume        = {abs/2212.12017},
  year          = {2022},
  eprint        = {2212.12017},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{lewis2021paLM,
  title         = {PaLM: Scaling Language Modeling with Pathways},
  author        = {Lewis, Mike and others},
  booktitle     = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  year          = {2021},
  pages         = {1104--1115}
}
@misc{chiang2023vicuna,
  title         = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%\textasteriskcentered ChatGPT Quality},
  author        = {Chiang, W.-L. and Li, Z. and Lin, Z. and Sheng, Y. and Wu, Z. and Zhang, H. and Zheng, L. and Zhuang, S. and Zhuang, Y. and Gonzalez, J. E. and Stoica, I. and Xing, E. P.},
  year          = {2023},
  note          = {[Online]. Available: \url{https://vicuna.lmsys.org}}
}
@article{krell2021efficient,
  title         = {Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance},
  author        = {Krell, Michael M. and Kosec, Mario and Perez, Santiago P. and Fitzgibbon, Andrew},
  journal       = {CoRR},
  volume        = {abs/2107.02027},
  year          = {2021},
  eprint        = {2107.02027},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{cao2023instruction,
  title         = {Instruction Mining: When Data Mining Meets Large Language Model Finetuning},
  author        = {Yihan Cao and Yanbin Kang and Chi Wang and Lichao Sun},
  year          = {2023},
  eprint        = {2307.06290},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{xu2023wizardlm,
  title         = {WizardLM: Empowering Large Language Models to Follow Complex Instructions},
  author        = {Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
  year          = {2023},
  eprint        = {2304.12244},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{tamkin2021understanding,
  title         = {Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models},
  author        = {Tamkin, Alex and Trisha, Singh and Giovanardi, Davide and Goodman, Noah},
  journal       = {arXiv preprint arXiv:2102.02503},
  year          = {2021}
}
