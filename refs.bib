%! Author = amatarazzo
%! Date = 10/03/24
@article{survey,
  author        = {Wayne Xin Zhao and Kun Zhou\textasteriskcentered and Junyi Li\textasteriskcentered and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  year          = {2023},
  title         = {A Survey of Large Language Models},
  publisher     = {ArXiv}
}
@article{bengio2003neural,
  title         = {A Neural Probabilistic Language Model},
  author        = {Bengio, Yoshua and Ducharme, Rejean and Vincent, Pascal and Janvin, Christian},
  journal       = {Journal of Machine Learning Research},
  volume        = {3},
  pages         = {1137--1155},
  year          = {2003}
}
@inproceedings{mikolov2013distributed,
  title         = {Distributed Representations of Words and Phrases and Their Compositionality},
  author        = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeffrey},
  booktitle     = {Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a Meeting Held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  editor        = {Burges, C. J. C. and Bottou, L. and Ghahramani, Z. and Weinberger, K. Q.},
  year          = {2013},
  pages         = {3111--3119}
}
@inproceedings{mikolov2013efficient,
  title         = {Efficient Estimation of Word Representations in Vector Space},
  author        = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  booktitle     = {1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  editor        = {Bengio, Yoshua and LeCun, Yann},
  year          = {2013}
}
@misc{peters2018deep,
  author        = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year          = {2018},
  title         = {Deep Contextualized Word Representations},
  howpublished  = {arXiv preprint},
  url           = {https://arxiv.org/abs/1802.05365}
}
@inproceedings{devlin2019bert,
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title         = {Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle     = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019},
  editor        = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  volume        = {1},
  number        = {Long and Short Papers},
  series        = {NAACL-HLT '19},
  pages         = {4171--4186},
  year          = {2019},
  month         = jun,
  address       = {Minneapolis, MN, USA},
  publisher     = {Association for Computational Linguistics},
  doi           = {10.18653/v1/N19-1423},
  url           = {https://www.aclweb.org/anthology/N19-1423}
}
@online{radford2019language,
  author        = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya et al.},
  title         = {Language Models Are Unsupervised Multitask Learners},
  howpublished  = {OpenAI Blog},
  year          = {2019},
  url           = {https://openai.com/blog/better-language-models/}
}
@misc{brown2020language,
  title         = {Language Models Are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{raffel2023exploring,
  author        = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Mengye and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal       = {Journal of Machine Learning Research},
  volume        = {21},
  pages         = {140:1--140:67},
  year          = {2020}
}
@article{anderson1972more,
  author        = {Philip W. Anderson},
  year          = {1972},
  title         = {More is Different: Broken Symmetry and the Nature of the Hierarchical Structure of Science},
  publisher     = {Science},
  url           = {http://www.lanais.famaf.unc.edu.ar/cursos/em/Anderson-MoreDifferent-1972.pdf}
}
@misc{wei2022emergent,
  title         = {Emergent Abilities of Large Language Models},
  author        = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  year          = {2022},
  eprint        = {2206.07682},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{adiwardana2020towards,
  title         = {Towards a Human-like Open-Domain Chatbot},
  author        = {Daniel Adiwardana and Minh-Thang Luong and David R. So and Jamie Hall and Noah Fiedel and Romal Thoppilan and Zi Yang and Apoorv Kulshreshtha and Gaurav Nemade and Yifeng Lu and Quoc V. Le},
  year          = {2020},
  eprint        = {2001.09977},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{openai2024gpt4,
  title         = {GPT-4 Technical Report},
  author        = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Sim\'{o}n Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and \L{}ukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and \L{}ukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David M\'{e}ly and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer\'{o}n Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year          = {2024},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{radford2021learning,
  title         = {Learning Transferable Visual Models From Natural Language Supervision},
  author        = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  year          = {2021},
  eprint        = {2103.00020},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{ramesh2021zero,
  title         = {Zero-Shot Text-to-Image Generation},
  author        = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
  year          = {2021},
  eprint        = {2102.12092},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{kaplan2020scaling,
  author        = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title         = {Scaling Laws for Neural Language Models},
  journal       = {CoRR},
  volume        = {abs/2001.08361},
  year          = {2020}
}
@article{hoffmann2022training,
  author        = {Hoffmann, Jan and Borgeaud, Samuel and Mensch, Andrei and Buchatskaya, Elena and Cai, Tianlong and Rutherford, Edward and de Las Casas, Daniel and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Alexander and Hennigan, Tim and Noland, Emily and Millican, Kelsey and van den Driessche, Gilles and Damoc, Bogdan and Guy, Adam and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  title         = {Training Compute-Optimal Large Language Models},
  journal       = {CoRR},
  volume        = {abs/2203.15556},
  year          = {2022}
}
@article{dai2022why,
  author        = {D. Dai AND Y. Sun AND L. Dong AND Y. Hao AND Z. Sui AND F. Wei},
  year          = {2022},
  title         = {Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers},
  publisher     = {CoRR}
}
@article{ouyang2022training,
  title         = {Training Language Models to Follow Instructions with Human Feedback},
  author        = {Ouyang, L. and Wu, J. and Jiang, X. and Almeida, D. and Wainwright, C. L. and Mishkin, P. and Zhang, C. and Agarwal, S. and Slama, K. and Ray, A. and Schulman, J. and Hilton, J. and Kelton, F. and Miller, L. and Simens, M. and Askell, A. and Welinder, P. and Christiano, P. F. and Leike, J. and Lowe, R.},
  journal       = {CoRR},
  volume        = {abs/2203.02155},
  year          = {2022}
}
@inproceedings{wei2022fine,
  title         = {Fine-tuned Language Models are Zero-shot Learners},
  author        = {Wei, J. and Bosma, M. and Zhao, V. Y. and Guu, K. and Yu, A. W. and Lester, B. and Du, N. and Dai, A. M. and Le, Q. V.},
  booktitle     = {The Tenth International Conference on Learning Representations, ICLR 2022},
  year          = {2022},
  address       = {Virtual Event},
  month         = {April 25-29},
  organization  = {OpenReview.net}
}
@article{chung2022scaling,
  title         = {Scaling Instruction-Finetuned Language Models},
  author        = {Chung, H. W. and Hou, L. and Longpre, S. and Zoph, B. and Tay, Y. and Fedus, W. and Li, E. and Wang, X. and Dehghani, M. and Brahma, S. and Webson, A. and Gu, S. S. and Dai, Z. and Suzgun, M. and Chen, X. and Chowdhery, A. and Narang, S. and Mishra, G. and Yu, A. and Zhao, V. Y. and Huang, Y. and Dai, A. M. and Yu, H. and Petrov, S. and Chi, E. H. and Dean, J. and Devlin, J. and Roberts, A. and Zhou, D. and Le, Q. V. and Wei, J.},
  journal       = {CoRR},
  volume        = {abs/2210.11416},
  year          = {2022}
}
@article{wei2022chain,
  title         = {Chain of thought prompting elicits reasoning in large language models},
  author        = {Wei, J. and Wang, X. and Schuurmans, D. and Bosma, M. and Chi, E. H. and Le, Q. and Zhou, D.},
  journal       = {CoRR},
  volume        = {abs/2201.11903},
  year          = {2022}
}
@misc{vaswani2023attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  note          = {v7}
}
@inproceedings{christiano2017deep,
  title         = {Deep reinforcement learning from human preferences},
  author        = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle     = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017},
  editor        = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year          = {2017},
  pages         = {4299--4307},
  address       = {Long Beach, CA, USA},
  organization  = {Curran Associates, Inc.},
  date          = {2017-12-04/2017-12-09}
}
@inproceedings{hendrycks2021measuring,
  title         = {Measuring Massive Multitask Language Understanding},
  author        = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year          = {2021}
}
@article{henighan2020scaling,
  title         = {Scaling Laws for Autoregressive Generative Modeling},
  author        = {Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B. and Dhariwal, Prafulla and Gray, Scott and others},
  journal       = {arXiv preprint arXiv:2010.14701},
  year          = {2020}
}
@misc{chen2021evaluating,
  title         = {Evaluating Large Language Models Trained on Code},
  author        = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year          = {2021},
  howpublished  = {arXiv preprint arXiv:2107.03374}
}
@misc{touvron2023llama,
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\'{e}e Lacroix and Baptiste Rozi\`{e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year          = {2023},
  eprint        = {2302.13971},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{shazeer2020glu,
  title         = {GLU Variants Improve Transformer},
  author        = {Shazeer, Noam},
  journal       = {arXiv preprint arXiv:2002.05202},
  year          = {2020}
}
@article{su2021roformer,
  title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author        = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal       = {arXiv preprint arXiv:2104.09864},
  year          = {2021}
}
@misc{gemma_google_ai,
  author        = {Jeanine Banks and Tris Warkentin},
  title         = {Gemma: Google introduces new state-of-the-art open models},
  howpublished  = {Google AI Blog},
  year          = {2024},
  url           = {https://blog.google/technology/developers/gemma-open-models/}
}
@misc{gemmateam2024gemma,
  title         = {Gemma: Open Models Based on Gemini Research and Technology},
  author        = {Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivi\`{e}re and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and L\'{e}onard Hussenot and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Am\'{e}lie H\'{e}liou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Cl\'{e}ment Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Miku\l{}a and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Pier Giuseppe Sessa and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Cl\'{e}ment Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
  year          = {2024},
  eprint        = {2403.08295},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{alsentzer2019publicly,
  title         = {Publicly available clinical BERT embeddings},
  author        = {Alsentzer, Emily and others},
  journal       = {arXiv preprint arXiv:1904.03323},
  year          = {2019}
}
@article{shickel2018deep,
  title         = {Deep EHR: A survey of recent advances in deep learning techniques for electronic health record (EHR) analysis},
  author        = {Shickel, Benjamin and others},
  journal       = {IEEE journal of biomedical and health informatics},
  volume        = {22},
  number        = {5},
  pages         = {1589--1604},
  year          = {2018},
  publisher     = {IEEE}
}
@article{zhavoronkov2019deep,
  title         = {Deep learning enables rapid identification of potent DDR1 kinase inhibitors},
  author        = {Zhavoronkov, Alex and others},
  journal       = {Nature Biotechnology},
  volume        = {37},
  pages         = {1038--1040},
  year          = {2019},
  doi           = {10.1038/d41573-019-00170-0}
}
@article{kocaballi2019personalization,
  title         = {The Personalization of Conversational Agents in Health Care: Systematic Review},
  author        = {Kocaballi, A. Baki and Berkovsky, Shlomo and Quiroz, Juan C. and Laranjo, Liliana and Tong, Huong Ly and Rezazadegan, Dana and others},
  journal       = {Journal of Medical Internet Research},
  volume        = {21},
  number        = {11},
  year          = {2019},
  doi           = {10.2196/15360},
  url           = {https://www.jmir.org/2019/11/e15360/}
}
@article{beam2018big,
  title         = {Big Data and Machine Learning in Health Care},
  author        = {Beam, Andrew L. and Kohane, Isaac S.},
  journal       = {JAMA},
  volume        = {319},
  number        = {13},
  pages         = {1317--1318},
  year          = {2018}
}
@article{chen2019single,
  title         = {Single-cell trajectories reconstruction, exploration and mapping of omics data with STREAM},
  author        = {Chen, H. and others},
  journal       = {Nature Communications},
  volume        = {10},
  number        = {1},
  pages         = {1903},
  year          = {2019},
  doi           = {10.1038/s41467-019-09670-4},
  url           = {https://www.nature.com/articles/s41467-019-09670-4}
}
@article{hamburg2010path,
  title         = {The Path to Personalized Medicine},
  author        = {Hamburg, M. A. and Collins, F. S.},
  journal       = {New England Journal of Medicine},
  volume        = {363},
  number        = {4},
  pages         = {301--304},
  year          = {2010},
  doi           = {10.1056/NEJMp1006304},
  url           = {https://sci-hub.se/10.1056/NEJMp1006304}
}
@article{buehler2018deep,
  title         = {Deep learning and algorithmic trading},
  author        = {Buehler, Hans and Gonon, Lukas and Teichmann, Josef and Wood, Ben},
  journal       = {Financial Markets and Portfolio Management},
  volume        = {32},
  number        = {3},
  pages         = {239--260},
  year          = {2018},
  publisher     = {Springer}
}
@article{li2020natural,
  title         = {Natural language processing in risk management and compliance},
  author        = {Li, Jin and Spangler, Scott and Yu, Yue},
  journal       = {Journal of Risk Management in Financial Institutions},
  volume        = {13},
  number        = {2},
  pages         = {158--175},
  year          = {2020},
  publisher     = {Henry Stewart Publications}
}
@article{pal2021enhancing,
  title         = {Enhancing customer service through AI-driven virtual assistants in the banking sector},
  author        = {Pal, Arpan and Kundu, Aniruddha and Chakraborty, Rajdeep},
  journal       = {Journal of Banking and Financial Technology},
  volume        = {5},
  number        = {1},
  pages         = {1--12},
  year          = {2021},
  publisher     = {Springer}
}
@article{smith2019improving,
  title         = {Improving fraud detection in financial services through deep learning},
  author        = {Smith, Timothy and Kumar, Manish},
  journal       = {Journal of Financial Crime},
  volume        = {26},
  number        = {4},
  pages         = {1062--1073},
  year          = {2019},
  publisher     = {Emerald Publishing Limited}
}
@article{jones2020ethical,
  title         = {Ethical considerations for AI in finance},
  author        = {Jones, Michael and Barn, Ravinder and Karim, Mohammed and Nurse, Jason R C},
  journal       = {AI \& Society},
  volume        = {35},
  number        = {1},
  pages         = {287--300},
  year          = {2020},
  publisher     = {Springer}
}
@article{zhang2021medical,
  title         = {Medical image analysis with artificial intelligence},
  author        = {Zhang, Jun and others},
  journal       = {IEEE Transactions on Biomedical Engineering},
  volume        = {68},
  number        = {5},
  pages         = {1375--1379},
  year          = {2021},
  publisher     = {IEEE}
}
@article{li2021survey,
  title         = {A survey on deep learning in medical image analysis},
  author        = {Li, Zhi and Zhang, Qiang and Dou, Qi and others},
  journal       = {Medical image analysis},
  volume        = {67},
  pages         = {101813},
  year          = {2021},
  publisher     = {Elsevier}
}
@article{singhal2022large,
  title         = {Large language models encode clinical knowledge},
  author        = {Singhal, Karan and Azizi, Sebti and Tu, Tony and Mahdavi, Seyed Sepehr and Wei, Jason and Chung, Han Wu and Scales, Nicholas and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen R. and others},
  journal       = {arXiv preprint arXiv:2212.13138},
  year          = {2022}
}
@article{wang2022self,
  title         = {Self-consistency improves chain of thought reasoning in language models},
  author        = {Wang, X. and Wei, J. and Schuurmans, D. and Le, Q. and Chi, E. and Zhou, D.},
  journal       = {arXiv preprint arXiv:2203.11171},
  year          = {2022}
}
@misc{wu2023bloomberggpt,
  title         = {BloombergGPT: A Large Language Model for Finance},
  author        = {Shijie Wu and Ozan Irsoy and Steven Lu and Vadim Dabravolski and Mark Dredze and Sebastian Gehrmann and Prabhanjan Kambadur and David Rosenberg and Gideon Mann},
  year          = {2023},
  eprint        = {2303.17564},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{luo2022biogpt,
  title         = {BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining},
  author        = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
  journal       = {Briefings in Bioinformatics},
  volume        = {23},
  number        = {6},
  year          = {2022},
  month         = {sep},
  doi           = {10.1093/bib/bbac409},
  url           = {https://doi.org/10.1093\%2Fbib\%2Fbbac409}
}
@misc{bolton2023biomedlm,
  title         = {BioMedLM},
  author        = {Bolton, Elliot and Hall, David and Yasunaga, Michihiro and Lee, Tony and Manning, Chris and Liang, Percy},
  year          = {2023},
  howpublished  = {\url{https://github.com/stanford-crfm/BioMedLM}}
}
@misc{taylor2022galactica,
  title         = {Galactica: A Large Language Model for Science},
  author        = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  year          = {2022},
  month         = {11},
  howpublished  = {\url{http://arxiv.org/abs/2211.09085}},
  note          = {arXiv:2211.09085}
}
@misc{liang2022holistic,
  title         = {Holistic Evaluation of Language Models},
  author        = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and R{\'e}, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel J. and Zheng, Lucia and Y{\"u}ksekg{\"o}n{\"u}l, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri S. and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  year          = {2022},
  journal       = {CoRR},
  volume        = {abs/2211.09110},
  doi           = {10.48550/arXiv.2211.09110},
  url           = {https://doi.org/10.48550/arXiv.2211.09110}
}
@misc{lee2024survey,
  title         = {A Survey of Large Language Models in Finance (FinLLMs)},
  author        = {Jean Lee and Nicholas Stevens and Soyeon Caren Han and Minseok Song},
  year          = {2024},
  eprint        = {2402.02315},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{xie2023pixiu,
  title         = {Pixiu: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance},
  author        = {Xie, Q. and Han, W. and Zhang, X. and Lai, Y. and Peng, M. and Lopez-Lira, A. and Huang, J.},
  booktitle     = {Proceedings of NeurIPS Datasets and Benchmarks},
  year          = {2023}
}
@article{yang2023investlm,
  title         = {InvestLM: A Large Language Model for Investment Using Financial Domain Instruction Tuning},
  author        = {Yang, Yi and Tang, Yixuan and Tam, Kar Yan},
  journal       = {arXiv preprint arXiv:2309.13064},
  year          = {2023}
}
@article{wang2023fingpt,
  title         = {FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},
  author        = {Wang, Neng and Yang, Hongyang and Wang, Christina Dan},
  journal       = {arXiv preprint arXiv:2309.13064},
  year          = {2023}
}
@article{malo2014good,
  title         = {Good debt or bad debt: Detecting semantic orientations in economic texts},
  author        = {Malo, Pekka and Sinha, Ankur and Korhonen, Pekka and Wallenius, Jyrki and Takala, Pyry},
  journal       = {JASIST},
  volume        = {65},
  number        = {4},
  pages         = {782--796},
  year          = {2014}
}
@article{maia2018www,
  title         = {WWW'18 Open Challenge: Financial Opinion Mining and Question Answering},
  author        = {Maia, Macedo and Handschuh, Siegfried and Freitas, Andr{\'e} and others},
  journal       = {Companion Proceedings of WWW},
  pages         = {1941--1942},
  year          = {2018}
}
@inproceedings{alvarado2015domain,
  title         = {Domain adaption of named entity recognition to support credit risk assessment},
  author        = {Alvarado, Julio Cesar Salinas and Verspoor, Karin and Baldwin, Timothy},
  booktitle     = {Proceedings of ALTA Workshop},
  pages         = {84--90},
  year          = {2015}
}
@article{loukas2022finer,
  title         = {Finer: Financial numeric entity recognition for xbrl tagging},
  author        = {Lefteris Loukas and Manos Fergadiotis and Ilias Chalkidis and Eirini Spyropoulou and others},
  journal       = {Proceedings of ACL},
  pages         = {4419--4431},
  year          = {2022}
}
@inproceedings{chen2022convfinqa,
  title         = {ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering},
  author        = {Chen, Zhiyu and Li, Shiyang and Smiley, Charese and Ma, Zhiqiang and Shah, Sameena and Wang, William Yang},
  booktitle     = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages         = {6279--6292},
  year          = {2022}
}
@inproceedings{xu2018stock,
  title         = {Stock movement prediction from tweets and historical prices},
  author        = {Xu, Yumo and Cohen, Shay B},
  booktitle     = {Proceedings of ACL},
  pages         = {1970--1979},
  year          = {2018}
}
@inproceedings{wu2018hybrid,
  title         = {Hybrid deep sequential modeling for social text-driven stock prediction},
  author        = {Wu, Huizhe and Zhang, Wei and Shen, Weiwei and Wang, Jun},
  booktitle     = {Proceedings of ACM CIKM},
  pages         = {1627--1630},
  year          = {2018}
}
@inproceedings{soun2022accurate,
  title         = {Accurate stock movement prediction with self-supervised learning from sparse noisy tweets},
  author        = {Soun, Yejun and Yoo, Jaemin and Cho, Minyong and Jeon, Jihyeong and Kang, U},
  booktitle     = {IEEE Big Data},
  pages         = {1691--1700},
  year          = {2022}
}
@inproceedings{sinha2021impact,
  title         = {Impact of news on the commodity market: Dataset and results},
  author        = {Sinha, Ankur and Khandait, Tanmay},
  booktitle     = {Proceedings of FICC},
  pages         = {589--601},
  year          = {2021}
}
@inproceedings{mukherjee2022ectsum,
  title         = {ECTSum: A New Benchmark Dataset for Bullet Point Summarization of Long Earnings Call Transcripts},
  author        = {Mukherjee, Rajdeep and Bohra, Abhinav and Banerjee, Akash and Sharma, Soumya and others},
  booktitle     = {Proceedings of EMNLP},
  pages         = {10893--10906},
  year          = {2022}
}
@inproceedings{sharma2022finred,
  title         = {Finred: A dataset for relation extraction in financial domain},
  author        = {Sharma, Soumya and Nayak, Tapas and Bose, Arusarka and Meena, Ajay Kumar and others},
  booktitle     = {Companion Proceedings of WWW},
  pages         = {595--597},
  year          = {2022}
}
@inproceedings{zhou2021trade,
  title         = {Trade the event: Corporate events detection for news-based event-driven trading},
  author        = {Zhou, Zhihan and Ma, Liqian and Liu, Han},
  booktitle     = {Findings of ACL-IJCNLP},
  pages         = {2114--2124},
  year          = {2021}
}
@inproceedings{mariko2020financial,
  title         = {The financial document causality detection shared task (fincausal 2020)},
  author        = {Mariko, Dominique and Akl, Hanna Abi and Labidurie, Estelle and others},
  booktitle     = {Proceedings of the Workshop on FNP-FNS},
  pages         = {23--32},
  year          = {2020}
}
@inproceedings{zheng2021global,
  title         = {Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context},
  author        = {Zheng, Xinyi and Burdick, Douglas and Popa, Lucian and Zhong, Xu and Wang, Nancy Xin Ru},
  booktitle     = {Proceedings of the IEEE/CVF WACV},
  pages         = {697--706},
  year          = {2021}
}
@inproceedings{li2020maec,
  title         = {MAEC: A Multimodal Aligned Earnings Conference Call Dataset for Financial Risk Prediction},
  author        = {Li, Jiazheng and Yang, Linyi and Smyth, Barry and Dong, Ruihai},
  booktitle     = {Proceedings of ACM CIKM},
  pages         = {3063--3070},
  year          = {2020}
}
@inproceedings{mathur2022monopoly,
  title         = {Monopoly: Financial prediction from monetary policy conference videos using multimodal cues},
  author        = {Mathur, Puneet and Neerkaje, Atula and Chhibber, Malika and Sawhney, Ramit and others},
  booktitle     = {Proceedings of ACM MM},
  pages         = {2276--2285},
  year          = {2022}
}
@inproceedings{gerz2021multilingual,
  title         = {Multilingual and cross-lingual intent detection from spoken data},
  author        = {Gerz, Daniela and Su, Pei-Hao and Kuszto, Razvan and Mondal, Avishek and Lis, Micha\l{} and others},
  booktitle     = {Proceedings of EMNLP},
  pages         = {7468--7475},
  year          = {2021}
}
@inproceedings{lee2023stockemotions,
  title         = {StockEmotions: Discover Investor Emotions for Financial Sentiment Analysis and Multivariate Time Series},
  author        = {Lee, Jean and Youn, Hoyoul Luis and Poon, Josiah and Han, Soyeon Caren},
  booktitle     = {AAAI-24 Bridge},
  year          = {2023}
}
@inproceedings{jorgensen2023multifin,
  title         = {MultiFin: A Dataset for Multilingual Financial NLP},
  author        = {J\o{}rgensen, Rasmus and Brandt, Oliver and Hartmann, Mareike and Dai, Xiang and Igel, Christian and Elliott, Desmond},
  booktitle     = {Findings of the European Chapter of the Association for Computational Linguistics (EACL)},
  pages         = {864--879},
  year          = {2023}
}
@misc{li2023chatgpt,
  title         = {Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks},
  author        = {Xianzhi Li and Samuel Chan and Xiaodan Zhu and Yulong Pei and Zhiqiang Ma and Xiaomo Liu and Sameena Shah},
  year          = {2023},
  eprint        = {2305.05862},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{chen2022afinqa,
  title         = {FinQA: A Dataset of Numerical Reasoning Over Financial Data},
  author        = {Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and Wang, William Yang},
  year          = {2022},
  note          = {Presumed publication year and citation style as "2022a", specifics such as journal name, volume, issue, pages, and DOI are not provided and should be added}
}
@misc{workshop2023bloom,
  title         = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author        = {BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ili\'{c} and Daniel Hesslow and Roman Castagn\'{e} and Alexandra Sasha Luccioni and Fran\c{c}ois Yvon and Matthias Gall\'{e} and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Beno\^{\i}t Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Lauren\c{c}on and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo Gonz\'{a}lez Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and G\'{e}rard Dupont and Germ\'{a}n Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and J\"{o}rg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Mu\~{n}oz and Maraim Masoud and Mar\'{\i}a Grandury and Mario \v{S}a\v{s}ko and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis L\'{o}pez and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Ta\c{s}ar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre Fran\c{c}ois Lavall\'{e}e and R\'{e}mi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and St\'{e}phane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aur\'{e}lie N\'{e}v\'{e}ol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zden\v{e}k Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Mu\~{n}oz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Cl\'{e}mentine Fourrier and Daniel Le\'{o}n Peri\~{n}\'{a}n and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc P\`{a}mies and Maria A Castillo and Marianna Nezhurina and Mario S\"{a}nger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Th\'{e}o Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
  year          = {2023},
  eprint        = {2211.05100},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{hendrycks2016gelu,
  title         = {Gaussian Error Linear Units (GELUs)},
  author        = {Hendrycks, Dan and Gimpel, Kevin},
  journal       = {arXiv preprint arXiv:1606.08415},
  year          = {2016}
}
@inproceedings{lescao2022milliongpuhours,
  title         = {What language model to train if you have one million GPU hours?},
  author        = {Le Scao, Teven and Wang, Thomas and Hesslow, Daniel and Bekman, Stas and Bari, M Saiful and Biderman, Stella and Elsahar, Hady and Muennighoff, Niklas and Phang, Jason and Press, Ofir and others},
  booktitle     = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages         = {765--782},
  year          = {2022},
  month         = {December},
  publisher     = {Association for Computational Linguistics},
  address       = {Abu Dhabi, United Arab Emirates},
  url           = {https://aclanthology.org/2022.findings-emnlp.54}
}
@article{malinka2023educationalimpact,
  title         = {On the educational impact of ChatGPT: Is artificial intelligence ready to obtain a university degree?},
  author        = {Malinka, K. and Peres{\'i}ni, M. and Firc, A. and Hujnak, O. and Janus, F.},
  journal       = {CoRR},
  volume        = {abs/2303.11146},
  year          = {2023},
  url           = {https://arxiv.org/abs/2303.11146}
}
@article{susnjak2022chatgpt,
  title         = {ChatGPT: The end of online exam integrity?},
  author        = {Susnjak, T.},
  journal       = {CoRR},
  volume        = {abs/2212.09292},
  year          = {2022},
  url           = {https://arxiv.org/abs/2212.09292}
}
@article{blairstanek2023gpt3statutory,
  author        = {Andrew Blair-Stanek and Nils Holzenberger and Benjamin Van Durme},
  title         = {Can {GPT-3} perform statutory reasoning?},
  journal       = {CoRR},
  volume        = {abs/2302.06100},
  year          = {2023},
  url           = {https://arxiv.org/abs/2302.06100},
  archiveprefix = {arXiv},
  eprint        = {2302.06100}
}
@article{trautmann2022legalprompt,
  author        = {Daniel Trautmann and Aleksandra Petrova and Frank Schilder},
  title         = {Legal prompt engineering for multilingual legal judgement prediction},
  journal       = {CoRR},
  volume        = {abs/2212.02199},
  year          = {2022},
  url           = {https://arxiv.org/abs/2212.02199},
  archiveprefix = {arXiv},
  eprint        = {2212.02199}
}
@article{choi2023chatgptlaw,
  author        = {Jinho H. Choi and Kristin E. Hickman and Andrew Monahan and Daniel Schwarcz},
  title         = {ChatGPT goes to law school},
  year          = {2023},
  howpublished  = {Available at SSRN},
  url           = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=number},
  note          = {Accessed: 2024-02-14}
}
@article{nay2022lawinformscode,
  title         = {Law informs code: A legal informatics approach to aligning artificial intelligence with humans},
  author        = {Nay, J. J.},
  journal       = {CoRR},
  volume        = {abs/2209.13020},
  year          = {2022},
  eprint        = {2209.13020},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CY},
  url           = {https://arxiv.org/abs/2209.13020}
}
@article{jin2019pubmedqa,
  title         = {PubMedQA: A Dataset for Biomedical Research Question Answering},
  author        = {Jin, Qingyu and Dhingra, Bhuwan and Liu, Zhengdong and Cohen, William W. and Lu, Xinghua},
  year          = 2019,
  journal       = {Proceedings of EMNLP-IJCNLP},
  pages         = {2567--2577}
}
@misc{krithara2022bioasq,
  title         = {BioASQ-QA: A manually curated corpus for biomedical question answering},
  author        = {Krithara, Anastasia and Nentidis, Anastasios and Bougiatiotis, Konstantinos and Paliouras, Georgios},
  year          = 2022
}
@article{lewkowycz2022minerva,
  title         = {Solving quantitative reasoning problems with language models},
  author        = {Lewkowycz, Aitor and others},
  year          = 2022,
  journal       = {CoRR},
  volume        = {abs/2206.14858}
}
@article{zhang2023smallstep,
  title         = {One small step for generative AI, one giant leap for AGI: A complete survey on ChatGPT in AIGC era},
  author        = {Zhang, C. and Zhang, C. and Li, C. and Qiao, Y. and Zheng, S. and Dam, S. K. and Zhang, M. and Kim, J. U. and Kim, S. T. and Choi, J. and Park, G. and Bae, S. and Lee, L. and Hui, P. and Kweon, I. S. and Hong, C. S.},
  journal       = {CoRR},
  volume        = {abs/2304.06488},
  year          = {2023},
  eprint        = {2304.06488},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2304.06488}
}
@article{haman2023usingchatgpt,
  title         = {Using ChatGPT to Conduct a Literature Review},
  author        = {Haman, Micha\l{} and Skolnik, Marcin},
  journal       = {Accountability in Research},
  year          = {2023},
  publisher     = {Taylor \& Francis}
}
@article{aydin2022openaichatgpt,
  title         = {OpenAI ChatGPT Generated Literature Review: Digital Twin in Healthcare},
  author        = {Ayd\i{}n, \"{O}\u{g}uzhan and Karaarslan, Emre},
  journal       = {SSRN Electronic Journal},
  year          = {2022},
  url           = {https://ssrn.com/abstract=number},
  note          = {Please replace "number" with the actual abstract number.}
}
@misc{park2023chatgpt,
  title         = {Can ChatGPT be Used to Generate Scientific Hypotheses?},
  author        = {Park, Yujin J. and others},
  year          = 2023
}
@article{hassan2023chatgptdatascientist,
  title         = {ChatGPT as Your Personal Data Scientist},
  author        = {Hassan, Md Mahadi and Knipper, Richard A. and Santu, Shakked K. K.},
  journal       = {CoRR},
  volume        = {abs/2305.13657},
  year          = {2023},
  eprint        = {2305.13657},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2305.13657}
}
@article{cheng2023gpt4dataanalyst,
  title         = {Is GPT-4 a Good Data Analyst?},
  author        = {Cheng, Long and Li, Xiang and Bing, Lidong},
  journal       = {CoRR},
  volume        = {abs/2305.15038},
  year          = {2023},
  eprint        = {2305.15038},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2305.15038}
}
@article{alkaissi2023artificialhallucinations,
  title         = {Artificial Hallucinations in ChatGPT: Implications in Scientific Writing},
  author        = {Hussam Alkaissi, S. I. M.},
  journal       = {PubMed},
  year          = {2023},
  note          = {Available on PubMed},
  url           = {https://pubmed.ncbi.nlm.nih.gov/ARTICLE\_ID}
}
@article{azaria2023chatgptexperts,
  title         = {ChatGPT is a Remarkable Tool – For Experts},
  author        = {Azaria, A. and Azoulay, R. and Reches, S.},
  journal       = {CoRR},
  volume        = {abs/2306.03102},
  year          = {2023},
  eprint        = {2306.03102},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2306.03102}
}
@article{buruk2023academicwriting,
  title         = {Academic Writing with GPT-3.5: Reflections on Practices, Efficacy and Transparency},
  author        = {Buruk, O. O.},
  journal       = {CoRR},
  volume        = {abs/2304.11079},
  year          = {2023},
  eprint        = {2304.11079},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2304.11079}
}
@article{liu2023reviewergpt,
  title         = {Reviewergpt? An Exploratory Study on Using Large Language Models for Paper Reviewing},
  author        = {Liu, R. and Shah, N. B.},
  journal       = {CoRR},
  volume        = {abs/2306.00622},
  year          = {2023},
  eprint        = {2306.00622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2306.00622}
}
@article{kosinski2023theoryofmind,
  title         = {Theory of Mind May Have Spontaneously Emerged in Large Language Models},
  author        = {Kosinski, M.},
  journal       = {CoRR},
  volume        = {abs/2302.02083},
  year          = {2023},
  eprint        = {2302.02083},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2302.02083}
}
@article{amin2023affectivecomputing,
  title         = {Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT},
  author        = {Amin, M. M. and Cambria, E. and Schuller, B. W.},
  journal       = {CoRR},
  volume        = {abs/2303.03186},
  year          = {2023},
  eprint        = {2303.03186},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2303.03186}
}
@article{sridhara2023chatgptsoftware,
  title         = {ChatGPT: A Study on Its Utility for Ubiquitous Software Engineering Tasks},
  author        = {Sridhara, G. and R. H. G. and Mazumdar, S.},
  journal       = {CoRR},
  volume        = {abs/2305.16837},
  year          = {2023},
  eprint        = {2305.16837},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2305.16837}
}
@article{sun2023code,
  title         = {Automatic Code Summarization via ChatGPT: How Far Are We?},
  author        = {Sun, W. and Fang, C. and You, Y. and Miao, Y. and Liu, Y. and Li, Y. and Deng, G. and Huang, S. and Chen, Y. and Zhang, Q. and Qian, H. and Liu, Y. and Chen, Z.},
  journal       = {CoRR},
  volume        = {abs/2305.12865},
  year          = {2023},
  eprint        = {2305.12865},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2305.12865}
}
@article{xia2023conversationalrepair,
  title         = {Conversational Automated Program Repair},
  author        = {Xia, C. S. and Zhang, L.},
  journal       = {CoRR},
  volume        = {abs/2301.13246},
  year          = {2023},
  eprint        = {2301.13246},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2301.13246}
}
@online{kim2022replacegrammarly,
  author        = {Sung Kim},
  title         = {Replace Grammarly Premium with OpenAI ChatGPT},
  year          = {2022},
  url           = {https://medium.com/geekculture/replace-grammarly-premium-with-openai-chatgpt-320049179c79}
}
@online{maximov2023englishgrammar,
  author        = {Lev Maximov},
  title         = {Do You Know English Grammar Better Than ChatGPT?},
  year          = {2023},
  url           = {https://medium.com/writing-cooperative/do-you-know-english-grammar-better-than-chatgpt-8fc550f23681}
}
@inproceedings{liu2019roberta,
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author        = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle     = {arXiv preprint arXiv:1907.11692},
  year          = {2019}
}
@inproceedings{strubell2019energy,
  title         = {Energy and Policy Considerations for Deep Learning in NLP},
  author        = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle     = {ACL 2019},
  year          = {2019}
}
@article{bender2021dangers,
  title         = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author        = {Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  journal       = {FAccT '21},
  year          = {2021}
}
@inproceedings{ruder2019transfer,
  title         = {Transfer Learning in Natural Language Processing},
  author        = {Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas},
  editor        = {Sarkar, Anoop and Strube, Michael},
  booktitle     = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  month         = jun,
  year          = {2019},
  address       = {Minneapolis, Minnesota},
  publisher     = {Association for Computational Linguistics},
  url           = {https://aclanthology.org/N19-5004},
  doi           = {10.18653/v1/N19-5004},
  pages         = {15--18},
  abstract      = {The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.}
}
@misc{gururangan2020don,
  title         = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author        = {Suchin Gururangan and Ana Marasovi\'{c} and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
  year          = {2020},
  eprint        = {2004.10964},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{phang2019sentence,
  title         = {Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks},
  author        = {Jason Phang and Thibault F\'{e}vry and Samuel R. Bowman},
  year          = {2019},
  eprint        = {1811.01088},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{howard2018universal,
  title         = {Universal Language Model Fine-tuning for Text Classification},
  author        = {Jeremy Howard and Sebastian Ruder},
  year          = {2018},
  eprint        = {1801.06146},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@book{zhu2005semi,
  title         = {Semi-supervised Learning Literature Survey},
  author        = {Zhu, Xiaojin},
  year          = {2005},
  publisher     = {University of Wisconsin-Madison Department of Computer Sciences}
}
@book{chapelle2009semi,
  title         = {Semi-supervised Learning},
  author        = {Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
  year          = {2009},
  publisher     = {MIT Press}
}
@misc{yang2017transfer,
  title         = {Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks},
  author        = {Zhilin Yang and Ruslan Salakhutdinov and William W. Cohen},
  year          = {2017},
  eprint        = {1703.06345},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{bergmann2023semi,
  author        = {Dave Bergmann},
  title         = {What Is Semi-Supervised Learning?},
  year          = {2023},
  url           = {https://www.ibm.com/cloud/learn/semi-supervised-learning},
  urldate       = {2023-12-12},
  note          = {IBM}
}
@article{lee2013pseudo,
  title         = {Pseudo-Label: The Simple and Efficient Semi-supervised Learning Method for Deep Neural Networks},
  author        = {Lee, Dong-Hyun},
  journal       = {ICML 2013 Workshop: Challenges in Representation Learning (WREPL)},
  year          = {2013}
}
@inproceedings{sajjadi2016regularization,
  title         = {Regularization with stochastic transformations and perturbations for deep semi-supervised learning},
  author        = {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
  booktitle     = {Advances in neural information processing systems},
  pages         = {1163--1171},
  year          = {2016}
}
@book{vapnik1998statistical,
  title         = {Statistical Learning Theory},
  author        = {Vapnik, Vladimir},
  year          = {1998},
  publisher     = {Wiley-Interscience}
}
@inproceedings{joachims1999transductive,
  title         = {Transductive inference for text classification using support vector machines},
  author        = {Joachims, Thorsten},
  booktitle     = {ICML},
  year          = {1999},
  organization  = {Citeseer}
}
@book{mitchell1997machine,
  title         = {Machine Learning},
  author        = {Mitchell, Tom M.},
  year          = {1997},
  publisher     = {McGraw-Hill}
}
@inproceedings{zhou2004learning,
  title         = {Learning with unlabeled data and its application to image retrieval},
  author        = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas Navin and Weston, Jason and Scholkopf, Bernhard},
  booktitle     = {Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining},
  year          = {2004},
  organization  = {ACM}
}
@inproceedings{belkin2006manifold,
  title         = {Manifold regularization: A geometric framework for learning from labeled and unlabeled examples},
  author        = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  booktitle     = {Journal of machine learning research},
  year          = {2006},
  organization  = {MIT Press}
}
@inproceedings{zhu2015aligning,
  title         = {Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author        = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard S. and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle     = {2015 IEEE International Conference on Computer Vision (ICCV)},
  pages         = {19--27},
  year          = {2015},
  organization  = {IEEE Computer Society},
  address       = {Santiago, Chile},
  month         = {dec},
  doi           = {10.1109/ICCV.2015.10}
}
@misc{projectgutenberg,
  title         = {Project Gutenberg},
  howpublished  = {\url{https://www.gutenberg.org/}},
  note          = {Accessed: 2024-04-14}
}
@article{trinh2018simple,
  title         = {A Simple Method for Commonsense Reasoning},
  author        = {Trinh, Trieu H. and Le, Quoc V.},
  journal       = {CoRR},
  volume        = {abs/1806.02847},
  year          = {2018},
  eprint        = {1806.02847},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@inproceedings{zellers2019defending,
  title         = {Defending Against Neural Fake News},
  author        = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle     = {Advances in Neural Information Processing Systems 32},
  editor        = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alch{\'e}-Buc, Florence and Fox, Emily B. and Garnett, Roman},
  pages         = {9051--9062},
  year          = {2019},
  address       = {Vancouver, BC, Canada},
  publisher     = {NeurIPS},
  note          = {NeurIPS 2019, December 8-14}
}
@misc{gokaslan2019openwebtext,
  author        = {Gokaslan, Aaron and Pavlick, Ellie and Tellex, Stefanie},
  title         = {OpenWebText Corpus},
  year          = {2019},
  howpublished  = {\url{http://Skylion007.github.io/OpenWebTextCorpus}}
}
@inproceedings{baumgartner2020pushshift,
  title         = {The Pushshift Reddit Dataset},
  author        = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  booktitle     = {Proceedings of the Fourteenth International AAAI Conference on Web and Social Media},
  pages         = {830--839},
  year          = {2020},
  publisher     = {AAAI Press},
  address       = {Atlanta, Georgia, USA},
  note          = {ICWSM 2020, Held Virtually}
}
@misc{wikipedia,
  title         = {Wikipedia},
  howpublished  = {\url{https://en.wikipedia.org/wiki/Main\_Page}},
  note          = {Accessed: 2024-04-14}
}
@misc{bigquerydataset,
  title         = {BigQuery Dataset},
  howpublished  = {\url{https://cloud.google.com/bigquery?hl=zh-cn}},
  note          = {Accessed: 2024-04-14}
}
@article{gao2021pile,
  title         = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author        = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal       = {CoRR},
  volume        = {abs/2101.00027},
  year          = {2021},
  eprint        = {2101.00027},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{laurencon2022bigscience,
  title         = {The BigScience ROOTS Corpus: A 1.6 TB Composite Multilingual Dataset},
  author        = {Lauren\c{c}on, Herv{\'e} and Saulnier, Thibault and Wang, Teven and Akiki, Colin and del Moral, Angelina V. and Le Scao, Teven and Von Werra, Leandro and Mou, Charlie and Ponferrada, E. G. and Nguyen, Hang et al.},
  booktitle     = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year          = {2022},
  organization  = {NeurIPS}
}
@misc{commoncrawl,
  title         = {Common Crawl},
  howpublished  = {\url{https://commoncrawl.org/}},
  note          = {Accessed: 2024-04-15}
}
@misc{radford2018improving,
  title         = {Improving Language Understanding by Generative Pre-training},
  author        = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year          = {2018},
  note          = {Available online}
}
@article{nijkamp2022codegen,
  title         = {CodeGen: An Open Large Language Model for Code with Multi-turn Program Synthesis},
  author        = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal       = {arXiv preprint arXiv:2203.13474},
  year          = {2022}
}
@article{smith2022deepspeed,
  title         = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  author        = {Smith, Samyam and Patwary, Mostofa and Norick, Bryan and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhen and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zheng, Emily and Child, Rewon and Aminabadi, Raul Y. and Bernauer, James and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Shital and Catanzaro, Bryan},
  journal       = {CoRR},
  volume        = {abs/2201.11990},
  year          = {2022}
}
@article{zhang2022opt,
  title         = {OPT: open pre-trained transformer language models},
  author        = {Zhang, Sheng and Roller, Stephen and Goyal, Nitish and Artetxe, Mikel and Chen, Mingda and Chen, Shiyue and Dewan, Chandan and Diab, Mona T. and Li, Xiang and Lin, Xiang Vincent and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Konstantin and Simig, Deniz and Koura, Paul S. and Sridhar, Anirudh and Wang, Tianyi and Zettlemoyer, Luke},
  journal       = {CoRR},
  volume        = {abs/2205.01068},
  year          = {2022}
}
@article{chowdhery2022palm,
  title         = {PaLM: Scaling Language Modeling with Pathways},
  author        = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhik and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jamie and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Trevor and Levskaya, Anna and Ghemawat, Sanjay and Dev, Sharan and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kensen and Fedus, Liam and Zhou, Daisy and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexey and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Tharun S. and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zihang and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeffrey and Petrov, Slav and Fiedel, Noah},
  journal       = {CoRR},
  volume        = {abs/2204.02311},
  year          = {2022}
}
@article{austin2021program,
  title         = {Program synthesis with large language models},
  author        = {Austin, James and Odena, Augustus and Nye, Maxwell I. and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Chris J. and Terry, Michael and Le, Quoc V. and Sutton, Charles},
  journal       = {CoRR},
  volume        = {abs/2108.07732},
  year          = {2021}
}
@article{li2022competition,
  title         = {Competition-level code generation with AlphaCode},
  author        = {Li, Y. and Choi, D. H. and Chung, J. and Kushman, N. and Schrittwieser, J. and Leblond, R. and Eccles, T. and Keeling, J. and Gimeno, F. and Lago, A. D. and others},
  journal       = {Science},
  year          = {2022}
}
@inproceedings{xu2022systematic,
  title         = {A Systematic Evaluation of Large Language Models of Code},
  author        = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J.},
  booktitle     = {MAPS\@PLDI},
  year          = {2022}
}
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoff- mann, H. F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. M. Jayakumar, E. Buchatskaya, D. Budden, E. Suther- land, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sotti- aux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d’Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. J. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, “Scaling language models: Methods, analysis & insights from training gopher,” CoRR, vol. abs/2112.11446, 2021
@article{rae2021scaling,
  title         = {Scaling language models: Methods, analysis \& insights from training Gopher},
  author        = {Rae, Jack W. and Borgeaud, Samuel and Cai, Tom and Millican, Kevin and Hoffmann, Jannis and Song, H. Francis and Aslanides, John and Henderson, Scott and Ring, Robert and Young, Stephen and Rutherford, Edward and Hennigan, Tom and Menick, Jacob and Cassirer, Alex and Powell, Robert and van den Driessche, George and Hendricks, Luke A. and Rauh, Michael and Huang, Peter and Glaese, Alexander and Welbl, Johannes and Dathathri, Sumanth and Huang, Sharan and Uesato, Jonathan and Mellor, Joe and Higgins, Iain and Creswell, Antonia and McAleese, Neil and Wu, Andrew and Elsen, Erich and Jayakumar, Siddhant M. and Buchatskaya, Elena and Budden, David and Sutherland, Ewan and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, James and Li, X. L. and Kuncoro, Adji B. and Nematzadeh, Azalia and Gribovskaya, Daria and Donato, Davide and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean and Tsimpoukelli, Maria and Grigorev, Nikita and Fritz, David and Sottiaux, Thibaut and Pajarskas, Titas and Pohlen, Thomas and Gong, Zhi and Toyama, Daniel and de Masson d'Autume, Charles and Li, Yutong and Terzi, Tugrul and Mikulik, Vojtech and Babuschkin, Igor and Clark, Andrew and de Las Casas, David and Guy, Alex and Jones, Chris and Bradbury, James and Johnson, Michael J. and Hechtman, Ben A. and Weidinger, Luke and Gabriel, Ilya and Isaac, William S. and Lockhart, Edward and Osindero, Simon and Rimell, Luke and Dyer, Chris and Vinyals, Oriol and Ayoub, Karim and Stanway, Jonathan and Bennett, Luke and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  journal       = {CoRR},
  volume        = {abs/2112.11446},
  year          = {2021},
  eprint        = {2112.11446},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{longpre2023pretrainer,
  title         = {A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity},
  author        = {Longpre, Scott and Yauney, Greg and Reif, Emily and Lee, Kaitao and Roberts, Adam and Zoph, Dan and Zhou, Dianqi and Wei, Jason and Robinson, Kyle and Mimno, David and others},
  journal       = {arXiv preprint arXiv:2305.13169},
  year          = {2023}
}
@article{chen2023datajuicer,
  title         = {Data-Juicer: A One-Stop Data Processing System for Large Language Models},
  author        = {Chen, Dong and Huang, Yuxuan and Ma, Zhiyi and Chen, Hao and Pan, Xinyu and Ge, Cheng and Gao, Dong and Xie, Yuxuan and Liu, Zhiyuan and Gao, Jianfeng and Li, Yizhong and Ding, Bo and Zhou, Jie},
  journal       = {arXiv preprint arXiv:2305.13169},
  year          = {2023}
}
@inproceedings{du2022glam,
  title         = {GLAM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author        = {Du, Nan and Huang, Yuxuan and Dai, Andrew M. and Tong, Shiyu and Lepikhin, Dmitry and Xu, Yanshuai and Krikun, Maxim and Zhou, Yuxiong and Yu, Andrew W. and Firat, Orhan and Zoph, Barret and Fedus, William and Bosma, Maarten P. and Zhou, Zhi and Wang, Tianyi and Wang, Yifan E. and Webster, Kevin and Pellat, Marie and Robinson, Kyle and Meier-Hellstern, Kathy S. and Duke, Trevor and Dixon, Luke and Zhang, Kevin and Le, Quoc V. and Wu, Yuxiong and Chen, Zhifeng and Cui, Can},
  booktitle     = {International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA},
  year          = {2022},
  pages         = {5547--5569}
}
@misc{huggingface2023perplexity,
  title         = {Perplexity - Transformers},
  url           = {https://huggingface.co/docs/transformers/perplexity},
  note          = {Accessed: 2024-04-06},
  organization  = {Hugging Face},
  year          = {2023}
}
@article{hernandez2022scaling,
  title         = {Scaling laws and interpretability of learning from repeated data},
  author        = {Hernandez, Daniel and Brown, Tom B. and Conerly, Thomas and DasSarma, Gaurav and Drain, Daniel and Showk, Sina Ehsani and Elhage, Nabil and Hatfield-Dodds, Luke and Henighan, Tom and Hume, Thomas and Johnston, Scott and Mann, Benjamin and Olah, Chris and Olsson, Carl and Amodei, Dario and Joseph, Nicholas and Kaplan, Jared and McCandlish, Sam},
  journal       = {CoRR},
  volume        = {abs/2205.10487},
  year          = {2022},
  eprint        = {2205.10487},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{holzman2020curious,
  title         = {The Curious Case of Neural Text Degeneration},
  author        = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal       = {8th International Conference on Learning Representations, ICLR 2020},
  year          = {2020},
  note          = {OpenReview.net},
  url           = {https://openreview.net/forum?id=rygGQyrFvH}
}
@inproceedings{lee2022deduplicating,
  title         = {Deduplicating training data makes language models better},
  author        = {Lee, Kenton and Ippolito, Daniel and Nystrom, Daniel and Zhang, Chris and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle     = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022},
  year          = {2022},
  pages         = {8424--8445}
}
@article{carlini2022quantifying,
  title         = {Quantifying memorization across neural language models},
  author        = {Carlini, Nicholas and Ippolito, Daniel and Jagielski, Marcin and Lee, Kenton and Tram\`{e}r, Florian and Zhang, Chris},
  journal       = {CoRR},
  volume        = {abs/2202.12488},
  year          = {2022},
  eprint        = {2202.12488},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{carlini2021extracting,
  title         = {Extracting training data from large language models},
  author        = {Carlini, Nicholas and Tram\`{e}r, Florian and Wallace, Eric and Jagielski, Marcin and Herbert-Voss, Andreas and Lee, Kenton and Roberts, Adam and Brown, Tom B. and Song, Dawn and Erlingsson, \'{U}lfar and Oprea, Alina and Raffel, Colin},
  booktitle     = {30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021},
  year          = {2021},
  pages         = {2633--2650}
}
@inproceedings{lafferty2001conditional,
  title         = {Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
  author        = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
  booktitle     = {Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001)},
  editor        = {Brodley, Carla E. and Danyluk, Andrea P.},
  pages         = {282--289},
  year          = {2001},
  address       = {Williams College, Williamstown, MA, USA},
  publisher     = {Morgan Kaufmann},
  month         = jun
}
@inproceedings{sennrich2016neural,
  title         = {Neural machine translation of rare words with subword units},
  author        = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle     = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers},
  year          = {2016},
  publisher     = {The Association for Computer Linguistics}
}
@inproceedings{kudo2018sentencepiece,
  title         = {Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author        = {Kudo, Taku and Richardson, John},
  booktitle     = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations},
  editor        = {Blanco, Eduardo and Lu, Wei},
  year          = {2018},
  address       = {Brussels, Belgium},
  publisher     = {Association for Computational Linguistics},
  month         = oct
}
@article{wu2016google,
  title         = {Google's Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation},
  author        = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Lukasz and Gouws, Stephan and Kato, Yoshiki and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeff},
  journal       = {CoRR},
  volume        = {abs/1609.08144},
  year          = {2016},
  url           = {http://arxiv.org/abs/1609.08144},
  eprint        = {1609.08144},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{lewis2020bart,
  title         = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author        = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle     = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages         = {7871--7880},
  year          = {2020},
  organization  = {Association for Computational Linguistics},
  url           = {https://www.aclweb.org/anthology/2020.acl-main.703}
}
@misc{li2021prefixtuning,
  title         = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author        = {Xiang Lisa Li and Percy Liang},
  year          = {2021},
  eprint        = {2101.00190},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{zhang2022examining,
  title         = {Examining scaling and transfer of language model architectures for machine translation},
  author        = {Zhang, Biao and Ghorbani, Amir and Bapna, Ankur and Cheng, Yuxiang and Garcia, Xavier and Shen, Jonathan and Firat, Orhan},
  booktitle     = {International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA},
  year          = {2022},
  pages         = {26176--26192}
}
@inproceedings{dong2019unified,
  title         = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
  author        = {Dong, Li and Yang, Nan and Wang, Wei and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  booktitle     = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  year          = {2019},
  pages         = {13042--13054}
}
@article{li2022ptuning,
  title         = {P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author        = {Li, Xiang Lisa and Liang, Percy},
  journal       = {CoRR},
  volume        = {abs/2202.12108},
  year          = {2022},
  eprint        = {2202.12108},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{raschka2023encoderdecoder,
  author        = {Raschka, Sebastian},
  title         = {Understanding Encoder and Decoder},
  year          = {2023},
  url           = {https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder},
  urldate       = {2024-04-13}
}
@article{zeng2021pangu,
  title         = {Pangu-$\alpha$: Large-scale autoregressive pretrained Chinese language models with auto-parallel computation},
  author        = {Zeng, Weihua and Ren, Xiang and Su, Tao and Wang, Haoyuan and Liao, Yuxuan and Wang, Zhiyuan and Jiang, Xiaowei and Yang, Zhi and Wang, Kai and Zhang, Xiaodong and Li, Chen and Gong, Zhe and Yao, Yuxian and Huang, Xiaodan and Wang, Jie and Yu, Jinsong and Guo, Qipeng and Yu, Yuxian and Zhang, Yuxian and Wang, Jie and Tao, Haoyu and Yan, Da and Yi, Zhi and Peng, Fei and Jiang, Fan and Zhang, Huan and Deng, Li and Zhang, Yizhong and Lin, Zhiyuan and Zhang, Chen and Zhang, Shuai and Guo, Ming and Gu, Sheng and Fan, Guotong and Wang, Yuxuan and Jin, Xiaodong and Liu, Qun and Tian, Ying},
  journal       = {CoRR},
  volume        = {abs/2104.12369},
  year          = {2021},
  eprint        = {2104.12369},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{thoppilan2022lamda,
  title         = {LaMDA: Language Models for Dialog Applications},
  author        = {Thoppilan, Romal and Freitas, Daniel De and Hall, Jason and Shazeer, Noam and Kulshreshtha, Abhishek and Cheng, Hongrae and Jin, Annie and Bos, Timothee and Baker, Lucy and Du, Yuan and Li, Yifeng and Lee, Hyung Won and Zheng, Henry Shu and Ghafouri, Ammar and Menegali, Matheus and Huang, Yuhui and Krikun, Maxim and Lepikhin, Dmitry and Qin, Jiahui and Chen, Dehao and Xu, Yexin and Chen, Zhifeng and Roberts, Adam and Bosma, Martin and Zhou, Yonghui and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marcus and Meier-Hellstern, Katherine S. and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Romulo Drummond and Duke, Trevor and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Meredith and Hutchinson, Ben and Olson, Kristen and Molina, Adam and Hoffman-John, Elizabeth and Lee, Jennifer and Aroyo, Lora and Rajakumar, Rajesh and Butryna, Anna and Lamm, Matthew and Kuzmina, Valentina and Fenton, Josh and Cohen, Alon and Bernstein, Robert and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Clara and Croak, Marian and Chi, Ed H. and Le, Quoc},
  journal       = {CoRR},
  volume        = {abs/2201.08239},
  year          = {2022},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  eprint        = {2201.08239}
}
@article{lieber2021jurassic,
  title         = {Jurassic-1: Technical details and evaluation},
  author        = {Lieber, Or and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal       = {White Paper. AI21 Labs},
  volume        = {1},
  year          = {2021}
}
@article{touvron2023llama2,
  title         = {LLaMA 2: Open Foundation and Fine-Tuned Chat Models},
  author        = {Touvron, Hugo and Martin, Ludovic and Stone, Kevin and Albert, Paul and Alma- hairi, Antoine and Babaei, Yashar and Bashlykov, Nikita and Batra, Saurabh and Bhargava, Pranav and Bhosale, Shubham and others},
  journal       = {arXiv preprint arXiv:2307.09288},
  year          = {2023}
}
@online{penedo2023refinedweb,
  title         = {The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only},
  author        = {Penedo, Gerardo and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Rares and Cappelli, Andrea and Alobeidli, Hamad and Pannier, Benjamin and Almazrouei, Eisa and Launay, Julien},
  year          = {2023},
  eprint        = {2306.01116},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{zeng2022glm130b,
  title         = {GLM-130B: An Open Bilingual Pre-trained Model},
  author        = {Zeng, Ailing and Liu, Xinxin and Du, Zihan and Wang, Zhiwei and Lai, Heng and Ding, Ming and Yang, Zitao and Xu, Yixuan and Zheng, Weijie and Xia, Xue and Tam, Wai Lam and Ma, Zhiyuan and Xue, Yixiao and Zhai, Junfeng and Chen, Wei and Zhang, Peng and Dong, Yan and Tang, Jie},
  year          = {2022},
  eprint        = {2210.02414},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{bahdanau2014neural,
  title         = {Neural machine translation by jointly learning to align and translate},
  author        = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal       = {CoRR},
  volume        = {abs/1409.0473},
  year          = {2014},
  eprint        = {1409.0473},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{britz2017massive,
  title         = {Massive exploration of neural machine translation architectures},
  author        = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc V.},
  journal       = {CoRR},
  volume        = {abs/1703.03906},
  year          = {2017},
  eprint        = {1703.03906},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{gu2022efficiently,
  title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
  author        = {Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle     = {The Tenth International Conference on Learning Representations},
  year          = {2022},
  url           = {https://openreview.net/forum?id=uYLFoz1vlAC},
  note          = {Accessed: 2024-04-13}
}
@article{mehta2022long,
  title         = {Long Range Language Modeling via Gated State Spaces},
  author        = {Mehta, Hrushikesh and Gupta, Ankush and Cutkosky, Ashok and Neyshabur, Behnam},
  journal       = {CoRR},
  volume        = {abs/2206.13947},
  year          = {2022},
  doi           = {10.48550/arXiv.2206.13947},
  url           = {https://doi.org/10.48550/arXiv.2206.13947}
}
@article{dao2022hungry,
  title         = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author        = {Dao, Tri and Fu, Daniel Y. and Saab, Khaled K. and Thomas, Albert W. and Rudra, Atri and R{\'e}, Christopher},
  journal       = {CoRR},
  volume        = {abs/2212.14052},
  year          = {2022},
  doi           = {10.48550/arXiv.2212.14052},
  url           = {https://doi.org/10.48550/arXiv.2212.14052}
}
@inproceedings{poli2023hyena,
  title         = {Hyena hierarchy: Towards larger convolutional language models},
  author        = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  booktitle     = {ICML},
  year          = {2023}
}
@article{peng2023rwkv,
  title         = {RWKV: Reinventing RNNs for the Transformer Era},
  author        = {Peng, Bin and Alcaide, Eduardo and Anthony, Quincy and Albalak, Aaron and Arcadinho, Sofia and Cao, Hui and Cheng, Xiaohu and Chung, Minjun and Grella, Matt and G.V., Krishna Kishore and He, Xiang and Hou, Han and Kazienko, Przemys{\l}aw and Kocon, Jan and Kong, Ji and Koptyra, Bartosz and Lau, Hok Shing and Mantri, Krishna Sai Inkoolu and Mom, Fabian and Saito, Akira and Tang, Xiao and Wang, Bing and Wind, John Sebastian and Wozniak, Stanis{\l}aw and Zhang, Rui and Zhang, Zhen and Zhao, Qian and Zhou, Ping and Zhu, Jun and Zhu, Rong},
  journal       = {CoRR},
  volume        = {abs/2305.13048},
  year          = {2023},
  url           = {https://doi.org/10.48550/arXiv.2305.13048},
  doi           = {10.48550/arXiv.2305.13048}
}
@article{sun2023retentive,
  title         = {Retentive Network: A Successor to Transformer for Large Language Models},
  author        = {Sun, Yi and Dong, Lei and Huang, Shuming and Ma, Shujie and Xia, Ying and Xue, Jingjing and Wang, Jin and Wei, Furu},
  journal       = {CoRR},
  volume        = {abs/2307.08621},
  year          = {2023},
  eprint        = {2307.08621},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2307.08621}
}
@inproceedings{ding2021cogview,
  title         = {CogView: Mastering Text-to-Image Generation via Transformers},
  author        = {Ding, Ming and Yang, Zizhao and Hong, Wei and Zheng, Weijie and Zhou, Cheng and Yin, Dong and Lin, Jie and Zou, Xiaochun and Shao, Zhiyang and Yang, Hui and Tang, Jie},
  booktitle     = {Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual},
  year          = {2021},
  pages         = {19822--19835}
}
@article{ba2016layer,
  title         = {Layer normalization},
  author        = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  journal       = {CoRR},
  volume        = {abs/1607.06450},
  year          = {2016},
  eprint        = {1607.06450},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{zhang2019root,
  title         = {Root Mean Square Layer Normalization},
  author        = {Zhang, Biao and Sennrich, Rico},
  booktitle     = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  year          = {2019},
  pages         = {12360--12371}
}
@article{wang2022deepnet,
  title         = {DeepNet: Scaling Transformers to 1,000 Layers},
  author        = {Wang, Haoyuan and Ma, Shujie and Dong, Lei and Huang, Shuming and Zhang, Dong and Wei, Furu},
  journal       = {CoRR},
  volume        = {abs/2203.00555},
  year          = {2022},
  eprint        = {2203.00555},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{nair2010rectified,
  title         = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  author        = {Nair, Vinod and Hinton, Geoffrey E.},
  booktitle     = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  year          = {2010},
  pages         = {807--814}
}
@inproceedings{wang2018glue,
  title         = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author        = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  booktitle     = {Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP\@EMNLP 2018, Brussels, Belgium, November 1, 2018},
  editor        = {Linzen, Tal and Chrupala, Grzegorz and Alishahi, Afra},
  publisher     = {Association for Computational Linguistics},
  year          = {2018},
  pages         = {353--355}
}
@article{ramachandran2017searching,
  title         = {Searching for activation functions},
  author        = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  journal       = {arXiv preprint arXiv:1710.05941},
  year          = {2017}
}
@article{ioffe2015batch,
  title         = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author        = {Ioffe, Sergey and Szegedy, Christian},
  journal       = {CoRR},
  volume        = {abs/1502.03167},
  year          = {2015},
  eprint        = {1502.03167},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{narang2021transformer,
  title         = {Do Transformer Modifications Transfer Across Implementations and Applications?},
  author        = {Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, William and F{\'e}vry, Thibault and Matena, Michael and Malkan, Kellie and Fiedel, Nick and Shazeer, Noam and Lan, Zhenzhong and Zhou, Yuxiang and Li, Wei and Ding, Ning and Marcus, Joshua and Roberts, Adam and Raffel, Colin},
  booktitle     = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021},
  year          = {2021},
  pages         = {5758--5773}
}
@inproceedings{xiong2020layer,
  title         = {On Layer Normalization in the Transformer Architecture},
  author        = {Xiong, Ruibo and Yang, Yuxian and He, Di and Zheng, Kai and Zheng, Shizhen and Xing, Chen and Zhang, Hui and Lan, Yanyan and Wang, Lu and Liu, Tie-Yan},
  booktitle     = {ICML},
  year          = {2020}
}
@inproceedings{baevski2019adaptive,
  title         = {Adaptive Input Representations for Neural Language Modeling},
  author        = {Baevski, Alexei and Auli, Michael},
  booktitle     = {7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019},
  year          = {2019},
  note          = {OpenReview.net}
}
@inproceedings{liu2020understanding,
  title         = {Understanding the difficulty of training transformers},
  author        = {Liu, Lizi and Liu, Xiang and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  booktitle     = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020},
  year          = {2020},
  pages         = {5747--5763}
}
@article{he2016deep,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal       = {CoRR},
  volume        = {abs/1512.03385},
  year          = {2016},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{glorot2011deep,
  title         = {Deep Sparse Rectifier Neural Networks},
  author        = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  journal       = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011},
  year          = {2011}
}
@article{maas2013rectifier,
  title         = {Rectifier nonlinearities improve neural network acoustic models},
  author        = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
  journal       = {CoRR},
  volume        = {abs/1312.6026},
  year          = {2013},
  eprint        = {1312.6026},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{hendrycks2016gaussian,
  title         = {Gaussian Error Linear Units (GELUs)},
  author        = {Hendrycks, Dan and Gimpel, Kevin},
  journal       = {arXiv preprint arXiv:1606.08415},
  year          = {2016}
}
@inproceedings{press2022train,
  title         = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author        = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
  booktitle     = {The Tenth International Conference on Learning Representations},
  year          = {2022},
  url           = {https://openreview.net/forum?id=JZJ9Zz1vZ6},
  note          = {Accessed: 2024-04-13}
}
@article{shaw2018self,
  title         = {Self-attention with relative position representations},
  author        = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal       = {CoRR},
  volume        = {abs/1803.02155},
  year          = {2018},
  eprint        = {1803.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{peng2021random,
  title         = {Random Feature Attention},
  author        = {Peng, Baolin and Li, Xiang and Liang, Percy},
  journal       = {CoRR},
  volume        = {abs/2106.14448},
  year          = {2021},
  eprint        = {2106.14448},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{zaheer2020big,
  title         = {Big Bird: Transformers for Longer Sequences},
  author        = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kunal and Ainslie, Jonathon and Alberti, Chris and Ontan\~o\'n, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle     = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual},
  year          = {2020}
}
@article{child2019generating,
  title         = {Generating Long Sequences with Sparse Transformers},
  author        = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal       = {CoRR},
  volume        = {abs/1904.10509},
  year          = {2019},
  eprint        = {1904.10509},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{shazeer2019fast,
  title         = {Fast Transformer Decoding: One Write-Head is All You Need},
  author        = {Shazeer, Noam},
  journal       = {CoRR},
  volume        = {abs/1911.02150},
  year          = {2019},
  url           = {http://arxiv.org/abs/1911.02150},
  eprint        = {1911.02150},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{li2023starcoder,
  title         = {StarCoder: may the source be with you!},
  author        = {Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and Jo\~{a}o Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Mu\~{n}oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
  year          = {2023},
  eprint        = {2305.06161},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{ainslie2023gqa,
  title         = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author        = {Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr\'{o}n and Sumit Sanghai},
  year          = {2023},
  eprint        = {2305.13245},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{dao2022flashattention,
  title         = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author        = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher R\'{e}},
  year          = {2022},
  eprint        = {2205.14135},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{liu2022fast,
  title         = {Fast and Memory-Efficient Attention with FlashAttention-2},
  author        = {Liu, Lizi and Liu, Xiang and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal       = {CoRR},
  volume        = {abs/2205.14135},
  year          = {2022},
  eprint        = {2205.14135},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{vllm2023,
  title         = {vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention},
  howpublished  = {Available online},
  year          = {2023},
  url           = {https://vllm.ai/}
}
@article{tay2021longrange,
  title         = {Long Range Arena: A Benchmark for Efficient Transformers},
  author        = {Tay, Yi and Baevski, Alexei and Fan, Angela and Nogueira, Rodrigo and Auli, Michael},
  journal       = {CoRR},
  volume        = {abs/2011.04006},
  year          = {2021},
  eprint        = {2011.04006},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{huggingfaceQuantization,
  author        = {{Hugging Face}},
  title         = {Quantization},
  year          = {2024},
  url           = {https://huggingface.co/docs/transformers/main/en/main\_classes/quantization},
  urldate       = {2024-04-22},
  note          = {Accessed: 2024-04-22}
}
@inproceedings{mishra2022crosstask,
  title         = {Cross-task generalization via natural language crowdsourcing instructions},
  author        = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle     = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages         = {3470--3487},
  year          = {2022},
  editor        = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  address       = {Dublin, Ireland},
  month         = {May 22-27},
  organization  = {ACL},
  url           = {https://aclanthology.org/2022.acl-long.243}
}
@article{bach2022promptsource,
  title         = {PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
  author        = {Bach, Sebastian H. and Sanh, Victor and Yong, Zhi-Xuan and Webson, Alex and Raffel, Colin and Nayak, Nikhil V. and Sharma, Ankit and Kim, Tae-Hwan and Bari, Md. Saiful and F{\'e}vry, Thibault and Alyafeai, Zaid and Dey, Manan and Santilli, Anthony and Sun, Zhi and Ben-David, Shai and Xu, Chen and Chhablani, Gaurav and Wang, Hao and Fries, Jason A. and AlShaibani, Mohammed S. and Sharma, Shubham and Thakker, Urvish and Almubarak, Khalid and Tang, Xinyi and Radev, Dragomir R. and Jiang, Ming-Ting and Rush, Alexander M.},
  journal       = {CoRR},
  volume        = {abs/2202.12108},
  year          = {2022},
  eprint        = {2202.12108},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{wang2022super,
  title         = {Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  author        = {Wang, Yada and Mishra, Swaroop and Alipoormolabashi, Parisa and Kordi, Yaser and Mirzaei, Amir and Naik, Aniruddha and Ashok, Anirudh and Dhanasekaran, Arun S. and Arunkumar, Anirudh and Stap, Daniel and Pathak, Eshaan and Karamanolakis, George and Lai, Hui Guan and Purohit, Ishan and Mondal, Indranil and Anderson, Jennifer and Kuznia, Kevin and Doshi, Khyati and Pal, Koustuv and Patel, Manan and Moradshahi, Mehrdad and Parmar, Mihir and Purohit, Mihir and Varshney, Naman and Kaza, Pranav R. and Verma, Prateek and Puri, Raghav Karia and Doshi, Sagar and Sampat, Sagar K. and Mishra, Swaroop R. A and Patro, Srikar and Dixit, Tanmay and Shen, Xiang},
  journal       = {CoRR},
  volume        = {abs/2209.13107},
  year          = {2022},
  eprint        = {2209.13107},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{tang2022mvp,
  title         = {MVP: Multi-Task Supervised Pre-training for Natural Language Generation},
  author        = {Tang, Tian and Li, Jindong and Zhao, Wenxuan and Wen, Ji-Rong},
  journal       = {CoRR},
  volume        = {abs/2206.12131},
  year          = {2022},
  eprint        = {2206.12131},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{muennighoff2022crosslingual,
  title         = {Crosslingual Generalization Through Multitask Finetuning},
  author        = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Le Scao, Teven and Bari, M. Saiful and Shen, Sheng and Yong, Zheng Xin and Schoelkopf, Hannes and Tang, Xiang and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Alvin and Raff, Edward and Raffel, Colin},
  journal       = {CoRR},
  volume        = {abs/2211.01786},
  year          = {2022},
  url           = {https://arxiv.org/abs/2211.01786}
}
@misc{bai2022training,
  title         = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author        = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
  year          = {2022},
  eprint        = {2204.05862},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{guo2023close,
  title         = {How close is ChatGPT to human experts? Comparison corpus, evaluation, and detection},
  author        = {Guo, Binbin and Zhang, Xiang and Wang, Zhiyuan and Jiang, Min and Nie, Jian-Yun and Ding, Yuxiang and Yue, Jie and Wu, Yan},
  journal       = {CoRR},
  volume        = {abs/2301.07597},
  year          = {2023},
  eprint        = {2301.07597},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{koepf2023openassistant,
  title         = {OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author        = {Kopf, Andreas and Kilcher, Yannic and von Rutte, David and Anagnostidis, Stefanos and Tam, Zhi-Rui and Stevens, Kevin and Barhoum, Ahmed and Duc, Nhat Minh and Stanley, Oliver and Nagyfi, Robert and others},
  journal       = {arXiv preprint arXiv:2304.07327},
  year          = {2023}
}
@article{wang2022selfinstruct,
  title         = {Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author        = {Wang, Yada and Kordi, Yaser and Mishra, Swaroop and Liu, Anqi and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal       = {CoRR},
  volume        = {abs/2212.10560},
  year          = {2022}
}
@misc{taori2023stanford,
  title         = {Stanford ALPACA: An Instruction-Following LLaMA Model},
  author        = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Ting and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  year          = {2023},
  howpublished  = {\url{https://github.com/tatsu-lab/stanford-alpaca}}
}
@article{fedus2021switch,
  title         = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author        = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal       = {J. Mach. Learn. Res},
  pages         = {1--40},
  year          = {2021}
}
@article{xu2023baize,
  title         = {Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
  author        = {Xu, Chen and Guo, Dong and Duan, Nan and McAuley, Julian},
  journal       = {arXiv preprint arXiv:2304.01196},
  year          = {2023}
}
@article{ji2023towards,
  title         = {Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation},
  author        = {Ji, Yuxuan and Gong, Yuxuan and Deng, Yuxuan and Peng, Yuxin and Niu, Qian and Ma, Bin and Li, Xiang},
  journal       = {arXiv preprint arXiv:2304.07854},
  year          = {2023}
}
@article{hubara2017quantized,
  title         = {Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations},
  author        = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal       = {J. Mach. Learn. Res},
  volume        = {18},
  pages         = {6869--6898},
  year          = {2017}
}
@article{miyashita2016convolutional,
  title         = {Convolutional Neural Networks Using Logarithmic Data Representation},
  author        = {Miyashita, Daisuke and Lee, Edward H. and Murmann, Boris},
  journal       = {CoRR},
  volume        = {abs/1603.01025},
  year          = {2016},
  eprint        = {1603.01025},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{jacob2017quantization,
  title         = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author        = {Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
  year          = {2017},
  eprint        = {1712.05877},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{sanhetal2022multitask,
  title         = {Multitask prompted training enables zero-shot task generalization},
  author        = {Sanh, Victor and Webson, Alex and Raffel, Colin and Bach, Sebastian H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Alex and Stiegler, Adam and Raja, Anirudh and Dey, Manan and Bari, M. Saiful and Xu, Chen and Thakker, Urvish and Sharma, Shubham S. and Szczechla, Eric and Kim, Tae-Hwan and Chhablani, Gaurav and Nayak, Nikhil V. and Datta, Debanjan and Chang, Jason and Jiang, Ming-Ting and Wang, Hao and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harsh and Bawden, Robert and Wang, Thomas and Neeraj, Tanmay and Rozen, Jonathan and Sharma, Ankit and Santilli, Anthony and F{\'e}vry, Thibault and Fries, Jason A. and Teehan, Ryan and Scao, Teven Le and Biderman, Stella and Gao, Li and Wolf, Thomas and Rush, Alexander M.},
  journal       = {The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022},
  year          = {2022},
  note          = {OpenReview.net}
}
@article{radford2023gpt4,
  title         = {GPT-4: A Large-Scale Generative Pre-trained Transformer},
  author        = {Radford, Alec and Kim, Jong Wook and Child, Rewon and Wu, Jeff and Amodei, Dario and Sutskever, Ilya},
  journal       = {CoRR},
  volume        = {abs/2304.07409},
  year          = {2023},
  eprint        = {2304.07409},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{liu2019multi,
  title         = {Multi-task deep neural networks for natural language understanding},
  author        = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal       = {CoRR},
  volume        = {abs/1901.11504},
  year          = {2019},
  eprint        = {1901.11504},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{aghajanyan2021muppet,
  title         = {Muppet: Massive multi-task representations with pre-finetuning},
  author        = {Aghajanyan, Anna and Gupta, Ankit and Shrivastava, Abhinav and Chen, Xiang and Zettlemoyer, Luke and Gupta, Saurabh},
  journal       = {CoRR},
  volume        = {abs/2109.08668},
  year          = {2021},
  eprint        = {2109.08668},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{longpre2023flan,
  title         = {The FLAN Collection: Designing Data and Methods for Effective Instruction Tuning},
  author        = {Longpre, Shayne and Hou, Lu and Vu, Trieu and Webson, Abigail and Chung, Hyung Won and Tay, Yi and Zhou, Da and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  journal       = {CoRR},
  volume        = {abs/2301.13688},
  year          = {2023},
  url           = {https://arxiv.org/abs/2301.13688}
}
@article{sanh2021distilbert,
  title         = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author        = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal       = {Proceedings of the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS},
  year          = {2019},
  pages         = {12--23}
}
@article{chen2023maybe,
  title         = {Maybe Only 0.5\% Data Is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning},
  author        = {Chen, H. and Zhang, Y. and Zhang, Q. and Yang, H. and Hu, X. and Ma, X. and Yanggong, Y. and Zhao, J.},
  journal       = {arXiv preprint arXiv:2305.09246},
  year          = {2023}
}
@article{iyer2022opt,
  title         = {OPT-IML: Scaling Language Model Instruction Meta Learning Through the Lens of Generalization},
  author        = {Iyer, Srinivasan and Lin, Xiang Vincent and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Deniz and Yu, Peng and Shuster, Konstantin and Wang, Thomas and Liu, Qian and Koura, Pierre-Simon and Li, Xiang and O'Horo, Benjamin and Pereyra, Gabriel and Wang, Jue and Dewan, Chandan and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal       = {CoRR},
  volume        = {abs/2212.12017},
  year          = {2022},
  eprint        = {2212.12017},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{chiang2023vicuna,
  title         = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%\textasteriskcentered ChatGPT Quality},
  author        = {Chiang, W.-L. and Li, Z. and Lin, Z. and Sheng, Y. and Wu, Z. and Zhang, H. and Zheng, L. and Zhuang, S. and Zhuang, Y. and Gonzalez, J. E. and Stoica, I. and Xing, E. P.},
  year          = {2023},
  note          = {[Online]. Available: \url{https://vicuna.lmsys.org}}
}
@article{krell2021efficient,
  title         = {Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance},
  author        = {Krell, Michael M. and Kosec, Mario and Perez, Santiago P. and Fitzgibbon, Andrew},
  journal       = {CoRR},
  volume        = {abs/2107.02027},
  year          = {2021},
  eprint        = {2107.02027},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{cao2023instruction,
  title         = {Instruction Mining: When Data Mining Meets Large Language Model Finetuning},
  author        = {Yihan Cao and Yanbin Kang and Chi Wang and Lichao Sun},
  year          = {2023},
  eprint        = {2307.06290},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{xu2023wizardlm,
  title         = {WizardLM: Empowering Large Language Models to Follow Complex Instructions},
  author        = {Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
  year          = {2023},
  eprint        = {2304.12244},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{tamkin2021understanding,
  title         = {Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models},
  author        = {Tamkin, Alex and Trisha, Singh and Giovanardi, Davide and Goodman, Noah},
  journal       = {arXiv preprint arXiv:2102.02503},
  year          = {2021}
}
@article{weng2018attention,
  title         = "Attention? Attention!",
  author        = "Weng, Lilian",
  journal       = "lilianweng.github.io",
  year          = "2018",
  url           = "https://lilianweng.github.io/posts/2018-06-24-attention/"
}
@article{kenton2021alignment,
  title         = {Alignment of language agents},
  author        = {Kenton, Z. and Everitt, T. and Weidinger, L. and Gabriel, I. and Mikulik, V. and Irving, G.},
  journal       = {CoRR},
  volume        = {abs/2103.14659},
  year          = {2021}
}
@article{askell2021general,
  title         = {A General Language Assistant as a Laboratory for Alignment},
  author        = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and Das Sarma, Nikhil and Elhage, Neel and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Chris and Amodei, Dario and Brown, Tom B. and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  journal       = {CoRR},
  volume        = {abs/2112.00861},
  year          = {2021}
}
@article{glaese2022improving,
  title         = {Improving Alignment of Dialogue Agents via Targeted Human Judgements},
  author        = {Glaese, A. and McAleese, N. and Trebacz, M. and Aslanides, J. and Firoiu, V. and Ewalds, T. and Rauh, M. and Weidinger, L. and Chadwick, M. and Thacker, P. and Campbell-Gillingham, L. and Uesato, J. and Huang, P. and Comanescu, R. and Yang, F. and See, A. and Dathathri, S. and Greig, R. and Chen, C. and Fritz, D. and Elias, J. S. and Green, R. and Mokra, S. and Fernando, N. and Wu, B. and Foley, R. and Young, S. and Gabriel, I. and Isaac, W. and Mellor, J. and Hassabis, D. and Kavukcuoglu, K. and Hendricks, L. A. and Irving, G.},
  journal       = {CoRR},
  volume        = {abs/2209.14375},
  year          = {2022}
}
@article{ziegler2019fine,
  title         = {Fine-tuning language models from human preferences},
  author        = {Ziegler, Daniel M and Stiennon, Nicolas and Wu, Jieren and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul F and Irving, Geoffrey},
  journal       = {CoRR},
  volume        = {abs/1909.08593},
  year          = {2019}
}
@article{nakano2021webgpt,
  title         = {WebGPT: Browser-assisted Question-Answering with Human Feedback},
  author        = {Nakano, R. and Hilton, J. and Balaji, S. and Wu, J. and Ouyang, L. and Kim, C. and Hesse, C. and Jain, S. and Kosaraju, V. and Saunders, W. and Jiang, X. and Cobbe, K. and Eloundou, T. and Krueger, G. and Button, K. and Knight, M. and Chess, B. and Schulman, J.},
  journal       = {CoRR},
  volume        = {abs/2112.09332},
  year          = {2021}
}
@misc{hu2021lora,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  year          = {2021},
  eprint        = {2106.09685},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{lester2021power,
  title         = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  author        = {Brian Lester and Rami Al-Rfou and Noah Constant},
  year          = {2021},
  eprint        = {2104.08691},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{houlsby2019parameterefficient,
  title         = {Parameter-Efficient Transfer Learning for NLP},
  author        = {Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  year          = {2019},
  eprint        = {1902.00751},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{hu2023llmadapters,
  title         = {LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  author        = {Zhiqiang Hu and Lei Wang and Yihuai Lan and Wanyu Xu and Ee-Peng Lim and Lidong Bing and Xing Xu and Soujanya Poria and Roy Ka-Wei Lee},
  year          = {2023},
  eprint        = {2304.01933},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{he2022unified,
  title         = {Towards a Unified View of Parameter-Efficient Transfer Learning},
  author        = {Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
  year          = {2022},
  eprint        = {2110.04366},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{liu2022ptuning,
  title         = {P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks},
  author        = {Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},
  year          = {2022},
  eprint        = {2110.07602},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{qin2021learning,
  title         = {Learning how to ask: Querying LMs with mixtures of soft prompts},
  author        = {Qin, Guanghui and Eisner, Jason},
  journal       = {CoRR},
  volume        = {abs/2104.06599},
  year          = {2021},
  eprint        = {2104.06599},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{zhang2023adalora,
  title         = {AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  author        = {Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
  year          = {2023},
  eprint        = {2303.10512},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{valipour2023dylora,
  title         = {DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation},
  author        = {Mojtaba Valipour and Mehdi Rezagholizadeh and Ivan Kobyzev and Ali Ghodsi},
  year          = {2023},
  eprint        = {2210.07558},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{liu2022good,
  title         = {What makes good in-context examples for gpt-3?},
  author        = {Liu, J. and Shen, D. and Zhang, Y. and Dolan, B. and Carin, L. and Chen, W.},
  booktitle     = {Proceedings of Deep Learning Inside Out (DeeLIO): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, at ACL 2022},
  year          = {2022},
  pages         = {100--114},
  address       = {Dublin, Ireland and Online},
  month         = {May}
}
@inproceedings{rubin2022learning,
  title         = {Learning to retrieve prompts for in-context learning},
  author        = {Rubin, O. and Herzig, J. and Berant, J.},
  booktitle     = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
  year          = {2022},
  pages         = {2655--2671},
  address       = {Seattle, WA, United States},
  month         = {July}
}
@article{kim2022self,
  title         = {Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator},
  author        = {Kim, H. J. and Cho, H. and Kim, J. and Kim, T. and Yoo, K. M. and Lee, S.},
  journal       = {CoRR},
  volume        = {abs/2206.08082},
  year          = {2022}
}
@inproceedings{zhou2023large,
  title         = {Large language models are human-level prompt engineers},
  author        = {Zhou, Y. and Muresanu, A. I. and Han, Z. and Paster, K. and Pitis, S. and Chan, H. and Ba, J.},
  booktitle     = {Proc. of ICLR},
  year          = {2023}
}
@inproceedings{lu2022fantastically,
  title         = {Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author        = {Lu, Y. and Bartolo, M. and Moore, A. and Riedel, S. and Stenetorp, P.},
  booktitle     = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year          = {2022},
  pages         = {8086--8098},
  address       = {Dublin, Ireland},
  month         = {May}
}
@article{fu2022complexity,
  title         = {Complexity-based prompting for multi-step reasoning},
  journal       = {CoRR},
  volume        = {abs/2210.00720},
  author        = {Fu, Y. and Peng, H. and Sabharwal, A. and Clark, P. and Khot, T.},
  year          = {2022}
}
@article{zhang2022automatic,
  title         = {Automatic chain of thought prompting in large language models},
  journal       = {CoRR},
  volume        = {abs/2210.03493},
  author        = {Zhang, Z. and Zhang, A. and Li, M. and Smola, A.},
  year          = {2022}
}
@article{creswell2022selection,
  title         = {Selection-inference: Exploiting large language models for interpretable logical reasoning},
  journal       = {CoRR},
  volume        = {abs/2205.09712},
  author        = {Creswell, A. and Shanahan, M. and Higgins, I.},
  year          = {2022}
}
@article{li2022making,
  title         = {Making Large Language Models Better Reasoners with Step-Aware Verifier},
  author        = {Yifei Li and Zeqi Lin and Shizhuo Zhang and Qiang Fu and Bei Chen and Jian-Guang Lou and Weizhu Chen},
  year          = {2023},
  eprint        = {2206.02336},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{wang2022rationale,
  title         = {Rationale-augmented ensembles in language models},
  journal       = {CoRR},
  volume        = {abs/2206.02336},
  author        = {Wang, X. and Wei, J. and Schuurmans, D. and Le, Q. V. and Chi, E. H. and Zhou, D.},
  year          = {2022}
}
@article{zhou2022least,
  title         = {Least-to-most prompting enables complex reasoning in large language models},
  journal       = {CoRR},
  volume        = {abs/2205.10625},
  author        = {Zhou, D. and Sch{\"a}rli, N. and Hou, L. and Wei, J. and Scales, N. and Wang, X. and Schuurmans, D. and Bousquet, O. and Le, Q. and Chi, E. H.},
  year          = {2022}
}
@article{khot2022decomposed,
  title         = {Decomposed prompting: A modular approach for solving complex tasks},
  journal       = {CoRR},
  volume        = {abs/2210.02406},
  author        = {Khot, T. and Trivedi, H. and Finlayson, M. and Fu, Y. and Richardson, K. and Clark, P. and Sabharwal, A.},
  year          = {2022},
  note          = {\url{https://doi.org/10.48550/arXiv.2210.02406}}
}
@article{wang2023plan,
  title         = {Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models},
  journal       = {CoRR},
  volume        = {abs/2305.04091},
  author        = {Wang, L. and Xu, W. and Lan, Y. and Hu, Z. and Lan, Y. and Lee, R. K. and Lim, E.},
  year          = {2023},
  note          = {\url{https://doi.org/10.48550/arXiv.2305.04091}}
}
@article{lyu2023faithful,
  title         = {Faithful chain-of-thought reasoning},
  journal       = {CoRR},
  volume        = {abs/2301.13379},
  author        = {Lyu, Q. and Havaldar, S. and Stein, A. and Zhang, L. and Rao, D. and Wong, E. and Apidianaki, M. and Callison-Burch, C.},
  year          = {2023}
}
@article{gao2022pal,
  title         = {PAL: program-aided language models},
  journal       = {CoRR},
  volume        = {abs/2211.10435},
  author        = {Gao, L. and Madaan, A. and Zhou, S. and Alon, U. and Liu, P. and Yang, Y. and Callan, J. and Neubig, G.},
  year          = {2022}
}
@article{shen2023hugginggpt,
  title         = {Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface},
  journal       = {arXiv preprint arXiv:2303.17580},
  author        = {Shen, Y. and Song, K. and Tan, X. and Li, D. and Lu, W. and Zhuang, Y.},
  year          = {2023}
}
@article{sun2023adaplanner,
  title         = {Adaplanner: Adaptive planning from feedback with language models},
  journal       = {arXiv preprint arXiv:2305.16653},
  author        = {Sun, H. and Zhuang, Y. and Kong, L. and Dai, B. and Zhang, C.},
  year          = {2023}
}
@article{lu2023multimodal,
  title         = {Multimodal procedural planning via dual text-image prompting},
  journal       = {CoRR},
  volume        = {abs/2305.01795},
  author        = {Lu, Y. and Lu, P. and Chen, Z. and Zhu, W. and Wang, X. E. and Wang, W. Y.},
  year          = {2023}
}
@article{hao2023reasoning,
  title         = {Reasoning with language model is planning with world model},
  journal       = {CoRR},
  volume        = {abs/2305.14992},
  author        = {Hao, S. and Gu, Y. and Ma, H. and Hong, J. J. and Wang, Z. and Wang, D. Z. and Hu, Z.},
  year          = {2023}
}
@article{chen2023chatcot,
  title         = {Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models},
  journal       = {CoRR},
  volume        = {abs/2305.14323},
  author        = {Chen, Z. and Zhou, K. and Zhang, B. and Gong, Z. and Zhao, W. X. and Wen, J.},
  year          = {2023}
}
@article{yao2022react,
  title         = {React: Synergizing reasoning and acting in language models},
  journal       = {CoRR},
  volume        = {abs/2210.03629},
  author        = {Yao, S. and Zhao, J. and Yu, D. and Du, N. and Shafran, I. and Narasimhan, K. and Cao, Y.},
  year          = {2022}
}
@article{shinn2023reflexion,
  title         = {Reflexion: Language agents with verbal reinforcement learning},
  author        = {Shinn, N. and Cassano, F. and Labash, B. and Gopinath, A. and Narasimhan, K. and Yao, S.},
  year          = {2023}
}
@article{yao2023tree,
  title         = {Tree of thoughts: Deliberate problem solving with large language models},
  journal       = {CoRR},
  volume        = {abs/2305.10601},
  author        = {Yao, S. and Yu, D. and Zhao, J. and Shafran, I. and Griffiths, T. L. and Cao, Y. and Narasimhan, K.},
  year          = {2023}
}
@article{hao2022structured,
  title         = {Structured prompting: Scaling in-context learning to 1,000 examples},
  author        = {Hao, S. and Sun, Y. and Dong, L. and Han, Z. and Gu, Y. and Wei, F.},
  journal       = {CoRR},
  volume        = {abs/2206.08082},
  year          = {2022}
}
@misc{dong2023survey,
  title         = {A Survey on In-context Learning},
  author        = {Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Zhiyong Wu and Baobao Chang and Xu Sun and Jingjing Xu and Lei Li and Zhifang Sui},
  year          = {2023},
  eprint        = {2301.00234},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{min2022metaicl,
  title         = "{M}eta{ICL}: Learning to Learn In Context",
  author        = "Min, Sewon  and Lewis, Mike  and Zettlemoyer, Luke and Hajishirzi, Hannaneh",
  editor        = "Carpuat, Marine  and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir",
  booktitle     = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month         = jul,
  year          = "2022",
  address       = "Seattle, United States",
  publisher     = "Association for Computational Linguistics",
  url           = "https://aclanthology.org/2022.naacl-main.201",
  doi           = "10.18653/v1/2022.naacl-main.201",
  pages         = "2791--2809",
  abstract      = "We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task training data, and outperforms much bigger models with nearly 8x parameters."
}
@misc{wei2023symbol,
  title         = {Symbol tuning improves in-context learning in language models},
  author        = {Jerry Wei and Le Hou and Andrew Lampinen and Xiangning Chen and Da Huang and Yi Tay and Xinyun Chen and Yifeng Lu and Denny Zhou and Tengyu Ma and Quoc V. Le},
  year          = {2023},
  eprint        = {2305.08298},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{xu2023knn,
  title         = {kNN Prompting: Learning Beyond the Context with Nearest Neighbor Inference},
  author        = {Xu, Benfeng and Wang, Quan and Mao, Zhendong and Lyu, Yajuan and She, Qiaoqiao and Zhang, Yongdong},
  year          = {2023},
  booktitle     = {International Conference on Learning Representations},
  note          = {2023a}
}
@misc{gonen2022demystifying,
  title         = {Demystifying Prompts in Language Models via Perplexity Estimation},
  author        = {Hila Gonen and Srini Iyer and Terra Blevins and Noah A. Smith and Luke Zettlemoyer},
  year          = {2022},
  eprint        = {2212.04037},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{wu2022selfadaptive,
  author        = {Wu, Zhiyong and Wang, Yaoxiang and Ye, Jiacheng and Kong, Lingpeng},
  title         = {Self-Adaptive In-Context Learning},
  year          = {2022},
  note          = {Provide additional details such as the journal name, volume, issue, pages, and DOI if available}
}
@inproceedings{sorensen2022information,
  title         = "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels",
  author        = "Sorensen, Taylor  and Robinson, Joshua  and Rytting, Christopher  and Shaw, Alexander  and Rogers, Kyle  and Delorey, Alexia  and Khalil, Mahmoud  and Fulda, Nancy  and Wingate, David",
  editor        = "Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline",
  booktitle     = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month         = may,
  year          = "2022",
  address       = "Dublin, Ireland",
  publisher     = "Association for Computational Linguistics",
  url           = "https://aclanthology.org/2022.acl-long.60",
  doi           = "10.18653/v1/2022.acl-long.60",
  pages         = "819--862",
  abstract      = "Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates \textit{without labeled examples} and \textit{without direct access to the model}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90{\%} of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels."
}
@misc{zhang2022active,
  title         = {Active Example Selection for In-Context Learning},
  author        = {Yiming Zhang and Shi Feng and Chenhao Tan},
  year          = {2022},
  eprint        = {2211.04486},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{li2023finding,
  author        = {Li, Xiaonan and Qiu, Xipeng},
  title         = {Finding Supporting Examples for In-Context Learning},
  year          = {2023},
  note          = {arXiv preprint arXiv:2302.13539},
  url           = {https://arxiv.org/abs/2302.13539}
}
@misc{li2023unified,
  title         = {Unified Demonstration Retriever for In-Context Learning},
  author        = {Xiaonan Li and Kai Lv and Hang Yan and Tianyang Lin and Wei Zhu and Yuan Ni and Guotong Xie and Xiaoling Wang and Xipeng Qiu},
  year          = {2023},
  eprint        = {2305.04320},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{wang2023large,
  author        = {Wang, Xinyi and Zhu, Wanrong and Wang, William Yang},
  title         = {Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning},
  year          = {2023},
  note          = {arXiv preprint arXiv:2301.11916},
  url           = {https://arxiv.org/abs/2301.11916}
}
@misc{honovich2022instruction,
  title         = {Instruction Induction: From Few Examples to Natural Language Task Descriptions},
  author        = {Or Honovich and Uri Shaham and Samuel R. Bowman and Omer Levy},
  year          = {2022},
  eprint        = {2205.10782},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{press2023measuring,
  title         = {Measuring and Narrowing the Compositionality Gap in Language Models},
  author        = {Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah A. Smith and Mike Lewis},
  year          = {2023},
  eprint        = {2210.03350},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{li2023mot,
  title         = {MoT: Memory-of-Thought Enables ChatGPT to Self-Improve},
  author        = {Xiaonan Li and Xipeng Qiu},
  year          = {2023},
  eprint        = {2305.05181},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@online{xu2023small,
  author        = {Xu, Canwen and Xu, Yichong and Wang, Shuohang and Liu, Yang and Zhu, Chenguang and McAuley, Julian},
  title         = {Small Models are Valuable Plug-ins for Large Language Models},
  year          = {2023},
  note          = {arXiv preprint arXiv:2305.08848},
  url           = {https://arxiv.org/abs/2305.08848}
}
@inproceedings{wang2022iteratively,
  author        = {Wang, Boshi and Deng, Xiang and Sun, Huan},
  title         = {Iteratively Prompt Pre-trained Language Models for Chain of Thought},
  booktitle     = {Proceedings of The 2022 Conference on Empirical Methods for Natural Language Processing (EMNLP)},
  year          = {2022},
  address       = {Online and in-person event},
  month         = {December}
}
@inproceedings{chen2022improving,
  title         = "Improving In-Context Few-Shot Learning via Self-Supervised Training",
  author        = "Chen, Mingda  and Du, Jingfei  and Pasunuru, Ramakanth  and Mihaylov, Todor  and Iyer, Srini  and Stoyanov, Veselin  and Kozareva, Zornitsa",
  editor        = "Carpuat, Marine  and de Marneffe, Marie-Catherine  and Meza Ruiz, Ivan Vladimir",
  booktitle     = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month         = jul,
  year          = "2022",
  address       = "Seattle, United States",
  publisher     = "Association for Computational Linguistics",
  url           = "https://aclanthology.org/2022.naacl-main.260",
  doi           = "10.18653/v1/2022.naacl-main.260",
  pages         = "3558--3573",
  abstract      = "Self-supervised pretraining has made few-shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in-context few-shot learning. In this paper, we propose to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning. We propose and evaluate four self-supervised objectives on two benchmarks. We find that the intermediate self-supervision stage produces models that outperform strong baselines. Ablation study shows that several factors affect the downstream performance, such as the amount of training data and the diversity of the self-supervised objectives. Human-annotated cross-task supervision and self-supervision are complementary. Qualitative analysis suggests that the self-supervised-trained models are better at following task requirements."
}
@online{gu2023pretraining,
  author        = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  title         = {Pre-training to Learn in Context},
  year          = {2023},
  note          = {arXiv preprint arXiv:2305.09137},
  url           = {https://arxiv.org/abs/2305.09137}
}
@online{liu2021pretrain,
  author        = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  title         = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  year          = {2021},
  note          = {arXiv preprint arXiv:2107.13586},
  url           = {https://arxiv.org/abs/2107.13586}
}
@article{wies2023learnability,
  author        = {Wies, N. and Levine, Y. and Shashua, A.},
  title         = {The Learnability of In-context Learning},
  journal       = {CoRR},
  volume        = {abs/2303.07895},
  year          = {2023}
}
@article{akyurek2022what,
  author        = {Aky{\"u}rek, E. and Schuurmans, D. and Andreas, J. and Ma, T. and Zhou, D.},
  title         = {What Learning Algorithm Is In-context Learning? Investigations with Linear Models},
  journal       = {CoRR},
  volume        = {abs/2211.15661},
  year          = {2022}
}
@article{pan2023what,
  author        = {Pan, J. and Gao, T. and Chen, H. and Chen, D.},
  title         = {What In-context Learning "Learns" In-context: Disentangling Task Recognition and Task Learning},
  journal       = {CoRR},
  volume        = {abs/2305.09731},
  year          = {2023}
}
@inproceedings{min2022noisy,
  title         = "Noisy Channel Language Model Prompting for Few-Shot Text Classification",
  author        = "Min, Sewon  and Lewis, Mike  and Hajishirzi, Hannaneh  and Zettlemoyer, Luke",
  editor        = "Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline",
  booktitle     = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month         = may,
  year          = "2022",
  address       = "Dublin, Ireland",
  publisher     = "Association for Computational Linguistics",
  url           = "https://aclanthology.org/2022.acl-long.365",
  doi           = "10.18653/v1/2022.acl-long.365",
  pages         = "5316--5330",
  abstract      = "We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required."
}
@inproceedings{zhao2021calibrate,
  title         = {Calibrate Before Use: Improving Few-shot Performance of Language Models},
  author        = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  pages         = {12697--12706},
  year          = {2021},
  editor        = {Meila, Marina and Zhang, Tong},
  volume        = {139},
  series        = {Proceedings of Machine Learning Research},
  month         = {18--24 Jul},
  publisher     = {PMLR},
  pdf           = {http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf},
  url           = {https://proceedings.mlr.press/v139/zhao21c.html},
  abstract      = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's accuracy (up to 30.0\% absolute) across different choices of the prompt, while also making learning considerably more stable.}
}
@inproceedings{shin2022effect,
  title         = "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
  author        = "Shin, Seongjin  and Lee, Sang-Woo  and Ahn, Hwijeen  and Kim, Sungdong  and Kim, HyoungSeok  and Kim, Boseop  and Cho, Kyunghyun  and Lee, Gichang  and Park, Woomyoung  and Ha, Jung-Woo  and Sung, Nako",
  editor        = "Carpuat, Marine  and de Marneffe, Marie-Catherine  and Meza Ruiz, Ivan Vladimir",
  booktitle     = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month         = jul,
  year          = "2022",
  address       = "Seattle, United States",
  publisher     = "Association for Computational Linguistics",
  url           = "https://aclanthology.org/2022.naacl-main.380",
  doi           = "10.18653/v1/2022.naacl-main.380",
  pages         = "5168--5186",
  abstract      = "Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance."
}
@article{min2022rethinking,
  author        = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  title         = {Rethinking the Role of Demonstrations: What Makes In-context Learning Work?},
  journal       = {CoRR},
  volume        = {abs/2202.12837},
  year          = {2022},
  url           = {https://arxiv.org/abs/2202.12837}
}
@article{an2023how,
  author        = {An, Shengnan and Lin, Zeqi and Fu, Qiang and Chen, Bei and Zheng, Nanning and Lou, Jian-Guang and Zhang, Dongmei},
  title         = {How Do In-context Examples Affect Compositional Generalization?},
  journal       = {CoRR},
  volume        = {abs/2305.04835},
  year          = {2023},
  url           = {https://arxiv.org/abs/2305.04835}
}
@misc{yoo2022groundtruth,
  title         = {Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations},
  author        = {Kang Min Yoo and Junyeob Kim and Hyuhng Joon Kim and Hyunsoo Cho and Hwiyeol Jo and Sang-Woo Lee and Sang-goo Lee and Taeuk Kim},
  year          = {2022},
  eprint        = {2205.12685},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wei2023larger,
  title         = {Larger language models do in-context learning differently},
  author        = {Jerry Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
  year          = {2023},
  eprint        = {2303.03846},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{chan2022data,
  title         = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
  author        = {Stephanie C. Y. Chan and Adam Santoro and Andrew K. Lampinen and Jane X. Wang and Aaditya Singh and Pierre H. Richemond and Jay McClelland and Felix Hill},
  year          = {2022},
  eprint        = {2205.05055},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{xie2022an,
  title         = {An Explanation of In-context Learning as Implicit Bayesian Inference},
  author        = {Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
  booktitle     = {International Conference on Learning Representations},
  year          = {2022},
  url           = {https://openreview.net/forum?id=RdJVFCHjUMI}
}
@misc{garg2023transformers,
  title         = {What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  author        = {Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
  year          = {2023},
  eprint        = {2208.01066},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{li2023transformers,
  title         = {Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author        = {Yingcong Li and M. Emrullah Ildiz and Dimitris Papailiopoulos and Samet Oymak},
  year          = {2023},
  eprint        = {2301.07067},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{hahn2023theory,
  title         = {A Theory of Emergent In-Context Learning as Implicit Structure Induction},
  author        = {Michael Hahn and Navin Goyal},
  year          = {2023},
  eprint        = {2303.07971},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{vonoswald2023transformers,
  title         = {Transformers learn in-context by gradient descent},
  author        = {Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and Jo\~{a}o Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
  year          = {2023},
  eprint        = {2212.07677},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{olsson2022incontext,
  title         = {In-context Learning and Induction Heads},
  author        = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  year          = {2022},
  eprint        = {2209.11895},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{suzgun2022challenging,
  title         = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author        = {Mirac Suzgun and Nathan Scales and Nathanael Sch\"{a}rli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny Zhou and Jason Wei},
  year          = {2022},
  eprint        = {2210.09261},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{srivastava2023imitation,
  title         = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author        = {Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri\`{a} Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlm\"{u}ller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karaka\c{s} and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bart\l{}omiej Bojanowski and Batuhan \"{O}zyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C\'{e}sar Ferri Ram\'{\i}rez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Mosegu\'{\i} Gonz\'{a}lez and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart\'{\i}nez-Plumed and Francesca Happ\'{e} and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germ\'{a}n Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-L\'{o}pez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Sch\"{u}tze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fern\'{a}ndez Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco\'{n} and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and J\"{o}rg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col\'{o}n and Luke Metz and L\"{u}tfi Kerem \c{S}enel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram\'{\i}rez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M\'{a}ty\'{a}s Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Micha\l{} Sw\k{e}drowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Mi\l{}kowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Rapha\"{e}l Milli\`{e}re and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Th\'{e}o Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
  year          = {2023},
  eprint        = {2206.04615},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wu2023openicl,
  title         = {OpenICL: An Open-Source Framework for In-context Learning},
  author        = {Zhenyu Wu and YaoXiang Wang and Jiacheng Ye and Jiangtao Feng and Jingjing Xu and Yu Qiao and Zhiyong Wu},
  year          = {2023},
  eprint        = {2303.02913},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{bar2022visual,
  author        = {Bar, Amir and Gandelsman, Yossi and Darrell, Trevor and Globerson, Amir and Efros, Alexei},
  title         = {Visual Prompting via Image Inpainting},
  booktitle     = {Advances in Neural Information Processing Systems},
  volume        = {35},
  pages         = {25005--25017},
  year          = {2022}
}
@inproceedings{wang2023images,
  author        = {Wang, Xinlong and Wang, Wen and Cao, Yue and Shen, Chunhua and Huang, Tiejun},
  title         = {Images Speak in Images: A Generalist Painter for In-Context Visual Learning},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages         = {6830--6839},
  year          = {2023},
  month         = {June}
}
@article{wang2023seggpt,
  author        = {Wang, Xinlong and Zhang, Xiaosong and Cao, Yue and Wang, Wen and Shen, Chunhua and Huang, Tiejun},
  title         = {SegGPT: Segmenting Everything in Context},
  journal       = {CoRR},
  volume        = {abs/2304.03284},
  year          = {2023},
  eprint        = {2304.03284},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@online{wang2023incontext,
  author        = {Wang, Zhendong and Jiang, Yifan and Lu, Yadong and Shen, Yelong and He, Pengcheng and Chen, Weizhu and Wang, Zhangyang and Zhou, Mingyuan},
  title         = {In-Context Learning Unlocked for Diffusion Models},
  year          = {2023},
  note          = {arXiv preprint arXiv:2305.01115},
  url           = {https://arxiv.org/abs/2305.01115}
}
@article{tsimpoukelli2021frozen,
  title         = {Frozen in Time: Temporal Contextualization for In-Context Learning},
  author        = {Tsimpoukelli, Katerina and Karamanolakis, Giorgos and Katsamanis, Athanasios and Maragos, Petros},
  journal       = {CoRR},
  volume        = {abs/2109.14867},
  year          = {2021},
  eprint        = {2109.14867},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{alayrac2022flamingo,
  title         = {Flamingo: a Visual Language Model for Few-Shot Learning},
  author        = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katherine Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year          = {2022},
  url           = {https://openreview.net/forum?id=EbMuimAbPbs}
}
@online{huang2023language,
  author        = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
  title         = {Language Is Not All You Need: Aligning Perception with Language Models},
  year          = {2023},
  note          = {arXiv preprint arXiv:2302.14045},
  url           = {https://arxiv.org/abs/2302.14045}
}
@online{hao2022language,
  author        = {Hao, Yaru and Song, Haoyu and Dong, Li and Huang, Shaohan and Chi, Zewen and Wang, Wenhui and Ma, Shuming and Wei, Furu},
  title         = {Language Models are General-Purpose Interfaces},
  year          = {2022},
  note          = {arXiv preprint arXiv:2206.06336},
  url           = {https://arxiv.org/abs/2206.06336}
}
@article{magister2022teaching,
  author        = {Magister, Alexander and Kuznetsova, Polina and Kuznetsov, Sergey},
  title         = {Teaching Language Models to Learn in Context},
  journal       = {CoRR},
  volume        = {abs/2205.10625},
  year          = {2022},
  eprint        = {2205.10625},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{chen2024relation,
  title         = {On the Relation between Sensitivity and Accuracy in In-context Learning},
  author        = {Yanda Chen and Chen Zhao and Zhou Yu and Kathleen McKeown and He He},
  year          = {2024},
  eprint        = {2209.07661},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{wang2023efficient,
  title         = {Efficient prompting via dynamic in-context learning},
  author        = {Wang, Chunshu and Jiang, Yuchen Eleanor and Cotterell, Ryan and Sachan, Mrinmaya},
  journal       = {arXiv preprint arXiv:2305.11170},
  year          = {2023}
}
@article{li2023contextual,
  title         = {Contextual Prompting for In-Context Learning},
  author        = {Li, Mukai and Gong, Shansan and Feng, Jiangtao and Xu, Yiheng and Zhang, Jun and Wu, Zhiyong and Kong, Lingpeng},
  journal       = {arXiv preprint arXiv:2302.04931},
  year          = {2023}
}
@misc{miao2021diverse,
  title         = {A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers},
  author        = {Shen-Yun Miao and Chao-Chun Liang and Keh-Yih Su},
  year          = {2021},
  eprint        = {2106.15772},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@misc{talmor2019commonsenseqa,
  title         = {CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge},
  author        = {Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
  year          = {2019},
  eprint        = {1811.00937},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{kojima2023large,
  title         = {Large Language Models are Zero-Shot Reasoners},
  author        = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
  year          = {2023},
  eprint        = {2205.11916},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{mudrakarta2018model,
  title         = {Did the model understand the question?},
  author        = {Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
  booktitle     = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages         = {1896--1906},
  year          = {2018},
  month         = jul,
  address       = {Melbourne, Australia},
  publisher     = {Association for Computational Linguistics},
  doi           = {10.18653/v1/P18-1176},
  url           = {https://aclanthology.org/P18-1176}
}
@inproceedings{sugawara2018makes,
  title         = {What makes reading comprehension questions easier?},
  author        = {Sugawara, Saku and Inui, Kentaro and Sekine, Satoshi and Aizawa, Akiko},
  booktitle     = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages         = {4208--4219},
  year          = {2018},
  month         = {October-November},
  address       = {Brussels, Belgium},
  publisher     = {Association for Computational Linguistics},
  doi           = {10.18653/v1/D18-1453},
  url           = {https://aclanthology.org/D18-1453}
}
@inproceedings{li2021why,
  title         = {Why machine reading comprehension models learn shortcuts?},
  author        = {Lai, Yuxuan and Zhang, Chen and Feng, Yansong and Huang, Quzhe and Zhao, Dongyan},
  booktitle     = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages         = {989--1002},
  year          = {2021},
  month         = {August},
  address       = {Online},
  publisher     = {Association for Computational Linguistics},
  doi           = {10.18653/v1/2021.findings-acl.85},
  url           = {https://aclanthology.org/2021.findings-acl.85}
}
@article{fu2022gptroadmap,
  title         = "How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources",
  author        = "Fu, Yao; Peng, Hao and Khot, Tushar",
  journal       = "Yao Fu's Notion",
  year          = "2022",
  month         = "Dec",
  url           = "https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1"
}
@article{madaan2022text,
  title         = {Text and patterns: For effective chain of thought, it takes two to tango},
  author        = {Madaan, Aman and Yazdanbakhsh, Alireza},
  journal       = {CoRR},
  volume        = {abs/2209.07686},
  year          = {2022},
  eprint        = {2209.07686},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{qian2022limitations,
  title         = {Limitations of Language Models in Arithmetic and Symbolic Induction},
  author        = {Jing Qian and Hong Wang and Zekun Li and Shiyang Li and Xifeng Yan},
  year          = {2022},
  eprint        = {2208.05051},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{bian2024chatgpt,
  title         = {ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models},
  author        = {Ning Bian and Xianpei Han and Le Sun and Hongyu Lin and Yaojie Lu and Ben He and Shanshan Jiang and Bin Dong},
  year          = {2024},
  eprint        = {2303.16421},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wang2023voyager,
  title         = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author        = {Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
  year          = {2023},
  eprint        = {2305.16291},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@article{liu2004conceptnet,
  title         = {Conceptnet--a practical commonsense reasoning tool-kit},
  author        = {Liu, Hugo and Singh, Push},
  journal       = {BT technology journal},
  volume        = {22},
  pages         = {211--226},
  year          = {2004}
}
@misc{jiang2024selfplanning,
  title         = {Self-planning Code Generation with Large Language Models},
  author        = {Xue Jiang and Yihong Dong and Lecheng Wang and Zheng Fang and Qiwei Shang and Ge Li and Zhi Jin and Wenpin Jiao},
  year          = {2024},
  eprint        = {2303.06689},
  archiveprefix = {arXiv},
  primaryclass  = {id='cs.SE' full\_name='Software Engineering' is\_active=True alt\_name=None in\_archive='cs' is\_general=False description='Covers design tools, software metrics, testing and debugging, programming environments, etc. Roughly includes material in all of ACM Subject Classes D.2, except that D.2.4 (program verification) should probably have Logics in Computer Science as the primary subject area.'}
}
@article{schick2023toolformer,
  title         = {Toolformer: Language models can teach themselves to use tools},
  author        = {Schick, Timo and Dwivedi-Yu, Jivat and Dess\`i, Roberta and Raileanu, Robyn and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal       = {CoRR},
  volume        = {abs/2302.04761},
  year          = {2023}
}
@misc{liu2023llmp,
  title         = {LLM+P: Empowering Large Language Models with Optimal Planning Proficiency},
  author        = {Bo Liu and Yuqian Jiang and Xiaohan Zhang and Qiang Liu and Shiqi Zhang and Joydeep Biswas and Peter Stone},
  year          = {2023},
  eprint        = {2304.11477},
  archiveprefix = {arXiv},
  primaryclass  = {id='cs.AI' full\_name='Artificial Intelligence' is\_active=True alt\_name=None in\_archive='cs' is\_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}
@article{singh2022progprompt,
  title         = {ProgPrompt: Generating Situated Robot Task Plans Using Large Language Models},
  author        = {Singh, Ishaan and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  journal       = {CoRR},
  volume        = {abs/2209.11302},
  year          = {2022}
}
